[["index.html", "Creating the flowtrend R package 1 Introduction", " Creating the flowtrend R package Sangwon Hyun, Tim Coleman, Francois Ribalet, Jacob Bien 2024-10-15 1 Introduction This package implements flowtrend, a model used for smooth estimation of mixture models across time. The documentation and package are both created using one simple command: litr::render(&quot;index.Rmd&quot;, output_format = litr::litr_gitbook(minimal_eval = TRUE)) "],["package-setup.html", "2 Package setup 2.1 1d data 2.2 2d data 2.3 3d data", " 2 Package setup The DESCRIPTION file is created using this code. usethis::create_package( path = &quot;.&quot;, fields = list( Package = params$package_name, Version = &quot;0.0.0.9000&quot;, Title = &quot;flowtrend&quot;, Description = &quot;Time-smooth mixture modeling for flow cytometry data.&quot;, `Authors@R` = person( given = &quot;Sangwon&quot;, family = &quot;Hyun&quot;, email = &quot;sangwonh@ucsc.edu&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;) ) ) ) usethis::use_mit_license(copyright_holder = &quot;Sangwon Hyun&quot;) The following is what will show up when someone types package?flowtrend in the console. #&#39; flowtrend #&#39; #&#39; This package implements the `flowtrend` method for automatic gating of flow cytometry data using trend filtering. #&#39; #&#39; @docType package This package will have some dependancies: library(tidyverse) library(ggplot2) usethis::use_package(&quot;tidyverse&quot;, type = &quot;depends&quot;) usethis::use_package(&quot;ggplot2&quot;) usethis::use_pipe() usethis::use_package(&quot;glmnet&quot;) usethis::use_package(&quot;clue&quot;) 2.1 1d data 2.1.1 Generating 1d data This function generates synthetic 1-dimensional data, and returns it in a “long” format matrix, with columns time, y, mu, and cluster. The latter two are the true underlying parameters. #&#39; Generates some synthetic 1-dimensional data with three clusters. Returns a #&#39; data frame with (1) time, (2) Y (3) mu (4) cluster assignments. #&#39; #&#39; @param TT Number of time points. #&#39; @param nt Number of particles at each time. #&#39; @param offset Defaults to 0. How much to push up cluster 1. #&#39; @param return_model If true, return true cluster means and probabilities #&#39; instead of data. #&#39; #&#39; @return long matrix with data or model. #&#39; @export gendat_1d &lt;- function(TT, ntlist, offset = 0, return_model = FALSE, sd3=NULL){ ## Basic checks stopifnot(length(ntlist) == TT) prob_link1 = rep(NA, TT) prob_link1[1] = 2 for(tt in 2:floor(TT/2)){ prob_link1[tt] = prob_link1[tt-1] + (1/TT) } for(tt in (floor(TT/2)+1):TT){ prob_link1[tt] = prob_link1[tt-1] - .5*(1/TT) } prob_link2 = sapply(1:TT, function(tt) 3 - 0.25*(tt/TT)) prob_link3 = sapply(1:TT, function(tt) 2.5) linkmat = cbind(prob_link1, prob_link2, prob_link3) ##%&gt;% matplot() mysum = exp(linkmat) %&gt;% rowSums() cluster_prob1 = exp(prob_link1) / mysum cluster_prob2 = exp(prob_link2) / mysum cluster_prob3 = exp(prob_link3) / mysum probs = cbind(cluster_prob1, cluster_prob2, cluster_prob3) colnames(probs) = 1:3 sd1 = 0.4 sd2 = 0.5 if(is.null(sd3)) sd3 = 0.35 ## if(!is.null(sd3)) sd3 = 1/1.5 ## The ratio of amplitude:data-standard-deviation is about ## ## $1.50695$. probs_long = as_tibble(probs) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) probs_long %&gt;% ggplot() + geom_line(aes(x=time,y=prob, group=cluster, col=cluster)) + ylim(c(0,1)) ## Make cluster means, by time means &lt;- matrix(NA, TT, 3) for(ii in 1:3){ for(tt in 1:TT){ means[tt, 1] &lt;- offset + tt/TT - 1.5 means[tt, 2] &lt;- sin(seq(-1, 1, length.out = TT)[tt] * 3.1415) means[tt, 3] &lt;- -3+sin(seq(-1, 1, length.out = TT)[tt] * 6.282) } } colnames(means) = c(1:3) means_long = as_tibble(means) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) model = full_join(means_long, probs_long, by = c(&quot;time&quot;, &quot;cluster&quot;)) model = model %&gt;% left_join(tibble(cluster = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;), sd = c(sd1, sd2, sd3)), by = &quot;cluster&quot;) ## Does this work? if(return_model) return(model) ys &lt;- lapply(1:TT, FUN = function(tt){ Y &lt;- vector(mode = &quot;list&quot;, length = 2) mu &lt;- vector(mode = &quot;list&quot;, length = 2) clusters_count &lt;- rmultinom(n = 1, size = ntlist[tt], prob = probs[tt,]) for(ii in 1:3){ if(ii == 1){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, mean=0, sd = sd1)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } if(ii == 2){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, mean=0, sd = sd2)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } if(ii == 3){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, mean=0, sd = sd3)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } } Y &lt;- unlist(Y) mu &lt;- unlist(mu) cluster &lt;- rep(1:3, times = clusters_count) one_df = tibble(time = tt, Y = Y, mu = mu, cluster = cluster) return(one_df) }) %&gt;% bind_rows() return(ys) } dt2ylist() is a helper that takes the output generated from gendat_1d(), and splits it by the time column to create a ylist object, which is a \\(T\\)-length list of \\(n_t \\times d\\) matrices. #&#39; Converting to a list of matrices, \\code{ylist}, to input to \\code{flowtrend()}. #&#39; #&#39; @param dt Output from \\code{gendat_1d()}. #&#39; #&#39; @return List of matrices #&#39; @export dt2ylist &lt;- function(dt){ dt%&gt;% dplyr::select(time, Y) %&gt;% arrange(time) %&gt;% group_by(time) %&gt;% group_split(.keep = FALSE) %&gt;% lapply(as.matrix) } Let’s generate some data using these functions. dt = gendat_1d(TT = 100, ntlist =rep(100,100)) print(dt) ylist = dt2ylist(dt) print(head(str(ylist[1:5]))) print(head(ylist[[1]])) Next, we’ll make some plotting functions 1d model and data. 2.1.2 Plotting 1d data Given 1d data ylist and an estimated model flowtrend object obj, we want to visualize these in a single plot. plot_1d() lets you do this. #&#39; Makes 1d plot of data and model #&#39; #&#39; @param ylist Data. A list of (|nt| by |dimdat|) matrices #&#39; @param obj A flowtrend (or flowmix) object. Defaults to NULL. #&#39; @param x Time points. Defaults to NULL. #&#39; @param alpha Between 0 and 1, how transparent to plot the data #&#39; points. Defaults to 0.1. #&#39; @param bin If TRUE, the data is binned. #&#39; #&#39; @return ggplot object with data, and optionally, a flowtrend model overlaid. #&#39; @export plot_1d &lt;- function(ylist, countslist=NULL, obj=NULL, x = NULL, alpha = .1, bin = FALSE, plot_band = TRUE){ ## Basic checks if(!is.null(x)){ stopifnot(length(x) == length(ylist)) times = x } else { times = 1:length(ylist) } dimdat = ncol(ylist[[1]]) assertthat::assert_that(dimdat == 1) ## If countslist is not provided, make a dummy if(is.null(countslist)) countslist = lapply(ylist, function(y) rep(1, nrow(y))) ## Make data into long matrix ymat &lt;- lapply(1:length(ylist), FUN = function(tt){ data.frame(time = times[tt], Y = ylist[[tt]], counts = countslist[[tt]]) }) %&gt;% bind_rows() %&gt;% as_tibble() colnames(ymat) = c(&quot;time&quot;, &quot;Y&quot;, &quot;counts&quot;) ## when ylist[[tt]] already has a column name, this is needed. if(bin){ gg = ymat %&gt;% ggplot() + geom_raster(aes(x = time, y = Y, fill = counts)) + theme_bw() + ylab(&quot;Data&quot;) + xlab(&quot;Time&quot;) + scale_fill_gradientn(colours = c(&quot;white&quot;, &quot;black&quot;)) } if(!bin){ gg = ymat %&gt;% ggplot() + geom_point(aes(x = time, y = Y), alpha = alpha) + theme_bw() + ylab(&quot;Data&quot;) + xlab(&quot;Time&quot;) } ## If there is a flowtrend object to plot, do it. if(is.null(obj)){ return(gg) } else { ## Add the model numclust = obj$numclust mnmat = obj$mn %&gt;% .[,1,] %&gt;% `colnames&lt;-`(1:numclust) %&gt;% as_tibble() %&gt;% add_column(time = times) probmat = obj$prob %&gt;% as_tibble() %&gt;% setNames(1:numclust) %&gt;% add_column(time = times) mn_long = mnmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) prob_long = probmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) est_long = full_join(mn_long, prob_long, by = c(&quot;time&quot;,&quot;cluster&quot;)) gg = gg + geom_path(aes(x = time, y = mean, linewidth = prob, group = cluster, color = cluster), data = est_long, lineend = &quot;round&quot;, linejoin=&quot;mitre&quot;) + scale_linewidth(range = c(0.05,5), limits = c(0, 1)) ## TODO: make it ignore the missing values at the gaps; currently this is not coded as NAs. if(plot_band){ ## Add the estimated 95% probability regions for data. stdev = obj$sigma %&gt;% .[,,1] %&gt;% sqrt() mn_long_by_clust = mn_long %&gt;% group_by(cluster) %&gt;% group_split() band_long_by_clust = lapply(1:numclust, function(iclust){ mn_long_by_clust[[iclust]] %&gt;% mutate(upper = mean + 1.96 * stdev[iclust]) %&gt;% mutate(lower = mean - 1.96 * stdev[iclust]) }) band_long = band_long_by_clust %&gt;% bind_rows() gg = gg + geom_line(aes(x = time, y = upper, group = cluster, color = cluster), data = band_long, size = rel(.7), alpha = .5) + geom_line(aes(x = time, y = lower, group = cluster, color = cluster), data = band_long, size = rel(.7), alpha = .5) + guides(size = &quot;none&quot;) # To turn off line size from legend } return(gg) } } The plotting function plot_1d() will be even more useful when we have a model, but can also simply plot the data ylist. Let’s try this out. set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100)) dt_model &lt;- gendat_1d(100, rep(100, 100), return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() plot_1d(ylist, NULL, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model, linetype = &quot;dashed&quot;, size=2, alpha = .7) Voilà! When we have a model fit on 3d data but want to visualize it, you can do: plot_1d(ylist, countslist) %&gt;% plot_1d_add_model(obj = obj, idim = 1) #&#39; Add a model. #&#39; #&#39; @param obj Model #&#39; @param gg0 result of plot_1d #&#39; @param idim dimension #&#39; @param plot_band If TRUE, plot a +1.96 standard deviation band. #&#39; #&#39; @return ggplot object #&#39; #&#39; @export plot_1d_add_model &lt;- function(gg0, obj, idim, plot_band = TRUE){ times = obj$x ## 1:(obj$TT) ## resolve this! numclust = obj$numclust mnmat = obj$mn %&gt;% .[,idim,] %&gt;% `colnames&lt;-`(1:numclust) %&gt;% as_tibble() %&gt;% add_column(time = times) probmat = obj$prob %&gt;% as_tibble() %&gt;% setNames(1:numclust) %&gt;% add_column(time = times) mn_long = mnmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) %&gt;% mutate(cluster = factor(cluster, levels=sapply(1:10, toString))) prob_long = probmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) %&gt;% mutate(cluster = factor(cluster, levels=sapply(1:10, toString))) est_long = full_join(mn_long, prob_long, by = c(&quot;time&quot;,&quot;cluster&quot;)) ## est_long = est_long %&gt;% mutate(cluster = factor(cluster, levels=sapply(1:10, toString))) gg = gg0 + geom_path(aes(x = time, y = mean, linewidth = prob, group = cluster, color = cluster), data = est_long, lineend = &quot;round&quot;, linejoin=&quot;mitre&quot;) + scale_linewidth(range = c(0,1)) ## Add the estimated 95% probability regions for data. if(plot_band){ stdev = obj$sigma %&gt;% .[,idim,idim] %&gt;% sqrt() names(stdev) = sapply(1:numclust, toString) mn_long_by_clust = mn_long %&gt;% group_by(cluster) %&gt;% group_split() band_long_by_clust = lapply(1:numclust, function(iclust){ mn_long_by_clust[[iclust]] %&gt;% mutate(upper = mean + 1.96 * stdev[iclust]) %&gt;% mutate(lower = mean - 1.96 * stdev[iclust]) }) band_long = band_long_by_clust %&gt;% bind_rows() gg = gg + geom_line(aes(x = time, y = upper, group = cluster, color = cluster), data = band_long, size = rel(.7), alpha = .5) + geom_line(aes(x = time, y = lower, group = cluster, color = cluster), data = band_long, size = rel(.7), alpha = .5) + guides(size = &quot;none&quot;) # To turn off line size from legend } return(gg) } Also, we will want to plot the estimated cluster probabilities of a model obj. #&#39; Makes cluster probability plot (lines over time). #&#39; #&#39; @param obj Estimated model (from e.g. \\code{flowtrend()}) #&#39; #&#39; @export plot_prob &lt;- function(obj, x = NULL){ ## Basic checks if(!is.null(x)){ times = x } else { ##stop(&quot;must provide x&quot;) times = 1:(obj$TT) } numclust = obj$numclust probmat = obj$prob %&gt;% as_tibble() %&gt;% setNames(1:numclust) %&gt;% add_column(time = times) prob_long = probmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) prob_long %&gt;% ggplot() + geom_line(aes(x=time, y = prob, group = cluster, col = cluster), size = rel(1)) + ggtitle(&quot;Estimated cluster probability&quot;) } We can’t test it out now, but we’ll use it later in 1d-example. Here is a 1d plotting function if you have cluster memberships memlist. #&#39; Makes 1d plot of data when #&#39; #&#39; @param ylist Particlel-level data. A list of (|nt| by |dimdat|) matrices. #&#39; @param memlist List of memberships for the particles. #&#39; @param x Time points. Defaults to NULL. #&#39; @param alpha Between 0 and 1, how transparent to plot the data #&#39; points. Defaults to 0.1. #&#39; #&#39; @return ggplot object with data, and optionally, a flowtrend model overlaid. #&#39; @export plot_1d_with_membership &lt;- function(ylist, memlist, countslist = NULL, x = NULL, alpha = .01){ ## Basic checks if(!is.null(x)){ stopifnot(length(x) == length(ylist)) times = x } else { times = 1:length(ylist) } dimdat = ncol(ylist[[1]]) ## If countslist is not provided, make a dummy if(is.null(countslist)) countslist = lapply(ylist, function(y) rep(1, nrow(y))) ## Make data into long matrix ymat &lt;- lapply(1:length(ylist), FUN = function(tt){ data.frame(time = times[tt], Y = ylist[[tt]], counts = countslist[[tt]], cluster = memlist[[tt]]) }) %&gt;% bind_rows() %&gt;% as_tibble() ymat = ymat %&gt;% mutate(cluster = as.factor(cluster)) colnames(ymat) = c(&quot;time&quot;, &quot;Y&quot;, &quot;counts&quot;, &quot;cluster&quot;) ## when ylist[[tt]] already has a column name, this is needed. gg = ymat %&gt;% ggplot() + geom_point(aes(x = time, y = Y, col = cluster),##, shape = cluster), alpha = alpha) + ##facet_wrap(~cluster) + theme_bw() + ylab(&quot;Data&quot;) + xlab(&quot;Time&quot;) return(gg) } 2.2 2d data 2.2.1 Generating 2d data #&#39; Generates some synthetic 2-dimensional data with three clusters. #&#39; #&#39; @param TT Number of time points. #&#39; @param nt Number of particles at each time. #&#39; #&#39; @return List containing (1) ylist, (2) mnlist, (3) clusterlist. #&#39; @export gendat_2d &lt;- function(TT, ntlist){ ## Basic checks stopifnot(length(ntlist) == TT) ## Make cluster probabilities, by time cluster_prob1 = sapply(1:TT, function(tt) sin(tt/24 * 2 * pi)/3 + 1 + (tt/TT)*5) cluster_prob2 = sapply(1:TT, function(tt) cos(tt/24 * 2 * pi)/3 + 8 - (tt/TT)*5) cluster_prob3 = rep(3, TT) probs = cbind(cluster_prob1, cluster_prob2, cluster_prob3) probs = probs/rowSums(probs) colnames(probs) = 1:3 probs_long = as_tibble(probs) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) probs_long %&gt;% ggplot() + geom_line(aes(x=time,y=prob, group=cluster, col=cluster)) + ylim(c(0,1)) ## Make cluster means, by time means &lt;- array(NA, dim = c(TT, 3, 2)) for(ii in 1:3){ for(tt in 1:TT){ means[tt, 1, 1] = means[tt, 1, 2] = tt/TT + 0.5 means[tt, 2, 1] = sin(tt/24 * 2 * pi)##seq(-1, 1, length.out = TT)[tt]*3.1415) means[tt, 2, 2] = 0 means[tt, 3, 1] = means[tt, 3, 2] = -3+cos(tt/24 * 2 * pi)##seq(-1, 1, length.out = TT)[tt]*6.282) } } dimnames(means)[[2]] = c(1:3) means_long = as_tibble(means) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) model = full_join(means_long, probs_long, by = c(&quot;time&quot;, &quot;cluster&quot;)) ylist = list() mulist = list() clusterlist = list() for(tt in 1:TT){ Y &lt;- vector(mode = &quot;list&quot;, length = 2) mu &lt;- vector(mode = &quot;list&quot;, length = 2) clusters_count &lt;- rmultinom(n = 1, size = ntlist[tt], prob = probs[tt,]) for(ii in 1:3){ if(ii == 1){ mn = means[tt,ii,,drop=TRUE] Sigma1 = matrix(c(0.4, 0.3, 0.3, 0.4), ncol = 2) Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0), Sigma= Sigma1)) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } if(ii == 2){ mn = means[tt,ii,, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0), Sigma = diag(c(0.5, 0.1)))) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } if(ii == 3){ mn = means[tt,ii,, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0), Sigma = diag(c(0.35, 0.35)))) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } } Y &lt;- Y %&gt;% purrr::compact() %&gt;% do.call(rbind, .) mu &lt;- mu %&gt;% purrr::compact() %&gt;% do.call(rbind, .) cluster &lt;- rep(1:3, times = clusters_count) ylist[[tt]] = Y mulist[[tt]] = mu clusterlist[[tt]] = cluster } return(list(ylist = ylist, mulist = mulist, clusterlist = clusterlist, probs = probs, means = means)) } 2.2.2 Plotting 2d data Here’s a simpler plotting function for 2d data at the particle level. #&#39; Simple plotter for 2d particle data. #&#39; #&#39; @param ylist Data. A list of (|nt| by |dimdat|) matrices #&#39; @param countslist Count data. #&#39; @param obj flowtrend (or flowmix) object. #&#39; @param time Out of 1 through \\code{lengthy(list)}, which time point to plot. #&#39; #&#39; @export #&#39; @return ggplot object. plot_2d &lt;- function(ylist, countslist = NULL, obj = NULL, tt, bin = TRUE, point_color = &quot;blue&quot;, raster_colours = c(&quot;white&quot;, &quot;blue&quot;)){ ## Basic checks stopifnot(ncol(ylist[[1]]) == 2) if(!is.null(obj)) stopifnot(obj$dimdat == 2) ## if(!bin) stop(&quot;2d plotting for particle level data isn&#39;t supported.&quot;) ## Take data from one time point y = ylist %&gt;% .[[tt]] if(is.null(colnames(y))){ colnames(y) = paste0(&quot;dim&quot;, c(1,2)) } y = y %&gt;% as_tibble() ## Handle counts if(is.null(countslist)){ counts = rep(1, nrow(ylist[[1]])) } if(!is.null(countslist)){ counts = countslist[[tt]] } ## Get variable names ## colnames(ylist[[1]]) = c(&quot;&quot;,&quot;&quot;,&quot;&quot;) varnames = y %&gt;% colnames() varname1 = varnames[1] varname2 = varnames[2] ## Get data from one timepoint y = y %&gt;% add_column(counts = counts) if(!bin){ p = y %&gt;% ggplot() + geom_point(aes(x = !!sym(varname1), y=!!sym(varname2)), size = rel(counts), alpha = .2, col = &quot;blue&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) + scale_size() } if(bin){ p = y %&gt;% ggplot() + geom_raster(aes(x = !!sym(varname1), y=!!sym(varname2), fill = counts)) + scale_fill_gradientn(guide=&quot;none&quot;, colours = raster_colours) } p = p + ggtitle(paste0(&quot;Time=&quot;, tt)) ## Adding visualizations of the model |obj| if(is.null(obj)){ return(p) } else { mnlist = lapply(1:obj$numclust, function(iclust){ one_mnmat = obj$mn[,,iclust] colnames(one_mnmat) = paste0(&quot;dim&quot;, 1:2) one_mnmat %&gt;% as_tibble() %&gt;% add_column(cluster = iclust) }) mnmat = do.call(rbind, mnlist) mn_colours = rep(&quot;red&quot;, obj$numclust) for(iclust in 1:obj$numclust){ ## Add ellipse el = ellipse::ellipse(x = obj$sigma[iclust,,], centre = obj$mn[tt,,iclust]) %&gt;% as_tibble() p = p + geom_path(aes(x = x, y = y), data = el, colour = mn_colours[iclust], lty = 2, linewidth = pmin(obj$prob[tt,iclust] * 8, 0.8)) ## Add mean p = p + geom_point(aes(x = dim1, y = dim2), data = mnmat %&gt;% subset(cluster == iclust) %&gt;% .[tt,], colour = mn_colours[iclust], ## size = rel(3)) size = obj$prob[tt,iclust] * 10) } } return(p) } ## This is from the many-cruises 02-helpers.R file; come back to this. if(FALSE){ p = datobj_2d %&gt;% ggplot() + theme_minimal() + geom_raster(aes(x = !!sym(varname1), y=!!sym(varname2), fill = counts)) + scale_fill_gradientn(colours = colours, guide=&quot;colorbar&quot;)+ xlim(c(0,8)) + ylim(c(0, 8)) + theme(legend.position = &quot;none&quot;) } Let’s try it out. datobj = gendat_2d(2, c(1000,1000)) plot_2d(datobj$ylist, tt=1, bin = FALSE) 2.3 3d data Given 3d data ylist and an estimated model object obj, we want to plot both in three two-dimensional plot. plot_3d() lets you do this. 2.3.1 Plotting 3d data First, here’s a plotting helper. #&#39; Combine list of ggplots #&#39; @export my_mfrow &lt;- function(glist, ncol = NULL, nrow = NULL){ if(is.null(ncol)) ncol = 1 if(is.null(nrow)) ncol = 3 do.call(ggpubr::ggarrange, c(glist, ncol=ncol, nrow=nrow)) } (TODO: change tt to time (and all the user-facing functions should have the same arguments and argument order)) #&#39; Makes three 2-dimensional plots of the 3d data #&#39; #&#39; @param ylist Data. A list of (|nt| by |dimdat|) matrices #&#39; @param countslist Count data. #&#39; @param obj flowmix or flowtrend object #&#39; @param tt Time point #&#39; @param return_list_of_plots If TRUE, return the list of three plots instead #&#39; of the combined plot #&#39; @export #&#39; @return #&#39; plot_3d &lt;- function(ylist, obj = NULL, tt, countslist = NULL, mn_colours = NULL, labels = NULL, bin = TRUE, plot_title = NULL, return_list_of_plots = FALSE){ ## Basic checks stopifnot(ncol(ylist[[1]]) == 3) if(!is.null(labels)) assertthat::assert_that(length(labels) == obj$numclust) ## if(!bin) stop(&quot;This function is only for binned 3d data!&quot;) ## Extract data ## y = ylist[[tt]][,dims] labs = colnames(ylist[[1]]) if(is.null(labs)) labs = paste0(&quot;dim&quot;, 1:3) if(is.null(countslist)){ counts = rep(1, nrow(ylist[[tt]])) } if(!is.null(countslist)){ counts = countslist[[tt]] } ## Aggregate counts into the two dimensions y2d_list = list() counts2d_list = list() for(ii in 1:3){ dims = list(c(1:2), c(2:3), c(3,1))[[ii]] if(bin){ yy = flowmix::collapse_3d_to_2d(ylist[[tt]], counts, dims) y2d = yy[,1:2] colnames(y2d) = labs[dims] one_counts = yy[,3] } else { y2d = ylist[[tt]][,dims] colnames(y2d) = labs[dims] one_counts = counts } y2d_list[[ii]] = y2d counts2d_list[[ii]] = one_counts } total_range = sapply(counts2d_list, range) %&gt;% range() ## Create three 2d plots plotlist = list() for(ii in 1:3){ dims = list(c(1:2), c(2:3), c(3,1))[[ii]] ## Make data plot ##one_countslist = (if(!is.null(countslist)) list(counts2d_list[[ii]]) else NULL) p = flowtrend::plot_2d(list(y2d_list[[ii]]), list(counts2d_list[[ii]]), obj = NULL, tt = 1, bin = bin) if(bin){ p$scales$scales &lt;- list() p = p + scale_fill_gradientn( colors = c(&quot;white&quot;, &quot;blue&quot;, &quot;yellow&quot;), limits = total_range, guide = &quot;none&quot;) } ## Adding visualizations of the model |obj| at time tt if(!is.null(obj)){ if(is.null(mn_colours)){ mn_colours = rep(&quot;red&quot;, obj$numclust) } ## Make data matrix mnlist = lapply(1:obj$numclust, function(iclust){ one_mnmat = obj$mn[tt,dims,iclust] %&gt;% t() colnames(one_mnmat) = paste0(&quot;dim&quot;, 1:2) one_mnmat %&gt;% as_tibble() %&gt;% add_column(cluster = iclust) }) mnmat = do.call(rbind, mnlist) for(iclust in 1:obj$numclust){ ## Add ellipse el = ellipse::ellipse(x = obj$sigma[iclust,dims,dims], centre = obj$mn[tt,dims,iclust]) %&gt;% as_tibble() p = p + geom_path(aes(x = x, y = y), data = el, colour = mn_colours[iclust], lty = 2, linewidth = pmin(obj$prob[tt,iclust] * 8, 0.8)) ## Add mean p = p + geom_point(aes(x = dim1, y = dim2), data = mnmat %&gt;% subset(cluster == iclust), colour = mn_colours[iclust], size = obj$prob[tt,iclust] * 10) ## Add cluster number as a label cex = rel(3) fac = 10 if(is.null(labels)){ labels = 1:(obj$numclust) } dt = data.frame(dim1 = mnmat[,1], dim2 = mnmat[,2], prob = obj$prob[tt,] * fac) p = p + ggrepel::geom_text_repel(aes(x = dim1, y = dim2, label = labels, point.size = sqrt(prob)), col = mn_colours, cex = cex, bg.color = &quot;white&quot;, bg.r = 0.1, fontface = &quot;bold&quot;, force_pull = 5, # do not pull toward data points ## data = mnmat, data = dt, seed = 1) } } ## Format a bit more and save if(ii == 1){ if(is.null(plot_title)) plot_title = paste0(&quot;Time=&quot;, tt) p = p + ggtitle(plot_title) } else { p = p + ggtitle(&quot;&quot;) } p = p + theme(legend.position = &quot;none&quot;) plotlist[[ii]] = p } p_combined = my_mfrow(plotlist) ## Return the plots if(return_list_of_plots) return(plotlist) return(p_combined) } (This is test code.) litr::load_all(&quot;~/repos/flowtrend/index.Rmd&quot;) load(&quot;~/repos/flowmix/paper-data/MGL1704-hourly-only-binned.Rdata&quot;, verbose=TRUE) load(&quot;~/repos/flowmix/paper-data/cvres-2-64-10.Rdata&quot;, verbose=TRUE) ## Shift/scale data to make it equivalent to what the model used for fitting. range.diam = c(-0.6706111, 1.3070264) range.chl = c(0.101313, 7.955767) range.pe = c(0.101313, 7.955767) ybin_list = lapply(ybin_list, function(y){ y[,1] = y[,1] - min(range.diam) y[,1] = y[,1] / width.diam y[,1] = y[,1] * max(range.chl) return(y) }) ## For back-compatibility res = cvres$bestres res$prob = res$pie ## Make the plot plot_3d(ylist = ybin_list, countslist = biomass_list, obj = res, tt = 280) 2.3.2 Generating 3d data #&#39; Generates some synthetic 3-dimensional data with three clusters. #&#39; #&#39; @param TT Number of time points. #&#39; @param nt Number of particles at each time. #&#39; #&#39; @return List containing (1) ylist, (2) mnlist, (3) clusterlist. #&#39; @export gendat_3d &lt;- function(TT, ntlist){ ## Basic checks stopifnot(length(ntlist) == TT) ## Make cluster probabilities, by time cluster_prob1 = sapply(1:TT, function(tt) sin(tt/24 * 2 * pi)/3 + 1 + (tt/TT)*5) cluster_prob2 = sapply(1:TT, function(tt) cos(tt/24 * 2 * pi)/3 + 8 - (tt/TT)*5) cluster_prob3 = rep(3, TT) probs = cbind(cluster_prob1, cluster_prob2, cluster_prob3) probs = probs/rowSums(probs) colnames(probs) = 1:3 probs_long = as_tibble(probs) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) probs_long %&gt;% ggplot() + geom_line(aes(x=time,y=prob, group=cluster, col=cluster)) + ylim(c(0,1)) ## Make cluster means, by time numclust = 3 dimdat = 3 ## means &lt;- array(NA, dim = c(TT, 3, 2)) means &lt;- array(NA, dim = c(TT, numclust, dimdat)) for(ii in 1:3){ for(tt in 1:TT){ ## First cluster means[tt, 1, 1] = means[tt, 1, 2] = means[tt, 1, 3] = tt/TT + 0.5 ## Second cluster means[tt, 2, 1] = sin(tt/24 * 2 * pi)##seq(-1, 1, length.out = TT)[tt]*3.1415) means[tt, 2, 2] = 0 means[tt, 2, 3] = 0 ## Third cluster means[tt, 3, 1] = means[tt, 3, 2] = means[tt, 3, 3] = -3+cos(tt/24 * 2 * pi)##seq(-1, 1, length.out = TT)[tt]*6.282) } } dimnames(means)[[2]] = c(1:3) means_long = as_tibble(means) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) model = full_join(means_long, probs_long, by = c(&quot;time&quot;, &quot;cluster&quot;)) ylist = list() mulist = list() clusterlist = list() for(tt in 1:TT){ Y &lt;- vector(mode = &quot;list&quot;, length = numclust) mu &lt;- vector(mode = &quot;list&quot;, length = numclust) clusters_count &lt;- rmultinom(n = 1, size = ntlist[tt], prob = probs[tt,]) for(ii in 1:3){ if(ii == 1){ mn = means[tt,ii,,drop=TRUE] Sigma1 = matrix(rep(0.3, 9), ncol = 3) diag(Sigma1) = 0.4 ## Sigma1 = matrix(c(0.4, 0.3, 0.3, 0.4), ncol = 2) Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0,0), Sigma= Sigma1)) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } if(ii == 2){ mn = means[tt,ii,, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0,0), Sigma = diag(c(0.5, 0.1, 0.2)))) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } if(ii == 3){ mn = means[tt,ii,, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0,0), Sigma = diag(c(0.35, 0.35, 0.35)))) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } } Y &lt;- Y %&gt;% purrr::compact() %&gt;% do.call(rbind, .) mu &lt;- mu %&gt;% purrr::compact() %&gt;% do.call(rbind, .) cluster &lt;- rep(1:3, times = clusters_count) colnames(Y) = paste0(&quot;dim&quot;, 1:3) ylist[[tt]] = Y mulist[[tt]] = mu clusterlist[[tt]] = cluster } return(list(ylist = ylist, mulist = mulist, clusterlist = clusterlist, probs = probs, means = means)) } Now, trying this out on some synthetic data and estimated model. datobj = gendat_3d(100, rep(100,100)) plot_3d(ylist = datobj$ylist, tt = 1) set.seed(2321) obj = flowtrend(ylist = datobj$ylist, l = 2, l_prob = 1, lambda = .01, lambda_prob = .01, numclust = 3, verbose = FALSE, maxdev = 3, niter = 50, nrestart = 3) plot_3d(ylist = datobj$ylist, tt = tt, obj = obj) "],["building-the-modelalgorithm.html", "3 Building the model/algorithm 3.1 Trend filtering 3.2 Objective (data log-likelihood) 3.3 Initial parameters for EM algorithm 3.4 E step 3.5 M step 3.6 The main “flowtrend” function", " 3 Building the model/algorithm 3.1 Trend filtering Trend filtering is a non-parametric regression technique for a sequence of output points \\(y = (y_1,..., y_T)\\) observed at locations \\(x = (x_1, ..., x_T)\\). It is usually assumed that \\(x_1, ..., x_T\\) are evenly spaced points, though this can be relaxed. The trend filtering estimate of order \\(l\\) of the time series \\(\\mu_t = \\mathbb{E}(y_t), t \\in x\\) is obtained by calculating \\[\\hat{\\mu} = \\mathop{\\mathrm{argmin}}_{\\mu \\in \\mathbb{R}^T} \\frac{1}{2}\\| \\mu - y\\|_2^2 + \\lambda \\| D^{(l+1)} \\mu\\|_1,\\] where \\(\\lambda\\) is a tuning parameter and \\(D^{(l+1)} \\in \\mathbb{R}^{T-l}\\) is the \\((l+1)^\\text{th}\\) order discrete differencing matrix. We first need to be able to construct the trend filtering “differencing matrix” used for smoothing the mixture parameters over time. The general idea of the trend filtering is explained masterfully in [Ryan’s paper, Section 6 and equation (41)]. The differencing matrix is formed by recursion, starting with \\(D^{(1)}\\). \\[\\begin{equation*} D^{(1)} = \\begin{bmatrix} -1 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 &amp; \\cdots &amp; 0 &amp; 0\\\\ \\vdots &amp; &amp; &amp; &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; -1 &amp; 1 \\end{bmatrix}. \\end{equation*}\\] For \\(l&gt;1\\), the differencing matrox \\(D^{(l+1)}\\) is defined recursively as \\(D^{(l+1)} = D^{(1)} D^{(l)}\\), starting with \\(D^{(1)}\\). #&#39; Generating Difference Matrix of Specified Order #&#39; #&#39; @param n length of vector to be differenced #&#39; @param l order of differencing #&#39; @param x optional. Spacing of input points. #&#39; #&#39; @return A n by n-l-1 matrix #&#39; @export #&#39; #&#39; @examples gen_diff_mat &lt;- function(n, l, x = NULL){ ## Basic check if(!is.null(x)) stopifnot(length(x) == n) if(is.unsorted(x)) stop(&quot;x must be in increasing order!&quot;) get_D1 &lt;- function(t) {do.call(rbind, lapply(1:(t-1), FUN = function(x){ v &lt;- rep(0, t) v[x] &lt;- -1 v[x+1] &lt;- 1 v }))} if(is.null(x)){ if(l == 0){ return(diag(rep(1,n))) } if(l == 1){ return(get_D1(n)) } if(l &gt; 1){ D &lt;- get_D1(n) for(k in 1:(l-1)){ D &lt;- get_D1(n-k) %*% D } return(D) } } else{ if(l == 0){ return(diag(rep(1,n))) } if(l == 1){ return(get_D1(n)) } if(l &gt; 1){ D &lt;- get_D1(n) for(k in 1:(l-1)){ D1 = get_D1(n-k) facmat = diag(k / diff(x, lag = k)) D &lt;- D1 %*% facmat %*% D } return(D) } } } For equally spaced inputs with \\(l=1\\) and \\(l=2\\): l = 1 Dl = gen_diff_mat(n = 10, l = l+1, x = NULL) print(Dl) l = 2 Dl = gen_diff_mat(n = 10, l = l+1, x = NULL) print(Dl) When the inputs have a gap in it: ## See what a l=2 difference matrix looks like: x = (1:10)[-(3)] l = 2 TT = length(x) Dl = gen_diff_mat(n = TT, l = l+1, x = x) print(Dl) ## Formally test it d1 = gen_diff_mat(n = 10, l = l+1, x = 1:10) d2 = gen_diff_mat(n = 10, l = l+1, x = (1:10)*2) print(d1) print(d2) d1_d2_ratio = as.numeric(d1/d2) %&gt;% na.omit() %&gt;% as.numeric() testthat::expect_true(all(d1_d2_ratio==d1_d2_ratio[1])) ## correct A formal test for unevenly spaced inputs will come soon, once we’ve defined gen_tf_mat, next. We now build a function to build a lasso regressor matrix \\(H\\) that can be used to solve an equivalent problem as the trend filtering of the l’th degree. (This is stated in Lemma 4, equation (25) from Tibshirani (2014)) #&#39; A lasso regressor matrix H that can be used to solve an equivalent problem as the trend filtering of the \\code{k}&#39;th degree. #&#39; #&#39; @param n Total number of time points. #&#39; @param k Degree of trend filtering for cluster probabilities. $k=0$ is fused lasso, $k=1$ is linear trend filtering, and so on. #&#39; @param x Time points #&#39; #&#39; @return $n$ by $n$ matrix. #&#39; #&#39; @export gen_tf_mat &lt;- function(n, k, x = NULL){ if(is.null(x) ){ x = (1:n)/n } if(!is.null(x)){ stopifnot(length(x) == n) } ## For every i,j&#39;th entry, use this helper function (from eq 25 of Tibshirani ## (2014)). gen_ij &lt;- function(x, i, j, k){ xi &lt;- x[i] if(j %in% 1:(k+1)){ return(xi^(j-1)) } if(j %in% (k+2):n){ ## Special handling for k==0, See lemma 4 eq 25 if(k == 0){ prd = 1 ind = j } if(k &gt;= 1){ ind = j - (1:k) prd = prod(xi - x[ind]) } return(prd * ifelse(xi &gt;= x[max(ind)], 1, 0)) ## if(k &gt;= 1) prd = prod(xi - x[(j-k):(j-1)]) ## return(prd * ifelse(xi &gt;= x[(j-1)], 1, 0)) } } ## Generate the H matrix, entry-wise. H &lt;- matrix(nrow = n, ncol = n) for(i in 1:n){ for(j in 1:n){ H[i,j] &lt;- gen_ij(x, i,j, k) } } return(H) } Here’s a simple test of gen_tf_mat(), against an alternative function built for equally spaced data. #&#39; Creates trendfiltering regression matrix using Lemma 2 of Ryan Tibshirani&#39;s #&#39; trendfilter paper (2014); works on equally spaced data only. #&#39; #&#39; @param n Number of data points #&#39; @param k Order of trend filter. 0 is fused lasso, and so on. #&#39; @examples #&#39; ord = 1 #&#39; H_tf &lt;- gen_tf_mat_equalspace(n = 100, k = ord) #&#39; H_tf[,1] * 100 #&#39; H_tf[,2] * 100 #&#39; H_tf[,3] * 100 #&#39; H_tf[,4] * 100 #&#39; H_tf[,99] * 100 #&#39; H_tf[,100] * 100 #&#39; @return n by n matrix. gen_tf_mat_equalspace &lt;- function(n, k){ nn = n kk = k ##&#39; Connects kk to kk-1. sigm &lt;- function(ii, kk){ if(kk == 0) return(rep(1, ii)) cumsum(sigm(ii, kk-1)) } mat = matrix(NA, ncol = nn, nrow = nn) for(ii in 1:nn){ for(jj in 1:nn){ if(jj &lt;= kk+1) mat[ii,jj] = ii^(jj-1) / nn^(jj-1) if(ii &lt;= jj-1 &amp; jj &gt;= kk+2) mat[ii, jj] = 0 if(ii &gt; jj-1 &amp; jj &gt;= kk+2){ mat[ii, jj] = (sigm(ii-jj+1, kk) %&gt;% tail(1)) * factorial(kk) / nn^kk } } } return(mat) } testthat::test_that(&quot;Trend filtering regression matrix is created correctly on equally spaced data.&quot;, { ## Check that equally spaced data creates same trendfilter regression matrix ## Degree 1 H1 &lt;- gen_tf_mat(10, 1) H1_other &lt;- gen_tf_mat(10, 1, x=(1:10)/10) testthat::expect_equal(H1, H1_other) ## Degree 1 H2 &lt;- gen_tf_mat(10, 2) H2_other &lt;- gen_tf_mat(10, 2, x=(1:10)/10) testthat::expect_equal(H2, H2_other) ## Check the dimension testthat::expect_equal(dim(H1), c(10,10)) ## Check against an alternative function. for(ord in c(0,1,2,3,4)){ H &lt;- gen_tf_mat(10, ord) H_eq = gen_tf_mat_equalspace(10, ord) testthat::expect_true(max(abs(H_eq- H)) &lt; 1E-10) } }) Finally, let’s test the difference matrix \\(D\\) formed using unevenly spaced \\(x\\)’s. testthat::test_that(&quot;Uneven spaced D matrix is formed correctly&quot;, { x = (1:6)[-(2)] ## k = 0 is piecewise constant, k=1 is piecewise linear, etc. for(k in 0:3){ l = k+1 ## Form Dl using our function Dl &lt;- flowtrend::gen_diff_mat(n=5, l = l, x=x) ## Compare it to rows of H matrix, per section 6 of Tibshirani et al. 2014 gen_tf_mat(n = length(x), k = k, x = x) %&gt;% solve() %&gt;% `*`(factorial(k)) %&gt;% ## This part is missing in Tibshirani et al. 2014 tail(length(x)-(k+1)) -&gt; Hx ratiomat = Hx/Dl ## Process the ratios of each entry, and check that they&#39;re equal to 1 ratios = ratiomat[!is.nan(ratiomat)] ratios = ratios[is.finite(ratios)] testthat::expect_true(all.equal(ratios, rep(1, length(ratios)))) } }) 3.2 Objective (data log-likelihood) The function loglik_tt() calculates the log-likelihood of one cytogram, which is: \\[\\sum_{i=1}^{n_t} C_i^{(t)} \\log\\left( \\sum_{k=1}^K \\pi_{itk} \\phi(y_i^{(t)}; \\mu_{kt}, \\Sigma_k) \\right). \\] This is the portion of the objective function of the flowtrend model. #&#39; Log likelihood for a single time point&#39;s data. #&#39; #&#39; @param mu Cluster means. #&#39; @param prob Cluster probabilities. #&#39; @param sigma Cluster variances. #&#39; @param ylist Data. #&#39; @param tt Time point. loglik_tt &lt;- function(ylist, tt, mu, sigma, prob, dimdat = NULL, countslist = NULL, numclust){ ## One particle&#39;s log likelihood (weighted density) weighted.densities = sapply(1:numclust, function(iclust){ if(dimdat == 1){ return(prob[tt,iclust] * dnorm(ylist[[tt]], mu[tt,,iclust], sqrt(sigma[iclust,,]))) } if(dimdat &gt; 1){ return(prob[tt,iclust] * dmvnorm_arma_fast(ylist[[tt]], mu[tt,,iclust], as.matrix(sigma[iclust,,]), FALSE)) } }) nt = nrow(ylist[[tt]]) counts = (if(!is.null(countslist)) countslist[[tt]] else rep(1, nt)) sum_wt_dens = rowSums(weighted.densities) sum_wt_dens = sum_wt_dens %&gt;% pmax(1E-100) return(sum(log(sum_wt_dens) * counts)) } Next, here is the function that calculates the entire objective from all cytograms, given model parameter mu, prob, and sigma: \\[\\sum_{t=1}^T\\sum_{i=1}^{n_t} C_i^{(t)} \\log\\left( \\sum_{k=1}^K \\pi_{itk} \\phi(y_i^{(t)}; \\mu_{kt}, \\Sigma_k) \\right). \\] #&#39; Evaluating the penalized negative log-likelihood on all data \\code{ylist} given parameters \\code{mu}, \\code{prob}, and \\code{sigma}. #&#39; #&#39; @param mu #&#39; @param prob #&#39; @param prob_link #&#39; @param sigma #&#39; @param ylist #&#39; @param Dl #&#39; @param l #&#39; @param lambda #&#39; @param l_prob #&#39; @param Dl_prob #&#39; @param lambda_prob #&#39; @param alpha #&#39; @param beta #&#39; @param denslist_by_clust #&#39; @param countslist #&#39; @param unpenalized if TRUE, return the unpenalized out-of-sample fit. #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples objective &lt;- function(mu, prob, prob_link = NULL, sigma, ylist, Dlp1 = NULL, l = NULL, lambda = 0, l_prob = NULL, Dlp1_prob = NULL, lambda_prob = 0, alpha = NULL, beta = NULL, denslist_by_clust = NULL, countslist = NULL, unpenalized = FALSE){ ## Set some important variables TT = dim(mu)[1] numclust = dim(mu)[3] if(is.null(countslist)){ ntlist = sapply(ylist, nrow) } else { ntlist = sapply(countslist, sum) } N = sum(ntlist) dimdat = ncol(ylist[[1]]) ## Calculate the log likelihood loglik = sapply(1:TT, function(tt){ if(is.null(denslist_by_clust)){ return(loglik_tt(ylist, tt, mu, sigma, prob, countslist, numclust = numclust, dimdat = dimdat)) } else { ## TODO: This function doesn&#39;t exist yet, but might need to, since.. speed! return(loglik_tt_precalculate(ylist, tt, denslist_by_clust, prob, countslist, numclust)) } }) if(unpenalized){ obj = -1/N * sum(unlist(loglik)) return(obj) } else { ## Return penalized likelihood mu.splt &lt;- asplit(mu, MARGIN = 2) ## This was 3, which I think produces the same result. diff_mu &lt;- sum(unlist(lapply(mu.splt, FUN = function(m) sum(abs(Dlp1 %*% m))))) diff_prob &lt;- sum(abs(Dlp1_prob %*% prob_link)) obj = -1/N * sum(unlist(loglik)) + lambda * diff_mu + lambda_prob * diff_prob return(obj) } } Here’s a helper to check numerical convergence of the EM algorithm. #&#39; Checks numerical improvement in objective value. Returns TRUE if |old-new|/|old| is smaller than tol. #&#39; #&#39; @param old Objective value from previous iteration. #&#39; @param new Objective value from current iteration. #&#39; @param tol Numerical tolerance. check_converge_rel &lt;- function(old, new, tol=1E-6){ return(abs((old-new)/old) &lt; tol ) } Here’s also a helper function to do the softmax-ing of \\(\\alpha_t \\in \\mathbb{R}^K\\). #&#39; Softmax function. #&#39; #&#39; @param prob_link alpha, which is a (T x K) matrix. #&#39; #&#39; @return exp(alpha)/rowSum(exp(alpha)). A (T x K) matrix. softmax &lt;- function(prob_link){ exp_prob_link = exp(prob_link) prob = exp_prob_link / rowSums(exp_prob_link) } testthat::test_that(&quot;Test for softmax&quot;,{ link = runif(100, min = -10, max = 10) %&gt;% matrix(nrow = 10, ncol = 10) testthat::expect_true(all(abs(rowSums(softmax(link)) - 1) &lt; 1E-13)) }) 3.3 Initial parameters for EM algorithm The EM algorithm requires some initial values for \\(\\mu\\), \\(\\pi\\) and \\(\\Sigma\\). Initializing \\(\\pi\\) is done in one line, prob = matrix(1/numclust, nrow = TT, ncol = numclust), which sets everything to \\(1/K\\). For \\(\\mu\\) and \\(\\Sigma\\), we write some functions. Essentially, initial means \\(\\mu\\) are jittered versions of a \\(K\\) means that are drawn from a downsampled version of \\(ylist\\) (downsampling is done because \\(ylist\\) can have a large number of particles). \\(\\Sigma\\) is \\(d\\times d\\) identity matrices, with fac=1 diagonal by default. #&#39; Initialize the cluster centers. #&#39; #&#39; @param ylist A T-length list of (nt by 3) datasets. There should be T of #&#39; such datasets. 3 is actually \\code{mulen}. #&#39; @param numclust Number of clusters (M). #&#39; @param TT total number of (training) time points. #&#39; #&#39; @return An array of dimension (T x dimdat x M). #&#39; @export init_mn &lt;- function(ylist, numclust, TT, dimdat, countslist = NULL, seed=NULL){ if(!is.null(seed)){ assertthat::assert_that(all((seed %&gt;% sapply(., class)) == &quot;integer&quot;)) assertthat::assert_that(length(seed) == 7) RNGkind(&quot;L&#39;Ecuyer-CMRG&quot;) .Random.seed &lt;&lt;- seed } if(is.null(countslist)){ ntlist = sapply(ylist, nrow) countslist = lapply(ntlist, FUN = function(nt) rep(1, nt)) } ## Initialize the means by (1) collapsing to one cytogram (2) random ## sampling from this distribution, after truncation, TT = length(ylist) ylist_downsampled &lt;- lapply(1:TT, function(tt){ y = ylist[[tt]] counts = countslist[[tt]] ## Sample so that, in total, we get mean(nt) * 30 sized sample. In the case ## of binned data, nt is the number of bins. if(nrow(y) &gt; 500) nsize = pmin(nrow(y) / TT * 30, nrow(y)) else nsize = nrow(y) some_rows = sample(1:nrow(y), size = nsize, prob = counts/sum(counts)) y[some_rows,, drop=FALSE] }) ## Jitter the means a bit yy = do.call(rbind, ylist_downsampled) new_means = yy[sample(1:nrow(yy), numclust),, drop=FALSE] jitter_sd = apply(yy, 2, sd) / 100 jitter_means = MASS::mvrnorm(n = nrow(new_means), mu = rep(0, dimdat), Sigma = diag(jitter_sd, ncol = dimdat)) new_means = new_means + jitter_means ## Repeat TT times == flat/constant initial means across time. mulist = lapply(1:TT, function(tt){ new_means }) ## } else { ## TT = length(ylist) ## ylist_downsampled &lt;- lapply(1:TT, function(tt){ ## y = ylist[[tt]] ## counts = countslist[[tt]] ## nsize = pmin(nrow(y) / TT * 30, nrow(y)) ## y[sample(1:nrow(y), size = nsize),, drop=FALSE] ## }) ## ## Combine all the particles ## yy = do.call(rbind, ylist_downsampled) ## ## Get K new means from these ## inds = sample(1:nrow(yy), numclust) ## new_means = yy[inds,, drop=FALSE] ## mulist = lapply(1:TT, function(tt){ new_means }) ## } ## New (T x dimdat x numclust) array is created. muarray = array(NA, dim=c(TT, dimdat, numclust)) for(tt in 1:TT){ muarray[tt,,] = as.matrix(mulist[[tt]]) } return(muarray) } #&#39; Initialize the covariances. #&#39; #&#39; @param data The (nt by 3) datasets. There should be T of them. #&#39; @param numclust Number of clusters. #&#39; @param fac Value to use for the diagonal of the (dimdat x dimdat) covariance #&#39; matrix. #&#39; #&#39; @return An (K x dimdat x dimdat) array containing the (dimdat by dimdat) #&#39; covariances. #&#39; @export init_sigma &lt;- function(data, numclust, fac = 1){ ndat = nrow(data[[1]]) pdat = ncol(data[[1]]) sigmas = lapply(1:numclust, function(iclust){ onesigma = diag(fac * rep(1, pdat)) if(pdat==1) onesigma = as.matrix(fac) colnames(onesigma) = paste0(&quot;datcol&quot;, 1:pdat) rownames(onesigma) = paste0(&quot;datcol&quot;, 1:pdat) return(onesigma) }) sigmas = abind::abind(sigmas, along=0) return(sigmas) } Here’s a helper function for printing the progress. #&#39; A helper function to print the progress of a loop or simulation. #&#39; #&#39; @param isim Replicate number. #&#39; @param nsim Total number of replicates. #&#39; @param type Type of job you&#39;re running. Defaults to &quot;simulation&quot;. #&#39; @param lapsetime Lapsed time, in seconds (by default). #&#39; @param lapsetimeunit &quot;second&quot;. #&#39; @param start.time start time. #&#39; @param fill Whether or not to fill the line. #&#39; #&#39; @return No return print_progress &lt;- function(isim, nsim, type = &quot;simulation&quot;, lapsetime = NULL, lapsetimeunit = &quot;seconds&quot;, start.time = NULL, fill = FALSE){ ## If lapse time is present, then use it if(fill) cat(fill = TRUE) if(is.null(lapsetime) &amp; is.null(start.time)){ cat(&quot;\\r&quot;, type, &quot; &quot;, isim, &quot;out of&quot;, nsim) } else { if(!is.null(start.time)){ lapsetime = round(difftime(Sys.time(), start.time, units = &quot;secs&quot;), 0) remainingtime = round(lapsetime * (nsim-isim)/isim,0) endtime = Sys.time() + remainingtime } cat(&quot;\\r&quot;, type, &quot; &quot;, isim, &quot;out of&quot;, nsim, &quot;with lapsed time&quot;, lapsetime, lapsetimeunit, &quot;and remaining time&quot;, remainingtime, lapsetimeunit, &quot;and will finish at&quot;, strftime(endtime)) } if(fill) cat(fill = TRUE) } 3.4 E step #&#39; E step, which updates the &quot;responsibilities&quot;, which are posterior membership probabilities of each particle. #&#39; #&#39; @param mn #&#39; @param sigma covariance #&#39; @param prob #&#39; @param ylist #&#39; @param numclust #&#39; @param denslist_by_clust #&#39; @param first_iter #&#39; @param countslist #&#39; #&#39; @return #&#39; @export #&#39; Estep &lt;- function(mn, sigma, prob, ylist = NULL, numclust, denslist_by_clust = NULL, first_iter = FALSE, countslist = NULL, padding = 1E-20){ ## Basic setup TT = length(ylist) ntlist = sapply(ylist, nrow) resp = list() dimdat = dim(mn)[2] assertthat::assert_that(dim(mn)[1] == length(ylist)) ## Helper to calculate Gaussian density for each \\code{N(y_{t,k},mu_{t,k} and ## Sigma_k)}. calculate_dens &lt;- function(iclust, tt, y, mn, sigma, denslist_by_clust, first_iter) { mu &lt;- mn[tt, , iclust] if (dimdat == 1) { dens = dnorm(y, mu, sd = sqrt(sigma[iclust, , ])) } else { dens = dmvnorm_arma_fast(y, mu, sigma[iclust,,], FALSE) } return(dens) } ## Calculate posterior probability of membership of $y_{it}$. ncol.prob = ncol(prob) for (tt in 1:TT) { ylist_tt = ylist[[tt]] densmat &lt;- sapply(1:numclust, calculate_dens, tt, ylist_tt, mn, sigma, denslist_by_clust, first_iter) wt.densmat &lt;- matrix(prob[tt, ], nrow = ntlist[tt], ncol = ncol.prob, byrow = TRUE) * densmat wt.densmat = wt.densmat + padding##1e-20 wt.densmat &lt;- wt.densmat/rowSums(wt.densmat) resp[[tt]] &lt;- wt.densmat } ## Weight the responsibilities by $C_{it}$. if (!is.null(countslist)) { resp &lt;- Map(function(myresp, mycount) { myresp * mycount }, resp, countslist) } return(resp) } The E step should return a list of exactly the same size and format as ylist, which is a \\(T\\) -length list of matrices of size \\(n_t \\times d\\). (This test fails for some reason; will come back to this.) testthat::test_that(&quot;E step returns appropriately sized responsibilities.&quot;,{ ## Generate some fake data TT = 10 ylist = lapply(1:TT, function(tt){ runif(90) %&gt;% matrix(ncol = 3, nrow = 30)}) numclust = 3 dimdat = 3 ## Initialize a few parameters, not carefully sigma = init_sigma(ylist, numclust) ## (T x numclust x (dimdat x dimdat)) mn = init_mn(ylist, numclust, TT, dimdat)##, countslist = countslist) prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. ## ## Calculate responsibility ## resp = Estep(mn = mn, sigma = sigma, prob = prob, ylist = ylist, numclust = numclust) ## Check these things ## testthat::expect_equal(length(resp), length(ylist)) ## ?testthat::expect_equal ## testthat::expect_equal(sapply(resp, dim), sapply(ylist, dim), reporter = &quot;stop&quot;) }) 3.5 M step The M step of the EM algorithm has three steps – one each for \\(\\mu\\), \\(\\pi\\), and \\(\\Sigma\\). 3.5.1 M step for \\(\\pi\\) #&#39; The M step for the cluster probabilities #&#39; #&#39; @param resp Responsibilities. #&#39; @param H_tf Trend filtering matrix. #&#39; @param countslist Particle multiplicities. #&#39; @param lambda_prob Regularization. #&#39; @param l_prob Trend filtering degree. #&#39; #&#39; @return (T x k) matrix containing the alphas, for \\code{prob = exp(alpha)/ #&#39; rowSums(exp(alpha))}. #&#39; @export #&#39; Mstep_prob &lt;- function(resp, H_tf, countslist = NULL, lambda_prob = NULL, l_prob = NULL, x = NULL){ ## Basic setup TT &lt;- length(resp) ## Basic checks stopifnot(is.null(l_prob) == is.null(lambda_prob)) ## If glmnet isn&#39;t actually needed, don&#39;t use it. if(is.null(l_prob) &amp; is.null(lambda_prob)){ ## Calculate the average responsibilities, per time point. if(is.null(countslist)){ resp.avg &lt;- lapply(resp, colMeans) %&gt;% do.call(rbind, .) } else { resp.avg &lt;- lapply(1:TT, FUN = function(ii){ colSums(resp[[ii]])/sum(countslist[[ii]]) }) %&gt;% do.call(rbind, .) } return(resp.avg) ## If glmnet is needed, use it. } else { lambda_range &lt;- function(lam, nlam = 50, lam.max = max(1, 5*lam)){ return(exp(seq(log(lam.max), log(lam), length.out = nlam))) } penalty.facs &lt;- c(rep(0, l_prob+1), rep(1, nrow(H_tf) - l_prob - 1)) resp.predict &lt;- do.call(rbind, lapply(resp, colSums)) glmnet_obj &lt;- glmnet::glmnet(x = H_tf, y = resp.predict, family = &quot;multinomial&quot;, penalty.factor = penalty.facs, maxit = 1e8, ## penalty.factor = penalty.facs, maxit = 1e7, lambda = mean(penalty.facs)*lambda_range(lambda_prob), standardize = F, intercept = FALSE) pred_link &lt;- predict(glmnet_obj, newx = H_tf, type = &quot;link&quot;, s = mean(penalty.facs) * lambda_prob)[,,1] return(pred_link) } } This should return a \\(T\\) by \\(K\\) matrix, which we’ll test here: testthat::test_that(&quot;Mstep of pi returns a (T x K) matrix.&quot;, { ## Generate some fake responsibilities and trend filtering matrix TT = 100 numclust = 3 nt = 10 resp = lapply(1:TT, function(tt){ oneresp = runif(nt*numclust) %&gt;% matrix(ncol=numclust) oneresp = oneresp/rowSums(oneresp) }) H_tf &lt;- gen_tf_mat(n = TT, k = 0) ## Check the size pred_link = Mstep_prob(resp, H_tf, l_prob = 0, lambda_prob = 1E-3) testthat::expect_equal(dim(pred_link), c(TT, numclust)) pred_link = Mstep_prob(resp, H_tf) testthat::expect_equal(dim(pred_link), c(TT, numclust)) ## Check the correctness pred_link = Mstep_prob(resp, H_tf) }) Each row of this matrix should contain the fitted values \\(\\alpha_k \\in \\mathbb{R}^3\\) where \\(\\alpha_{kt} = h_t^T w_{k}\\), for.. \\(h_t\\) that are rows of the trend filtering matrix \\(H \\in \\mathbb{R}^{T \\times T}\\). \\(w_k \\in \\mathbb{R}^{n}\\) that are the regression coefficients estimated by glmnet(). Here is a test for the correctness of the M step for \\(\\pi\\). testthat::test_that(&quot;Test the M step of \\pi against CVXR&quot;, {}) 3.5.2 M step for \\(\\Sigma\\) #&#39; M step for cluster covariance (sigma). #&#39; #&#39; @param resp Responsibility. #&#39; @param ylist Data. #&#39; @param mn Means #&#39; @param numclust Number of clusters. #&#39; #&#39; @return (K x d x d) array containing K (d x d) covariance matrices. #&#39; @export #&#39; #&#39; @examples Mstep_sigma &lt;- function(resp, ylist, mn, numclust){ ## Find some sizes TT = length(ylist) ntlist = sapply(ylist, nrow) dimdat = ncol(ylist[[1]]) cs = c(0, cumsum(ntlist)) ## Set up empty residual matrix (to be reused) cs = c(0, cumsum(ntlist)) vars &lt;- vector(mode = &quot;list&quot;, numclust) ylong = do.call(rbind, ylist) ntlist = sapply(ylist, nrow) irows = rep(1:nrow(mn), times = ntlist) for(iclust in 1:numclust){ resp.thisclust = lapply(resp, function(myresp) myresp[,iclust, drop = TRUE]) resp.long = do.call(c, resp.thisclust) mnlong = mn[irows,,iclust] if(is.vector(mnlong)) mnlong = mnlong %&gt;% cbind() resid &lt;- ylong - mnlong resid_weighted &lt;- resp.long * resid sig_temp &lt;- t(resid_weighted) %*% resid/sum(resp.long) vars[[iclust]] &lt;- sig_temp } ## Make into an array sigma_array = array(NA, dim=c(numclust, dimdat, dimdat)) for(iclust in 1:numclust){ sigma_array[iclust,,] = vars[[iclust]] } ## Basic check stopifnot(all(dim(sigma_array) == c(numclust, dimdat, dimdat))) return(sigma_array) } 3.5.3 M step for \\(\\mu\\) This is a big one. It uses the ADMM algorithm as written in section OO (TODO: fill in) of the paper, reproduced briefly here. We need a convergence checker for the outer layer of LA-ADMM, which checks whether the objective values have plateaued. Next, we define the main function Mstep_mu(). This uses a “locally-adaptive” ADMM paper. M-step_mu() calls la_admm_oneclust() on each cluster \\(k=1,\\cdots, K\\); this function will be introduced shortly. #&#39; Computes the M step for mu. TODO: use templates for the argument. As shown #&#39; here: #&#39; https://stackoverflow.com/questions/15100129/using-roxygen2-template-tags #&#39; #&#39; @param resp Responsbilities of each particle. #&#39; @param ylist #&#39; @param lambda #&#39; @param l #&#39; @param sigma #&#39; @param sigma_eig_by_clust #&#39; @param Dl #&#39; @param Dlp1 #&#39; @param TT #&#39; @param N #&#39; @param dimdat #&#39; @param first_iter #&#39; @param mus #&#39; @param Zs #&#39; @param Ws #&#39; @param uws #&#39; @param uzs #&#39; @param maxdev #&#39; @param x #&#39; @param niter #&#39; @param err_rel #&#39; @param err_abs #&#39; @param zerothresh #&#39; @param local_adapt #&#39; @param local_adapt_niter #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples Mstep_mu &lt;- function(resp, ylist, lambda = 0.5, l = 3, sigma, sigma_eig_by_clust = NULL, Dlsqrd, Dl, tDl, Dlp1, TT, N, dimdat, first_iter = TRUE, e_mat, ## Warm startable variables mus = NULL, Zs = NULL, Ws = NULL, uws = NULL, uzs = NULL, ## End of warm startable variables maxdev = NULL, x = NULL, niter = (if(local_adapt) 1e2 else 1e3), err_rel = 1E-3, err_abs = 0, zerothresh = 1E-6, local_adapt = FALSE, local_adapt_niter = 10, rho_init = 0.01, iter = NULL){ #################### ## Preliminaries ### #################### TT = length(ylist) numclust = ncol(resp[[1]]) dimdat = ncol(ylist[[1]]) ntlist = sapply(ylist, nrow) resp.sum = lapply(resp, colSums) %&gt;% do.call(rbind, .) N = sum(unlist(resp.sum)) ## Other preliminaries schur_syl_A_by_clust = schur_syl_B_by_clust = term3list = list() ybarlist = list() ycentered_list = Xcentered_list = yXcentered_list = list() Qlist = list() sigmainv_list = list() for(iclust in 1:numclust){ ## Retrieve sigma inverse from pre-computed SVD, if necessary if(is.null(sigma_eig_by_clust)){ sigmainv = solve(sigma[iclust,,]) } else { sigmainv = sigma_eig_by_clust[[iclust]]$sigma_inv } resp.iclust &lt;- lapply(resp, FUN = function(r) matrix(r[,iclust])) AB &lt;- get_AB_mats(y = y, resp = resp.iclust, Sigma_inv = sigmainv, e_mat = e_mat, N = N, Dlp1 = Dlp1, Dl = Dl, Dlsqrd = Dlsqrd, rho = rho_init, z = NULL, w = NULL, uz = NULL, uw = NULL) ## Store the Schur decomposition schur_syl_A_by_clust[[iclust]] = myschur(AB$A) schur_syl_B_by_clust[[iclust]] = myschur(AB$B) ycentered &lt;- NULL ycentered_list[[iclust]] = ycentered sigmainv_list[[iclust]] = sigmainv } ########################################## ## Run ADMM separately on each cluster ## ######################################### admm_niters = admm_inner_iters = vector(length = numclust, mode = &quot;list&quot;) if(first_iter) mus = vector(length = numclust, mode = &quot;list&quot;) # if(first_iter){ Zs &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT, ncol = dimdat)) Ws &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT - l, ncol = dimdat)) uzs &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT, ncol = dimdat)) uws &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT - l, ncol = dimdat)) # Zs = Ws = Us = vector(length = numclust, mode = &quot;list&quot;) # } ## For every cluster, run LA-ADMM start.time = Sys.time() for(iclust in 1:numclust){ resp.iclust &lt;- lapply(resp, FUN = function(r) matrix(r[,iclust])) ## Possibly locally adaptive ADMM, for now just running with rho == lambda ## if(iter == 3 &amp; iclust == 5) browser() res = la_admm_oneclust(K = (if(local_adapt) local_adapt_niter else 1), local_adapt = local_adapt, iclust = iclust, niter = niter, TT = TT, N = N, dimdat = dimdat, maxdev = maxdev, schurA = schur_syl_A_by_clust[[iclust]], schurB = schur_syl_B_by_clust[[iclust]], sigmainv = sigmainv_list[[iclust]], rho = rho_init, rhoinit = rho_init, ## rho = (if(iclust == 1) rho_init else res$rho/2), ## rhoinit = (if(iclust == 1) rho_init else res$rho/2), sigma = sigma, lambda = lambda, resp = resp.iclust, resp_sum = resp.sum[,iclust], l = l, Dlp1 = Dlp1, Dl = Dl, tDl = tDl, y = ylist, err_rel = err_rel, err_abs = err_abs, zerothresh = zerothresh, sigma_eig_by_clust = sigma_eig_by_clust, em_iter = iter, ## Warm starts from previous *EM* iteration first_iter = first_iter, ## mu = mus[[iclust]], ## I think we want this. uw = uws[[iclust]], uz = uzs[[iclust]], z = Zs[[iclust]], w = Ws[[iclust]]) ## print(res$rho) ## Store the results mus[[iclust]] = res$mu admm_niters[[iclust]] = res$kk admm_inner_iters[[iclust]] = res$inner.iter ## Store other things for for warmstart ## mus[[iclust]] = res$mu Zs[[iclust]] = res$Z uzs[[iclust]] = res$uz uws[[iclust]] = res$uw Ws[[iclust]] = res$W ## The upper triangular matrix remains the same. (code missing?) } ## Aggregate the yhats into one array mu_array = array(NA, dim = c(TT, dimdat, numclust)) for(iclust in 1:numclust){ mu_array[,,iclust] = mus[[iclust]] } ## Each are lists of length |numclust|. return(list(mns = mu_array, admm_niters = admm_niters, admm_inner_iters = admm_inner_iters, ## For warmstarts Zs = Zs, Ws = Ws, uws = uws, uzs = uzs, N = N, ## Return the column rho = res$rho, ## For using in the Sigma M step ycentered_list = ycentered_list, Xcentered_list = Xcentered_list, yXcentered_list = yXcentered_list, Qlist = Qlist )) } The locally adaptive admm for a single cluster involves an inner and outer loop. The inner loop is an ADMM for a fixed \\(\\rho\\). The outer loop is written here in la_admm_oneclust, and runs the inner ADMM with a fixed step-size \\(\\rho\\) while sequentially doubling \\(\\rho\\). #&#39; Locally adaptive ADMM. #&#39; #&#39; @param K #&#39; @param ... #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples la_admm_oneclust &lt;- function(K, ...){ ## Initialize arguments for ADMM. args &lt;- list(...) p = args$p l = args$l TT = args$TT dimdat = args$dimdat rhoinit = args$rhoinit ## This initialization can come from the previous *EM* iteration. if(args$first_iter){ mu = matrix(0, nrow = TT, ncol = dimdat) Z &lt;- matrix(0, nrow = TT, ncol = dimdat) W &lt;- matrix(0, nrow = TT-l, ncol = dimdat) uz &lt;- matrix(0, nrow = TT, ncol = dimdat) uw &lt;- matrix(0, nrow = TT - l, ncol = dimdat) args[[&#39;mu&#39;]] &lt;- mu args[[&#39;z&#39;]] &lt;- Z args[[&#39;w&#39;]] &lt;- W args[[&#39;uz&#39;]] &lt;- uz args[[&#39;uw&#39;]] &lt;- uw } cols = c() some_admm_objectives = c() ## Run ADMM repeatedly with (1) double rho, and (2) previous b for(kk in 1:K){ if(kk &gt; 1){ ## Z = matrix(0, nrow = TT, ncol = dimdat) ## W = matrix(0, nrow = TT - l , ncol = dimdat) ## uz = matrix(0, nrow = TT, ncol = dimdat) ## uw = matrix(0, nrow = TT - l , ncol = dimdat) ## These ensure warm starts are true args[[&#39;mu&#39;]] &lt;- mu args[[&#39;z&#39;]] &lt;- Z args[[&#39;w&#39;]] &lt;- W args[[&#39;uz&#39;]] &lt;- uz args[[&#39;uw&#39;]] &lt;- uw args[[&#39;rho&#39;]] &lt;- rho } ## Run ADMM args[[&#39;outer_iter&#39;]] &lt;- kk ## Call main function argn &lt;- lapply(names(args), as.name) names(argn) &lt;- names(args) call &lt;- as.call(c(list(as.name(&quot;admm_oneclust&quot;)), argn)) res = eval(call, args) if(any(abs(res$mu)&gt;1E2)){ stop(&quot;mu is blowing up! Probably because the initial ADMM step size (rho) is too large (and possibly the ball constraint on the means is large.&quot;) } some_admm_objectives = c(some_admm_objectives, res$single_admm_objective) ## Handling the scenario where the objectives are all zero padding = 1E-12 some_admm_objectives = some_admm_objectives + padding ## See if outer iterations should terminate if(res$converge){ res$converge &lt;- T break } ## Update some parameters; double the rho value, and update the B matrix rho = 2 * args$rho ## tQ = 2 * args$schurB$tQ ## This seems wrong. Delete now. mu = res$mu Z = res$Z W = res$W uz = res$uz uw = res$uw ## print(&quot;args$rho&quot;) ## print(args$rho) } if(!res$converge) warning(&quot;ADMM didn&#39;t converge for one cluster.&quot;) ## Record how long the admm took; in terms of # iterations. res$kk = kk ## Record the final rho res$rho = args$rho return(res) } Next, the inner loop. The workhorse admm_oneclust() actually performs the ADMM update for one cluster for a fixed step-size \\(\\rho\\) across optimization iterations iter=1,...,n. This admm_oneclust() uses the following helpers: W_update_fused(): this uses the prox function, which uses a RCpp function. prox_dp. Z_update() U_update_Z() U_update_W() A prox_dp function, written in C, will be used. #&#39; Solve a fused lasso problem for the W update. Internally, a fused lasso dynamic #&#39; programming solver \\code{prox()} (which calls \\code{prox_dp()} written in C) #&#39; is used. #&#39; W_update_fused &lt;- function(l, TT, mu, uw, rho, lambda, Dl){ # modified lambda for fused lasso routine mod_lam &lt;- lambda/rho # generating pseudo response xi if( l &lt; 0 ){ stop(&quot;l should never be /below/ zero!&quot;) } else if( l == 0 ){ xi &lt;- mu + 1/rho * uw ## This is faster } else { xi &lt;- Dl %*% mu + 1/rho * uw if(any(is.nan(xi))) browser() ## l = 2 is quadratic trend filtering ## l = 1 is linear trend filtering ## l = 0 is fused lasso ## D^{(1)} is first differences, so it correponds to l=0 ## Dl = gen_diff_mat(n = TT, l = l, x = x) &lt;--- (T-l) x T matrix } ## Running the fused LASSO ## which solves min_zhat 1/2 |z-zhat|_2^2 + lambda |D^{(1)}zhat| ## fit &lt;- prox(z = xi, lam = mod_lam) ## fit &lt;- prox_dp(z = xi, lam = mod_lam) ## instead of FlowTF::prox() ## fit &lt;- flowtrendprox::prox_dp(z = xi, lam = mod_lam) fit &lt;- FlowTF::prox(z = xi, lam = mod_lam) ## TODO: eventually change to fit &lt;- flowtrendprox::prox(z = xi, lam = mod_lam) return(fit) } ## This function is in FlowTF now. It&#39;s the last function there! ## #&#39; Fused LASSO for scalar inputs. ## #&#39; ## #&#39; @param z scalar input to be smoothed via the fused LASSO ## #&#39; @param lam Fused LASSO smoothing parameter ## #&#39; ## #&#39; @return Estimates of the fused LASSO solution ## #&#39; @export prox ## #&#39; ## #&#39; @references All credit for writing this function goes to Ryan Tibshirani. See ## #&#39; the original code for calling this function at ## #&#39; ## #&#39; @useDynLib FlowTF prox_dp ## prox &lt;- function(z, lam) { ## o &lt;- .C(&quot;prox_dp&quot;, ## as.integer(length(z)), ## as.double(z), ## as.double(lam), ## as.double(numeric(length(z))), ## # dup=FALSE, ## PACKAGE=&quot;FlowTF&quot;) ## return(o[[4]]) ## } Z_update &lt;- function(m, Uz, C, rho){ mat = m + Uz/rho Z = projCmat(mat, C) return(Z) } #&#39; Project /rows/ of matrix |mat| into a ball of size C. #&#39; #&#39; @param mat Matrix whose rows will be projected into a C-sized ball. #&#39; @param C radius #&#39; #&#39; @return Projected matrix. projCmat &lt;- function(mat, C){ if(!is.null(C)){ vlens = sqrt(rowSums(mat * mat)) inds = which(vlens &gt; C) if(length(inds) &gt; 0){ mat[inds,] = mat[inds,] * C / vlens[inds] } } return(mat) } We will test the projCmat function. set.seed(100) mat = matrix(rnorm(100), ncol=2) projected_mat = flowtrend:::projCmat(mat, 1) stopifnot(all(projected_mat %&gt;% apply(1, function(myrow)sum(myrow*myrow)) &lt; 1+1E-8)) #&#39; @param U (T x dimdat) matrix. U_update_Z &lt;- function(U, rho, mu, Z, TT){ # return(U + rho * (scale(mu, scale = F) - Z)) stopifnot(nrow(U) == TT) centered_mu = sweep(mu, 2, colMeans(mu)) ## stopifnot(all(abs(colMeans(centered_mu))&lt;1E-8)) Unew = U + rho * (centered_mu - Z) ## Expect a (T-l) x dimdat matrix. stopifnot(all(dim(U) == dim(Unew))) stopifnot(nrow(U) == TT) return(Unew) } #&#39; @param U ((T-l) x dimdat) matrix. U_update_W &lt;- function(U, rho, mu, W, l, Dl, TT){ # l = 2 is quadratic trend filtering # l = 1 is linear trend filtering # l = 0 is fused lasso # D^{(1)} is first differences, so it correponds to l=0 # D^{(l+1)} is used for order-l trend filtering. stopifnot(nrow(W) == TT - l) ## if(l == 0){ ## Unew = U + rho * (mu - W) ## } else { ## Unew = U + rho * ( diff(mu, differences = l) - W) ## } Unew &lt;- U + rho * (Dl %*% mu - W) ## Expect a (T-l) x dimdat matrix. stopifnot(all(dim(U) == dim(Unew))) stopifnot(nrow(U) == TT-l) return(Unew) } Here is that main workhorse admm_oneclust(). #&#39; One cluster&#39;s admm for a fixed step size (rho). admm_oneclust &lt;- function(iclust = 1, niter, y, Dl, tDl, Dlp1, l = NULL, TT, N, dimdat, maxdev, rho, rhoinit = rho, Xinv, schurA, schurB, sigmainv, lambda, resp, resp_sum, ylist, err_rel = 1e-3, err_abs = 0, zerothresh, mu, z, w, uw, uz, ## warmstart = FALSE, ## mu.warm = if(!warmstart) NULL, first_iter,## Not used em_iter, outer_iter, local_adapt, sigma, sigma_eig_by_clust){ ## Initialize the variables ### ## resid_mat = matrix(NA, nrow = ceiling(niter/5), ncol = 4) ## colnames(resid_mat) = c(&quot;primresid&quot;, &quot;primerr&quot;, &quot;dualresid&quot;, &quot;dualerr&quot;) resid_mat = matrix(NA, nrow = ceiling(niter/5), ncol = 6) colnames(resid_mat) = c(&quot;prim1&quot;, &quot;prim2&quot;, &quot;primresid&quot;, &quot;primerr&quot;, &quot;dualresid&quot;, &quot;dualerr&quot;) rhofac = rho / rhoinit ## print(&quot;iclust&quot;) ## print(iclust) ## if(iclust == 5) browser() ## This doesn&#39;t change over iterations schurB = myschur(schurB$orig * rhofac) ## In flowmix, this is done on A. Here, it&#39;s done on B (in AX + XB + C = 0). TA = schurA$T ##* rhofac TB = schurB$T UA = schurA$Q UB = schurB$Q tUA = schurA$tQ tUB = schurB$tQ ## This also doesn&#39;t change over iterations C1 &lt;- do.call(cbind, lapply(1:TT, FUN = function(tt){ multmat &lt;- apply(y[[tt]], FUN = function(yy) yy * resp[[tt]], MARGIN = 2) sigmainv %*% colSums(multmat) })) for(iter in 1:niter){ syl_C &lt;- get_C_mat(C1 = C1, resp_sum = resp_sum, TT = TT, dimdat = dimdat, Sigma_inv = sigmainv, N = N, Dl = Dl, rho = rho, z = z, w = w, uz = uz, uw = uw, l=l) FF = (-1) * tUA %*% syl_C %*% UB mu = UA %*% matrix_function_solve_triangular_sylvester_barebonesC2(TA, TB, FF) %*% tUB mu = t(mu) stopifnot(nrow(mu) == TT) stopifnot(ncol(mu) == dimdat) ## if(warmstart &amp; iter == 1){ ## print(&quot;warmed up mu!&quot;) ## mu = mu.warm ## } centered_mu = sweep(mu, 2, colMeans(mu)) ## stopifnot(all(abs(colMeans(centered_mu))&lt;1E-8)) z &lt;- Z_update(centered_mu, Uz = uz, C = maxdev, rho = rho) if(any(abs(mu)&gt;1E2)){ ## browser() stop(&quot;mu is blowing up!&quot;) ## break } wlist = lapply(1:dimdat, function(j){ W_update_fused(l = l, TT = TT, mu = mu[, j, drop = TRUE], rho = rho, lambda = lambda, uw = uw[,j,drop=TRUE], Dl = Dl)}) w &lt;- do.call(cbind, wlist) stopifnot(nrow(w) == TT-l) stopifnot(ncol(w) == dimdat) uz = U_update_Z(uz, rho, mu, z, TT) ## uw = U_update_W(uw, rho, mu, w, l, TT) uw = U_update_W(uw, rho, mu, w, l, Dl, TT) ## Check convergence if( iter &gt; 1 &amp; iter %% 5 == 0){## &amp; !local_adapt){ ## Calculate convergence criterion obj = check_converge(mu, rho, w, z, w_prev, z_prev, uw, uz, Dl, tDl, err_rel = err_rel, err_abs = err_abs) jj = (iter/ 5) resid_mat[jj,] = c( norm(obj$primal_resid1, &quot;F&quot;), ## Temp norm(obj$primal_resid2, &quot;F&quot;), ## Temp norm(obj$primal_resid, &quot;F&quot;), obj$primal_err, norm(obj$dual_resid,&quot;F&quot;), obj$dual_err) if(is.na(obj$converge)){ obj$converge = converge = FALSE warning(&quot;Convergence was NA&quot;) } if(obj$converge){ converge = TRUE break } else { converge = FALSE } } w_prev = w z_prev = z } if(FALSE){ ## Calculate optimization objective values for this cluster. obj.value &lt;- objective_per_cluster(y = y, mu = mu, resp = resp, Sigma_inv = sigmainv, TT = TT, d = dimdat, Dlp1 = Dlp1, Dl = Dl, l = l, maxdev = maxdev, lambda = lambda, rho = rho, N = N) ## This is very expensive to do within each ADMM iteration, so it&#39;s commented out for now. } obj.value = NA return(list(mu = mu, resid_mat = resid_mat, converge = converge, ## Other variables to return. Z = z, W = w, uz = uz, uw = uw, inner.iter = iter, single_admm_objective = obj.value)) } 3.5.4 Helpers for M step (\\(\\mu\\)) First, let’s start with a few helper functions. #&#39; @param TT Number of time points. etilde_mat &lt;- function(TT){ mats &lt;- lapply(1:TT, FUN = function(t){ e_vec &lt;- rep(0, TT) e_vec[t] &lt;- 1 (e_vec - 1/TT) %*% t(e_vec - 1/TT) }) Reduce(&#39;+&#39;, mats) } get_AB_mats &lt;- function(y, resp, Sigma_inv, e_mat, Dlsqrd, N, Dlp1, Dl, rho, z, w, uz, uw){ # A matrix A &lt;- 1/N * Sigma_inv # B matrix sum_resp &lt;- sapply(resp, sum) #B &lt;- rho*(t(Dl)%*%Dl + e_mat)%*% diag(1/unlist(sum_resp)) B &lt;- rho*(Dlsqrd + e_mat) B &lt;- B/sum_resp[col(B)] #B &lt;- rho*(Dlsqrd + e_mat) %*% diag(1/unlist(sum_resp)) return(list(A = A, B = B)) } get_C_mat &lt;- function(C1, resp_sum, TT, dimdat, Sigma_inv, e_mat, N, Dlp1, Dl, rho, z, w, uz, uw, l){ C2 &lt;- t(uz - rho*z) # averaging C2 &lt;- C2 - rowMeans(C2) # third component C3 &lt;- do.call(rbind, lapply(1:dimdat, FUN = function(j){ ((uw[,j, drop=TRUE] - rho * w[,j,drop=TRUE]) %*% Dl) %&gt;% as.numeric() })) # combining C &lt;- (-1/N * C1 + C2 + C3) C &lt;- C/resp_sum[col(C)] return(C) } #&#39; @param mat Matrix to Schur-decompose. myschur &lt;- function(mat){ stopifnot(nrow(mat) == ncol(mat)) if(is.numeric(mat) &amp; length(mat)==1) mat = mat %&gt;% as.matrix() obj = Matrix::Schur(mat) obj$tQ = t(obj$Q) obj$orig = mat return(obj) } Here’s a function to check convergence of the ADMM with a fixed step size. #&#39; Check convergence of ADMM with a fixed step size (rho). check_converge &lt;- function(mu, rho, w, z, w_prev, z_prev, uw, uz, Dl, tDl, err_rel = 1E-4, err_abs = 0 ){ ## Constraints are: Ax + Bz =c, where x = mu, z =(w, z) prim1 = rbind(Dl %*% mu, mu - colMeans(mu)) prim2 = rbind(-w, -z) primal_resid = prim1 + prim2 ## Ax + Bz - c change_z = z - z_prev change_w = w - w_prev dual_resid = rho * (-(change_z - colMeans(change_z)) - tDl %*% change_w) tAU = (uz - colMeans(uz)) + tDl %*% uw ## Form primal and dual tolerances. primal_err = sqrt(length(primal_resid)) * err_abs + err_rel * max(norm(prim1, &quot;F&quot;), norm(prim2, &quot;F&quot;)) dual_err = sqrt(length(dual_resid)) * err_abs + err_rel * norm(tAU, &quot;F&quot;) ## Check convergence. primal_resid_size = norm(primal_resid, &quot;F&quot;) dual_resid_size = norm(dual_resid, &quot;F&quot;) primal_converge = ( primal_resid_size &lt;= primal_err ) dual_converge = ( dual_resid_size &lt;= dual_err ) ## Some checks (trying to address problems with |converge|). assertthat::assert_that(is.numeric(primal_resid_size)) assertthat::assert_that(is.numeric(primal_err)) assertthat::assert_that(is.numeric(dual_resid_size)) assertthat::assert_that(is.numeric(dual_err)) ## return(primal_converge &amp; dual_converge) converge = primal_converge &amp; dual_converge return(list( primal_resid1 = prim1, primal_resid2 = prim2, primal_resid = primal_resid, primal_err = primal_err, dual_resid = dual_resid, dual_err = dual_err, converge = converge)) } Here’s a function to compute the augmented Lagrangian of the M-step (this is not used for now). #&#39; computes the Augmented lagrangian. aug_lagr &lt;- function(y, TT, d, z, w, l, uz, uw, mu, resp, Sigma_inv, Dlp1, Dl, maxdev, lambda, rho, N){ mu_dd &lt;- rowMeans(mu) # Check the Z&#39;s for ball constraint, up to a tolerance of 1e-4 znorms &lt;- apply(z, FUN = function(zz) sqrt(sum(zz^2)), MARGIN = 1) if(any(is.na(znorms))) browser() if(any(znorms &gt; (maxdev + 1e-4))){ warning(&quot;||z||_2 &gt; maxdev, current iterate not feasible.&quot;) return(Inf) } aug1 &lt;- sum(do.call(cbind, lapply(1:TT, FUN = function(t){ multmat &lt;- apply(y[[t]], FUN = function(yy){ t(yy - mu[,t]) %*% Sigma_inv %*% (yy - mu[,t])}, MARGIN = 1) sum(1/(2*N) * resp[[t]]*multmat) }))) aug2 &lt;- lambda*sum(do.call(rbind, lapply(1:d, FUN = function(j) sum(abs(diff(w[j,], differences = 1)))))) aug3 &lt;- sum(do.call(cbind, lapply(1:TT, FUN = function(t){ uz[t,]%*%(mu[,t] - mu_dd - z[t,]) + rho/2 * sum((mu[,t] - mu_dd - z[t,])^2) }))) aug4 &lt;- sum(do.call(rbind, lapply(1:d, FUN = function(j){ uw[,j] %*% (Dl %*% mu[j,] - w[j,]) + rho/2 * sum((Dl %*% mu[j,] - w[j,])^2) }))) total &lt;- aug1 + aug2 + aug3 + aug4 return(total) } Here’s a function to calculate the objective value for the ADMM. \\[\\frac{1}{2N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\hat{\\gamma}_{it} (y_i^{(t)} - \\mu_{\\cdot t})^T \\hat{\\Sigma}^{-1} ( y_i^{(t)} - \\mu_{\\cdot t}) + \\lambda \\sum_{j=1}^d \\|D^{(l+1)}\\mu_{j\\cdot }\\|_1\\] (This is the penalized surrogate objective \\(Q_{\\tilde \\theta}(\\mu, \\Sigma, \\pi) + \\lambda \\sum_{j=1}^d \\|D^{(l+1)} \\mu_{j \\cdot}\\|_1\\), taking only the parts, and leaving out a constant \\(C=-\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2} \\log det(\\Sigma_k)\\), since the constant \\(C\\) doesn’t change over ADMM iterations.) #&#39; computes the ADMM objective for one cluster objective_per_cluster &lt;- function(y, TT, d, l, mu, resp, Sigma_inv, Dlp1, Dl, maxdev, lambda, rho, N){ aug1 &lt;- sum(do.call(cbind, lapply(1:TT, FUN = function(tt){ multmat &lt;- apply(y[[tt]], FUN = function(yy){ t(yy - mu[tt,]) %*% Sigma_inv %*% (yy - mu[tt,])}, MARGIN = 1) sum(1/(2*N) * resp[[tt]] * multmat) }))) aug2 &lt;- lambda*sum(do.call(rbind, lapply(1:d, FUN = function(j) sum(abs(diff(mu[,j], differences = l)))))) total &lt;- aug1 + aug2 return(total) } 3.5.5 All Rcpp functions The main function we write in Rcpp is the “barebones” Sylvester equation solver that takes upper-triangular coefficient matrices. // [[Rcpp::depends(RcppArmadillo)]] // [[Rcpp::depends(RcppEigen)]] #include &lt;RcppArmadillo.h&gt; #include &lt;RcppEigen.h&gt; #include &lt;numeric&gt; using namespace arma; using namespace Eigen; using Eigen::Map; // &#39;maps&#39; rather than copies using Eigen::MatrixXd; // variable size matrix, double precision //&#39; Solve &quot;barebones&quot; sylvester equation that takes upper triangular matrices as coefficients. //&#39; //&#39; @param TA Upper-triangular matrix //&#39; @param TB Upper-triangular matrix //&#39; @param C matrix //&#39; @export // [[Rcpp::export]] Eigen::MatrixXd matrix_function_solve_triangular_sylvester_barebonesC2(const Eigen::MatrixXd &amp; TA, const Eigen::MatrixXd &amp; TB, const Eigen::MatrixXd &amp; C){ // Eigen::eigen_assert(TA.rows() == TA.cols()); // Eigen::eigen_assert(TA.Eigen::isUpperTriangular()); // Eigen::eigen_assert(TB.rows() == TB.cols()); // Eigen::eigen_assert(TB.Eigen::isUpperTriangular()); // Eigen::eigen_assert(C.rows() == TA.rows()); // Eigen::eigen_assert(C.cols() == TB.rows()); // typedef typename MatrixType::Index Index; // typedef typename MatrixType::Scalar Scalar; int m = TA.rows(); int n = TB.rows(); Eigen::MatrixXd X(m, n); for (int i = m - 1; i &gt;= 0; --i) { for (int j = 0; j &lt; n; ++j) { // Compute T_A X = \\sum_{k=i+1}^m T_A_{ik} X_{kj} double TAX; if (i == m - 1) { TAX = 0; } else { MatrixXd TAXmatrix = TA.row(i).tail(m-1-i) * X.col(j).tail(m-1-i); TAX = TAXmatrix(0,0); } // Compute X T_B = \\sum_{k=1}^{j-1} X_{ik} T_B_{kj} double XTB; if (j == 0) { XTB = 0; } else { MatrixXd XTBmatrix = X.row(i).head(j) * TB.col(j).head(j); XTB = XTBmatrix(0,0); } X(i,j) = (C(i,j) - TAX - XTB) / (TA(i,i) + TB(j,j)); } } return X; } Also, we define a faster dmvnorm function written in C++. // [[Rcpp::depends(RcppArmadillo)]] #include &lt;RcppArmadillo.h&gt; static double const log2pi = std::log(2.0 * M_PI); /* C++ version of the dtrmv BLAS function */ void inplace_tri_mat_mult(arma::rowvec &amp;x, arma::mat const &amp;trimat){ arma::uword const n = trimat.n_cols; for(unsigned j = n; j-- &gt; 0;){ double tmp(0.); for(unsigned i = 0; i &lt;= j; ++i) tmp += trimat.at(i, j) * x[i]; x[j] = tmp; } } // [[Rcpp::export]] arma::vec dmvnorm_arma_fast(arma::mat const &amp;x, arma::rowvec const &amp;mean, arma::mat const &amp;sigma, bool const logd = false) { using arma::uword; uword const n = x.n_rows, xdim = x.n_cols; arma::vec out(n); arma::mat const rooti = arma::inv(trimatu(arma::chol(sigma))); double const rootisum = arma::sum(log(rooti.diag())), constants = -(double)xdim/2.0 * log2pi, other_terms = rootisum + constants; arma::rowvec z; for (uword i = 0; i &lt; n; i++) { z = (x.row(i) - mean); inplace_tri_mat_mult(z, rooti); out(i) = other_terms - 0.5 * arma::dot(z, z); } if (logd) return out; return exp(out); } // All credit goes to https://gallery.rcpp.org/articles/dmvnorm_arma/ We need this blob (from https://github.com/jacobbien/litr-project/blob/main/examples/make-an-r-package-with-armadillo/create-witharmadillo.Rmd ): usethis::use_rcpp_armadillo(name = &quot;code&quot;) usethis::use_rcpp_eigen(name = &quot;code&quot;) ## ✔ Leaving &#39;src/code.cpp&#39; unchanged ## • Edit &#39;src/code.cpp&#39; ## ✔ Adding &#39;RcppArmadillo&#39; to LinkingTo field in DESCRIPTION ## ✔ Created &#39;src/Makevars&#39; and &#39;src/Makevars.win&#39; with requested compilation settings. ## ✔ Leaving &#39;src/code.cpp&#39; unchanged ## • Edit &#39;src/code.cpp&#39; ## Registered S3 methods overwritten by &#39;RcppEigen&#39;: ## method from ## predict.fastLm RcppArmadillo ## print.fastLm RcppArmadillo ## summary.fastLm RcppArmadillo ## print.summary.fastLm RcppArmadillo ## ## ✔ Adding &#39;RcppEigen&#39; to LinkingTo field in DESCRIPTION ## ✔ Adding &#39;@import RcppEigen&#39; to &#39;R/flowtrend-package.R&#39; ## • Run `devtools::document()` to update &#39;NAMESPACE&#39; #&#39; Testing against \\code{Mstep_mu()}, for ONE cluster. #&#39; @param ylist #&#39; @param resp #&#39; @param lambda #&#39; @param l #&#39; @param Sigma_inv inverse of Sigma #&#39; @param x covariates Mstep_mu_cvxr &lt;- function(ylist, resp, lambda, l, Sigma_inv, x = NULL, thresh = 1E-8, maxdev = NULL, dimdat, N, ecos_thresh = 1E-8, scs_eps = 1E-5){ ## Define dimensions TT = length(ylist) ## Responsibility Weighted Data ytildes &lt;- lapply(1:TT, FUN = function(tt){ yy &lt;- ylist[[tt]] g &lt;- resp[[tt]] yy &lt;- apply(yy, MARGIN = 2, FUN = function(x) x * g) yrow = matrix(c(NA, NA), nrow=1, ncol=dimdat) yrow[1,] = colSums(yy) yrow }) %&gt;% do.call(rbind, .) ## Auxiliary term, needed to make the objective interpretable aux.y &lt;- Reduce(&quot;+&quot;, lapply(1:TT, FUN = function(tt){ yy &lt;- ylist[[tt]] g &lt;- sqrt(resp[[tt]]) yy &lt;- apply(yy, MARGIN = 2, FUN = function(x) x * g) sum(diag(yy %*% Sigma_inv %*% t(yy))) })) ## Mu, d x T matrix mumat &lt;- CVXR::Variable(cols=dimdat, rows=TT) ## Summed sqrt responsibilities - needed in the objective. resp.sum.sqrt &lt;- lapply(resp, FUN = function(x) sqrt(sum(x))) ## Differencing Matrix, (TT-(l+1)) x TT Dlp1 &lt;- gen_diff_mat(n = TT, l = l+1, x = x) # l = 2 is quadratic trend filtering # l = 1 is linear trend filtering # l = 0 is fused lasso ## Forming the objective obj = 1/(2*N) *( Reduce(&quot;+&quot;, lapply(1:TT, FUN = function(tt) CVXR::quad_form(t(resp.sum.sqrt[[tt]]*mumat[tt,]), Sigma_inv))) -2 * Reduce(&quot;+&quot;, lapply(1:TT, FUN = function(tt) t(ytildes[tt,]) %*% Sigma_inv %*% t(mumat[tt,]))) + aux.y) + lambda * sum(CVXR::sum_entries(abs(Dlp1 %*% mumat), axis = 1)) ##Reduce(&quot;+&quot;, lapply(1:TT, FUN = function(tt) t(ytildes[tt,]) %*% Sigma_inv %*% (mumat[tt,]))) + aux.y ## a = t(resp.sum.sqrt[[tt]]*mumat[tt,]) ## CVXR::quad_form(resp.sum.sqrt[[tt]]*mumat[tt,], Sigma_inv) ## resp.sum.sqrt[[tt]]*mumat[tt,] %&gt;% dim() ## dim(Sigma_inv) ## mumat %&gt;% dim() ## ( (resp.sum.sqrt[[tt]]) * mumat[tt,]) %&gt;% dim() ## Putting together the ball constraint rowmns &lt;- matrix(rep(1, TT^2), nrow = TT)/TT mu_dotdot &lt;- rowmns %*% mumat constraints = list() if(!is.null(maxdev)){ constraints = list(CVXR::sum_entries(CVXR::square(mumat - mu_dotdot), axis = 2) &lt;= rep(maxdev^2, TT) ) } ## Try all two CVXR solvers. prob &lt;- CVXR::Problem(CVXR::Minimize(obj), constraints) result = NULL result &lt;- tryCatch({ CVXR::solve(prob, solver=&quot;ECOS&quot;, FEASTOL = ecos_thresh, RELTOL = ecos_thresh, ABSTOL = ecos_thresh) }, error=function(err){ err$message = paste(err$message, &quot;\\n&quot;, &quot;Lasso solver using ECOS has failed.&quot; ,sep=&quot;&quot;) cat(err$message, fill=TRUE) return(NULL) }) ## If anything is wrong, flag to use SCS solver scs = FALSE if(is.null(result)){ scs = TRUE } else { if(result$status != &quot;optimal&quot;) scs = TRUE } ## Use the SCS solver if(scs){ result = CVXR::solve(prob, solver=&quot;SCS&quot;, eps = scs_eps) if(any(is.na(result$getValue(mumat)))){ ## A clumsy way to check. stop(&quot;Lasso solver using both ECOS and SCS has failed.&quot;, sep=&quot;&quot;) } } ## Record Interesting Parameters num_iters &lt;- result$num_iters status &lt;- result$status mumat &lt;- result$getValue(mumat) val &lt;- result$value return(list(mu = mumat, value = val, status = status, num_iters = num_iters)) } This function solves the following problem: \\[ \\begin{align*} &amp;\\text{minimize}_{\\mu} {\\frac{1}{2N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\hat{\\gamma}_{it} (y_i^{(t)} - \\mu_{\\cdot t})^\\top \\hat{\\Sigma}^{-1} ( y_i^{(t)} - \\mu_{\\cdot t}) + \\lambda \\sum_{j=1}^d \\|D^{(l)}\\mu_{j\\cdot }\\|_1}\\\\ &amp;\\text{subject to}\\;\\; {\\| \\mu_{\\cdot t} - \\bar{\\mu}_{\\cdot \\cdot}\\|_2 \\le r \\;\\;\\forall t=1,\\cdots, T, } \\end{align*} \\] and is directly equivalent to Mstep_mu(). The resulting solution and the objective value should be the same. Let’s check that. First, set up some objects to run Mstep_mu() and Mstep_mu_cvxr(). numclust = 3 TT = 100 dimdat = 1 set.seed(0) dt = gendat_1d(TT = TT, ntlist = rep(TT, 100)) ylist = dt %&gt;% dt2ylist() sigma = init_sigma(ylist, numclust) ## (T x numclust x (dimdat x dimdat)) mn = init_mn(ylist, numclust, TT, dimdat)##, countslist = countslist) prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. resp = Estep(mn, sigma, prob, ylist = ylist, numclust) lambda = .01 l = 1 x = 1:TT Dlp1 = gen_diff_mat(n = TT, l = l+1, x = x) Dl = gen_diff_mat(n = TT, l = l, x = x) Dlsqrd &lt;- t(Dl) %*% Dl maxdev = NULL Then, compare the result of the two implementations. They should look identical. ## overall ADMM res1 = Mstep_mu(resp, ylist, lambda, l=l, sigma=sigma, Dlsqrd = Dlsqrd, Dl=Dl, Dlp1=Dlp1, TT=TT, N=N, dimdat=dimdat, e_mat=etilde_mat(TT = TT), maxdev = maxdev) mn1 = res1$mns ## CVXR just ONE cluster res2list = lapply(1:numclust, function(iclust){ Sigma_inv_oneclust = solve(sigma[iclust,,]) resp_oneclist = lapply(resp, function(resp_onetime){resp_onetime[,iclust, drop=FALSE]}) N = sum(unlist(resp)) res2 = Mstep_mu_cvxr(ylist, resp_oneclist, lambda, l, Sigma_inv_oneclust, thresh = 1E-8, maxdev = maxdev, dimdat, N) res2$mu }) mn2 = array(NA, dim=c(100, dimdat, 3)) for(iclust in 1:numclust){ mn2[,,iclust] = res2list[[iclust]] %&gt;% as.matrix() } ## Plot them plot(x = dt$time, y=dt$Y, col = rgb(0,0,0,0.04), main = &#39;admm (solid) vs cvxr (dashed)&#39;, ylab = &quot;&quot;, xlab = &quot;time&quot;) mn1[,1,] %&gt;% matlines(lwd = 1, lty = 1) mn2[,1,] %&gt;% matlines(lwd = 3, lty = 3) Next, do this for uneven inputs. ## Setup numclust = 3 TT = 100 dimdat = 1 set.seed(0) dt = gendat_1d(TT = TT, ntlist = rep(TT, 100)) ylist = dt %&gt;% dt2ylist() ## Subset them ylist = ylist[-seq(from=10,to=100,by=10)] x = (1:100)[-seq(from=10,to=100,by=10)] sigma = init_sigma(ylist, numclust) ## (T x numclust x (dimdat x dimdat)) mn = init_mn(ylist, numclust, TT, dimdat)##, countslist = countslist) prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. resp = Estep(mn, sigma, prob, ylist = ylist, numclust) lambda = .1 l = 2 ##x = 1:TT Dlp1 = gen_diff_mat(n = length(x), l = l+1, x = x) Dl = gen_diff_mat(n = length(x), l = l, x = x) Dlsqrd &lt;- t(Dl) %*% Dl maxdev = NULL ## Try the algorithm itelf. set.seed(100) obj = flowtrend_once(ylist = ylist, x = x, lambda = .1, lambda_prob = .1, l = 2, l_prob = 2, maxdev = 5, numclust = 3, rho_init = 0.01, verbose = TRUE) plot_1d(ylist=ylist, obj=obj) plot(obj$objectives, type =&#39;l&#39;) mn1 = obj$mn obj$mn[,1,] %&gt;% diff() %&gt;% diff() %&gt;% diff() %&gt;% matplot(type=&#39;l&#39;) mn[,1,] %&gt;% diff() %&gt;% diff() %&gt;% diff() %&gt;% matplot(type=&#39;l&#39;) mn[,1,] %&gt;% matplot(type=&#39;l&#39;) ## Okay, so cluster 3 has a very irregular mean. plot(x = dt$time, y=dt$Y, col = rgb(0,0,0,0.04), main = &#39;admm (solid) vs cvxr (dashed)&#39;, ylab = &quot;&quot;, xlab = &quot;time&quot;) mn_old[,1,] %&gt;% matlines(lwd = 1, x=x, lty = 1) mn_less_old[,1,] %&gt;% matlines(lwd = 1, x=x, lty = 2) mn[,1,] %&gt;% matlines(lwd = 1, x=x, lty = 3) TODO: We just need to bundle this into testthat style tests, and make sure to test several lambda values. TODO: maybe do this for unevenly spaced inputs. CVXR needs to take a different D matrix. testthat::test_that(&quot;Test the M step of \\mu against CVXR&quot;, {}) 3.6 The main “flowtrend” function Now we’ve assembled all ingredients we need, we’ll build the main function flowtrend_once() to estimate a flowtrend model. Here goes: #&#39; Estimate flowtrend model once. #&#39; #&#39; @param ylist Data. #&#39; @param countslist Counts corresponding to multiplicities. #&#39; @param x Times, if points are not evenly spaced. Defaults to NULL, in which #&#39; case the value becomes \\code{1:T}, for the $T==length(ylist)$. #&#39; @param numclust Number of clusters. #&#39; @param niter Maximum number of EM iterations. #&#39; @param l Degree of differencing for the mean trend filtering. l=0 will give #&#39; you piecewise constant means; l=1 is piecewise linear, and so forth. #&#39; @param l_prob Degree of differencing for the probability trend filtering. #&#39; @param mn Initial value for cluster means. Defaults to NULL, in which case #&#39; initial values are randomly chosen from the data. #&#39; @param lambda Smoothing parameter for means #&#39; @param lambda_prob Smoothing parameter for probabilities #&#39; @param verbose Loud or not? EM iteration progress is printed. #&#39; @param tol_em Relative numerical improvement of the objective value at which #&#39; to stop the EM algorithm #&#39; @param maxdev Maximum deviation of cluster means across time.. #&#39; @param countslist_overwrite #&#39; @param admm_err_rel #&#39; @param admm_err_abs #&#39; @param admm_local_adapt #&#39; @param admm_local_adapt_niter #&#39; #&#39; @return List object with flowtrend model estimates. #&#39; @export #&#39; #&#39; @examples flowtrend_once &lt;- function(ylist, countslist = NULL, x = NULL, numclust, niter = 1000, l, l_prob = NULL, mn = NULL, lambda = 0, lambda_prob = NULL, verbose = FALSE, tol_em = 1E-4, maxdev = NULL, countslist_overwrite = NULL, ## beta Mstep (ADMM) settings admm = TRUE, admm_err_rel = 1E-3, admm_err_abs = 1E-4, ## Mean M step (Locally Adaptive ADMM) settings admm_local_adapt = TRUE, admm_local_adapt_niter = if(admm_local_adapt) 10 else 1, rho_init = 0.1, ## Other options check_convergence = TRUE, ## Random seed seed = NULL){ ## Basic checks if(!is.null(maxdev)){ assertthat::assert_that(maxdev!=0) } else { maxdev = 1E10 } assertthat::assert_that(numclust &gt; 1) assertthat::assert_that(niter &gt; 1) if(is.null(countslist)){ ntlist = sapply(ylist, nrow) countslist = lapply(ntlist, FUN = function(nt) rep(1, nt)) } if(!is.null(seed)){ assertthat::assert_that(all((seed %&gt;% sapply(., class)) == &quot;integer&quot;)) assertthat::assert_that(length(seed) == 7) } ## Setup for EM algorithm TT = length(ylist) dimdat = ncol(ylist[[1]]) if(is.null(x)) x &lt;- 1:TT if(is.unsorted(x)) stop(&quot;x must be ordered!&quot;) # l = 2 is quadratic trend filtering # l = 1 is linear trend filtering # l = 0 is fused lasso # D^{(1)} is first differences, so it correponds to l=0 Dlp1 = gen_diff_mat(n = TT, l = l+1, x = x) if(l &gt; 1){ facmat = diag(l / diff(x, lag = l)) } else { facmat = diag(rep(1, TT-l)) } Dl = facmat %*% gen_diff_mat(n = TT, l = l, x = x) tDl = t(Dl) Dlsqrd &lt;- t(Dl) %*% Dl e_mat &lt;- etilde_mat(TT = TT) # needed to generate B Dlp1_prob = gen_diff_mat(n = TT, l = l_prob+1, x = x) H_tf &lt;- gen_tf_mat(n = TT, k = l_prob, x = x) if(is.null(mn)){ mn = init_mn(ylist, numclust, TT, dimdat, countslist = countslist, seed = seed) } ntlist = sapply(ylist, nrow) N = sum(ntlist) ## Initialize some objects prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. denslist_by_clust &lt;- NULL objectives = c(+1E20, rep(NA, niter-1)) sigma_fac &lt;- diff(range(do.call(rbind, ylist)))/8 sigma = init_sigma(ylist, numclust, sigma_fac) ## (T x numclust x (dimdat x dimdat)) sigma_eig_by_clust = NULL zero.betas = zero.alphas = list() ## The least elegant solution I can think of.. used only for blocked cv if(!is.null(countslist_overwrite)) countslist = countslist_overwrite #if(!is.null(countslist)) check_trim(ylist, countslist) vals &lt;- vector(length = niter) latest_rho = NA start.time = Sys.time() for(iter in 2:niter){ if(verbose){ print_progress(iter-1, niter-1, &quot;EM iterations.&quot;, start.time = start.time, fill = FALSE) } resp &lt;- Estep(mn, sigma, prob, ylist = ylist, numclust = numclust, denslist_by_clust = denslist_by_clust, first_iter = (iter == 2), countslist = countslist, padding = 1E-8) ## M step (three parts) ## 1. Means res_mu = Mstep_mu(resp, ylist, lambda = lambda, first_iter = (iter == 2), l = l, Dlp1 = Dlp1, Dl = Dl, tDl = tDl, Dlsqrd = Dlsqrd, sigma_eig_by_clust = sigma_eig_by_clust, sigma = sigma, maxdev = maxdev, e_mat = e_mat, Zs = NULL, Ws = NULL, uws = NULL, uzs = NULL, x = x, err_rel = admm_err_rel, err_abs = admm_err_abs, local_adapt = admm_local_adapt, local_adapt_niter = admm_local_adapt_niter, rho_init = rho_init, iter = iter) ## rho_init = (if(iter == 2) rho_init else latest_rho)) ## latest_rho = res_mu$rho mn = res_mu$mns ## 2. Sigma sigma = Mstep_sigma(resp, ylist, mn, numclust) ## 3. Probabilities prob_link = Mstep_prob(resp, countslist = countslist, H_tf = H_tf, lambda_prob = lambda_prob, l_prob = l_prob, x = x) prob = softmax(prob_link) objectives[iter] = objective(ylist = ylist, mu = mn, sigma = sigma, prob = prob, prob_link = prob_link, lambda = lambda, Dlp1 = Dlp1, l = l, countslist = countslist, Dlp1_prob = Dlp1_prob, l_prob = l_prob, lambda_prob = lambda_prob) ## Check convergence if(iter &gt; 10){ if(check_convergence &amp; check_converge_rel(objectives[iter-1], objectives[iter], tol = tol_em) &amp; check_converge_rel(objectives[iter-2], objectives[iter-1], tol = tol_em)&amp; check_converge_rel(objectives[iter-3], objectives[iter-2], tol = tol_em)){ ## check_converge_rel(objectives[iter-4], objectives[iter-3], tol = tol_em)){ break } } } return(structure(list(mn = mn, prob = prob, prob_link = prob_link, sigma = sigma, objectives = objectives[2:iter], final.iter = iter, ## Above is output, below are data/algorithm settings. dimdat = dimdat, TT = TT, N = N, l = l, l_prob = l_prob, x = x, numclust = numclust, lambda = lambda, lambda_prob = lambda_prob, maxdev = maxdev, niter = niter ), class = &quot;flowtrend&quot;)) } Next, flowtrend() is the main user-facing function. #&#39; Main function. Repeats the EM algorithm (\\code{flowtrend_once()}) with |nrep| restarts (5 by default). #&#39; #&#39; @param ... : arguments for \\code{flowtrend_once()} #&#39; @param nrestart : number of random restarts #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples flowtrend &lt;- function(..., nrestart = 5){ args = list(...) if(&quot;verbose&quot; %in% names(args)){ if(args$verbose){ cat(&quot;EM will restart&quot;, nrestart, &quot;times&quot;, fill=TRUE) } } out_models &lt;- lapply(1:nrestart, FUN = function(irestart){ if(&quot;verbose&quot; %in% names(args)){ if(args$verbose){ cat(&quot;EM restart:&quot;, irestart, fill=TRUE) } } model_temp &lt;- flowtrend_once(...) model_obj &lt;- tail(model_temp$objectives, n = 1) if(&quot;verbose&quot; %in% names(args)){ if(args$verbose){ cat(fill=TRUE) } } return(list(model = model_temp, final_objective = model_obj)) }) final_objectives &lt;- sapply(out_models, FUN = function(x) x$final_objective) best_model &lt;- which.min(final_objectives) final_model = out_models[[best_model]][[&quot;model&quot;]] ## Add the objectives final_model$all_objectives = lapply(1:nrestart, function(irestart){ one_model = out_models[[irestart]] data.frame(objective=one_model$model$objectives) %&gt;% mutate(iter=row_number(), irestart=irestart) %&gt;% select(irestart, iter, objective) }) %&gt;% bind_rows() return(final_model) } "],["reordering-clusters.html", "4 Reordering clusters", " 4 Reordering clusters It’s useful to be able to reorder, or permute, one model’s cluster labels (cluster 1,2,.. of newres which are arbitrary) to that of another model origres. The function reorder_kl() does this by (1) taking the posterior probabilities of the particles in ylist_particle (row-binded to be a \\(\\sum_t n_t \\times K\\) matrix), and then (2) using a Hungarian algorithm [@kuhn-hungarian] to best match the two elongated matrices \\(A\\) and \\(B\\) by measuring the (symmetric? TODO check) KL divergence between all permutations of the \\(K\\) columns (TODO: This is currently just copy-pasted from flowmix. But actually, this is an example of a function that can be directly borrowed; all the work is done in reorder_clust(). So we’ll deal with that!) #&#39; Reorder the cluster numbers for a new flowtrend object \\code{newres}; the best #&#39; permutation (reordering) is to match the original flowmix object #&#39; \\code{origres}. #&#39; #&#39; @param newres New flowtrend object to reorder. #&#39; @param origres Original flowtrend object. #&#39; @param ylist_particle The particle-level data. #&#39; @param fac Defaults to 100, to take 1/100&#39;th of the particles from each time point. #&#39; @param verbose Loud or not? #&#39; #&#39; @return Reordered res #&#39; #&#39; @export reorder_kl &lt;- function(newres, origres, ylist_particle, fac = 100, verbose = FALSE){ ## Randomly sample 1/100 of the original particles (mainly for memory reasons) TT = length(ylist_particle) N = sapply(ylist_particle, nrow) %&gt;% sum() ntlist = sapply(ylist_particle, nrow) indlist = lapply(1:TT, function(tt){ nt = ntlist[[tt]] ind = sample(1:nt, round(nt / fac), replace=FALSE) }) ## Sample responsibilities ylist_particle_small = Map(function(ind, y){ y[ind,,drop = FALSE] }, indlist, ylist_particle) ## Calculate new responsibilities resp_orig_small &lt;- Estep(origres$mn, origres$sigma, origres$prob, ylist = ylist_particle_small, numclust = origres$numclust, first_iter = TRUE) resp_new_small &lt;- Estep(newres$mn, newres$sigma, newres$prob, ylist = ylist_particle_small, numclust = newres$numclust, first_iter = TRUE) assertthat::assert_that(all(sapply(resp_orig_small, dim) == sapply(resp_new_small, dim))) ## Get best ordering (using symm. KL divergence and Hungarian algorithm for ## matching) best_ord &lt;- get_best_match_from_kl(resp_new_small, resp_orig_small) if(verbose) cat(&quot;New order is&quot;, best_ord, fill=TRUE) newres_reordered_kl = newres %&gt;% reorder_clust(ord = best_ord) ## Return the reordered object return(newres_reordered_kl) } This function uses get_best_match_from_kl(), which takes two lists containing responsibilities (posterior probabilities of particles) – one from each model – and returns the cluster ordering to apply to the model that produced resp_new. We define this function and a couple of helper functions next. #&#39; Compute KL divergence from responsibilities between two models&#39; #&#39; responsibilities \\code{resp_new} and \\code{resp_old}. #&#39; #&#39; @param resp_new New responsibilities #&#39; @param resp_orig Original responsiblities. #&#39; #&#39; @return Calculate reordering \\code{o} of the clusters in model represented #&#39; by \\code{resp_new}. To be clear, \\code{o[i]} of new model is the best #&#39; match with the i&#39;th cluster of the original model. #&#39; #&#39; @export #&#39; @importFrom clue solve_LSAP get_best_match_from_kl &lt;- function(resp_new, resp_orig){ ## Basic checks . = NULL ## Fixing check() assertthat::assert_that(all(sapply(resp_new, dim) == sapply(resp_orig , dim))) ## Row-bind all the responsibilities to make a long matrix distmat = form_symmetric_kl_distmat(resp_orig %&gt;% do.call(rbind,.), resp_new %&gt;% do.call(rbind,.)) ## Use Hungarian algorithm to solve. fit &lt;- clue::solve_LSAP(distmat) o &lt;- as.numeric(fit) ## Return the ordering return(o) } #&#39; From two probability matrices, form a (K x K) distance matrix of the #&#39; (n)-vectors. The distance between the vectors is the symmetric KL #&#39; divergence. #&#39; #&#39; @param mat1 Matrix 1 of size (n x K). #&#39; @param mat2 Matrix 2 of size (n x K). #&#39; #&#39; @return K x K matrix containing symmetric KL divergence of each column of #&#39; \\code{mat1} and \\code{mat2}. form_symmetric_kl_distmat &lt;- function(mat1, mat2){ ## Manually add some small, in case some columns are all zero mat1 = (mat1 + 1E-10) %&gt;% pmin(1) mat2 = (mat2 + 1E-10) %&gt;% pmin(1) ## Calculate and return distance matrix. KK1 = ncol(mat1) KK2 = ncol(mat2) distmat = matrix(NA, ncol=KK2, nrow=KK1) for(kk1 in 1:KK1){ for(kk2 in 1:KK2){ mydist = symmetric_kl(mat1[,kk1, drop=TRUE], mat2[,kk2, drop=TRUE]) distmat[kk1, kk2] = mydist } } stopifnot(all(!is.na(distmat))) return(distmat) } #&#39; Symmetric KL divergence, of two probability vectors. #&#39; #&#39; @param vec1 First probability vector. #&#39; @param vec2 Second prbability vector. #&#39; #&#39; @return Symmetric KL divergence (scalar). symmetric_kl &lt;- function(vec1, vec2){ stopifnot(all(vec1 &lt;= 1) &amp; all(vec1 &gt;= 0)) stopifnot(all(vec2 &lt;= 1) &amp; all(vec2 &gt;= 0)) kl &lt;- function(vec1, vec2){ sum(vec1 * log(vec1 / vec2)) } return((kl(vec1, vec2) + kl(vec2, vec1))/2) } Finally, the function that actually performs the manual reordering the clusters of an estimated model obj is reorder_clust(). #&#39; Reorder the results of one object so that cluster 1 through #&#39; \\code{numclust} is in a particular order. The default is decreasing order of #&#39; the averages (over time) of the cluster means. #&#39; #&#39; @param res Model object. #&#39; @param ord Defaults to NULL. Use if you have an ordering in mind. #&#39; #&#39; @return Same object, but with clusters reordered. #&#39; #&#39; @export reorder_clust &lt;- function(res, ord = NULL){ ## Find an order by sums (averages) stopifnot(class(res) == &quot;flowtrend&quot;) if(is.null(ord)) ord = res$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing = TRUE) if(!is.null(ord)) all(sort(ord) == 1:res$numclust) ## Reorder mean res$mn = res$mn[,,ord, drop=FALSE] ## Reorder sigma res$sigma = res$sigma[ord,,,drop=FALSE] ## Reorder prob res$prob = res$prob[,ord, drop=FALSE] ## Reorder the responsibilities ## if(&#39;resp&#39; %in% res){ resp_temp = res$resp for(tt in 1:res$TT){ resp_temp[[tt]] = res$resp[[tt]][,ord] } ## } res$resp = resp_temp return(res) } Here’s an example of how to use this. devtools::load_all(&quot;~/repos/FlowTF&quot;) set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45) dt_model &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45, return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() ## Fit model twice. set.seed(2) objlist &lt;- lapply(1:2, function(isim){ flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, nrestart = 1, verbose = TRUE)}) ## Perform the reordering, make three plots newres = objlist[[1]] origres = objlist[[2]] newres_reordered = reorder_kl(newres, origres, ylist, fac = 100, verbose = FALSE) plot_1d(ylist, newres, x = x) + ggtitle(&quot;before reordering, model 1&quot;) plot_1d(ylist, origres, x = x) + ggtitle(&quot;model 2&quot;) plot_1d(ylist, newres_reordered, x = x) + ggtitle(&quot;reordered model 1&quot;) "],["tuning-lambda.html", "5 Tuning \\(\\lambda\\) 5.1 Predicting and evaluating on new time points 5.2 Maximum \\((\\lambda_\\mu, \\lambda_\\pi)\\) values to test 5.3 Define CV data folds 5.4 CV = many single jobs 5.5 Running cross-validation 5.6 Summarizing the output 5.7 CV on your own computer 5.8 Plotting", " 5 Tuning \\(\\lambda\\) Now that the flowtrend() function has been built (we will test it in the next section, i.e., 6test.Rmd), we need to build up quite a few functions before we’re able to cross-validate. These include: Predicting out-of-sample, using predict_flowtrend(). Evaluating data fit (by likelihood) in an out-of-sample measurement, using objective(..., unpenalized = TRUE). Numerically estimating the maximum regularization values to test, using get_max_lambda(). Making data splits, using make_cv_folds(). 5.1 Predicting and evaluating on new time points First, let’s write a couple of functions interpolate_mn() and interpolate_prob() which linearly interpolate the means and probabilities at new time points. #&#39; Do a linear interpolation of the cluster means. #&#39; #&#39; @param x Training times. #&#39; @param tt Prediction time. #&#39; @param iclust Cluster number. #&#39; @param mn length(x) by dimdat by numclust matrix. #&#39; #&#39; @return A dimdat-length vector. interpolate_mn &lt;- function(x, tt, iclust, mn){ ## Basic checks stopifnot(length(x) == dim(mn)[1]) stopifnot(iclust &lt;= dim(mn)[3]) if(tt %in% x) return(mn[which(x==tt),,iclust,drop=TRUE]) ## Set up for linear interpolation floor_t &lt;- max(x[which(x &lt;= tt)]) ceiling_t &lt;- min(x[which(x &gt;= tt)]) floor_t_ind &lt;- which(x == floor_t) ceiling_t_ind &lt;- which(x == ceiling_t) ## Do the linear interpolation mn_t &lt;- mn[ceiling_t_ind,,iclust,drop=TRUE]*(tt - floor_t)/(ceiling_t - floor_t) + mn[floor_t_ind,,iclust,drop=TRUE]*(ceiling_t - tt)/(ceiling_t - floor_t) ## Basic checks stopifnot(length(mn_t) == dim(mn)[2]) return(mn_t) } #&#39; Do a linear interpolation of the cluster means. #&#39; #&#39; @param x Training times. #&#39; @param tt Prediction time. #&#39; @param iclust Cluster number. #&#39; @param prob length(x) by numclust array or matrix. #&#39; #&#39; @return One probability. interpolate_prob &lt;- function(x, tt, iclust, prob){ ## Basic checks numdat = dim(prob)[1] numclust = dim(prob)[2] stopifnot(length(x) == numdat) stopifnot(iclust &lt;= numclust) if(tt %in% x) return(prob[which(x == tt),iclust,drop=TRUE]) ## Set up for linear interpolation floor_t &lt;- max(x[which(x &lt;= tt)]) ceiling_t &lt;- min(x[which(x &gt;= tt)]) floor_t_ind &lt;- which(x == floor_t) ceiling_t_ind &lt;- which(x == ceiling_t) ## Do the linear interpolation prob_t &lt;- prob[ceiling_t_ind,iclust,drop=TRUE]*(tt - floor_t)/(ceiling_t - floor_t) + prob[floor_t_ind,iclust,drop=TRUE]*(ceiling_t - tt)/(ceiling_t - floor_t) # linear interpolation between floor_t and ceiling_t ## Basic checks stopifnot(length(prob_t) == 1) stopifnot(0 &lt;= prob_t &amp; prob_t &lt;= 1) return(prob_t) } Next, let’s build a prediction function predict_flowtrend() which takes the model object obj, and the new time points newtimes, and produces. #&#39; Prediction: Given new timepoints in the original time interval,generate a set #&#39; of means and probs (and return the same Sigma). #&#39; #&#39; @param obj Object returned from covariate EM flowtrend(). #&#39; @param newtimes New times at which to make predictions. #&#39; #&#39; @return List containing mean, prob, and sigma, and x. #&#39; #&#39; @export #&#39; predict_flowtrend &lt;- function(obj, newtimes = NULL){ ## Check the dimensions newx &lt;- newtimes if(is.null(newtimes)){ newx = obj$x } ## Check if the new times are within the time range of the original data stopifnot(all(sapply(newx, FUN = function(t) t &gt;= min(obj$x) &amp; t &lt;= max(obj$x)))) ## Setup some things x &lt;- obj$x TT_new = length(newx) numclust = obj$numclust dimdat = obj$dimdat ## Predict the means (manually). newmn_array = array(NA, dim = c(TT_new, dimdat, numclust)) for(iclust in 1:numclust){ newmn_oneclust &lt;- lapply(newx, function(tt){ interpolate_mn(x, tt, iclust, obj$mn) }) %&gt;% do.call(rbind, . ) newmn_array[,,iclust] = newmn_oneclust } ## Predict the probs. newprob = array(NA, dim = c(TT_new, numclust)) for(iclust in 1:numclust){ newprob_oneclust &lt;- lapply(newx, function(tt){ interpolate_prob(x, tt, iclust, obj$prob) }) %&gt;% do.call(c, .) newprob[,iclust] = newprob_oneclust } ## Basic checks stopifnot(all(dim(newprob) == c(TT_new,numclust))) stopifnot(all(newprob &gt;= 0)) stopifnot(all(newprob &lt;= 1)) ## Return the predictions return(list(mn = newmn_array, prob = newprob, sigma = obj$sigma, x = newx)) } Here’s a quick test (no new data) to make sure this function returns a list containing: the mean, probability, covariance, and new times. testthat::test_that(&quot;The prediction function returns the right things&quot;, { ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100)) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, ## nrestart = 1, niter = 3) predobj = predict_flowtrend(obj) testthat::expect_named(predobj, c(&quot;mn&quot;, &quot;prob&quot;, &quot;sigma&quot;, &quot;x&quot;)) }) Now, we try to make predictions at new held-out time points held_out=25:35, from a model that is estimated without those time points. ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100)) dt_model &lt;- gendat_1d(100, rep(100, 100), return_model = TRUE) held_out = 25:35 dt_subset = dt %&gt;% subset(time %ni% held_out) ylist = dt_subset %&gt;% dt2ylist() x = dt_subset %&gt;% pull(time) %&gt;% unique() set.seed(686) obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, l = 2, l_prob = 2, lambda = 0.02, lambda_prob = .1, ## nrestart = 5, verbose = TRUE) obj$all_objectives %&gt;% mutate(irestart = as.factor(irestart)) %&gt;% ggplot() + geom_line(aes(x=iter, y=objective, group = irestart, col = irestart)) ## Also reorder the cluster labels of the truth, to match the fitted model. ord = obj$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) lookup &lt;- setNames(c(1:obj$numclust), ord) dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %&gt;% as.factor() ## Reorder the cluster labels of the fitted model. obj = reorder_clust(obj) testthat::test_that(&quot;prediction function returns the right things&quot;, { predobj = predict_flowtrend(obj, newtimes = held_out) ## Check a few things testthat::expect_equal(predobj$x, held_out) testthat::expect_equal(rowSums(predobj$prob), rep(1, length(held_out))) testthat::expect_equal(dim(predobj$mn), c(length(held_out), 1, 3)) }) Plot the predicted means \\(\\mu\\) and probabilities \\(\\pi\\), with purple points at the interpolated means. We can see that it works as expected. predobj = predict_flowtrend(obj, newtimes = held_out) g = plot_1d(ylist = ylist, obj=obj, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model,## %&gt;% subset(time %ni% held_out), linetype = &quot;dashed&quot;, size=2, alpha = .7) ## Plot the predicted means preds = lapply(1:3, function(iclust){ tibble(mn = predobj$mn %&gt;% .[,,iclust, drop = TRUE], prob = predobj$prob %&gt;% .[,iclust, drop = TRUE], time = held_out, cluster = iclust) }) %&gt;% bind_rows() g + geom_line(aes(x=time, y=mn, group = cluster), data = preds, col = &#39;yellow&#39;, size = 2)##, alpha = .8) The estimated probabilities are shown here, with purple points showing the interpolation. It works as expected. plot_prob(obj, x=x) + geom_line(aes(x = time, y = prob, group = cluster, color = cluster), data = dt_model, linetype = &quot;dashed&quot;) + facet_wrap(~cluster) + geom_line(aes(x = time, y = prob), data = preds, col = &#39;yellow&#39;, size = 3) Let’s now try to space inputs unevenly, by x. set.seed(100) TT = 100 dt &lt;- gendat_1d(TT, rep(100, TT)) dt_model &lt;- gendat_1d(TT, rep(100, TT), return_model = TRUE) ylist_orig = dt %&gt;% dt2ylist() plot_1d(ylist_orig) x = sample(1:TT, floor(TT/2)) %&gt;% sort() ylist = ylist_orig[x] set.seed(55) obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, l = 2, l_prob = 2, lambda = .5, lambda_prob = .5, ## rho_init = .1, nrestart = 1, verbose = TRUE) obj$all_objectives %&gt;% mutate(irestart = as.factor(irestart)) %&gt;% ggplot() + geom_line(aes(x=iter, y=objective, group = irestart, col = irestart)) ## Make mean predictions newtimes = seq(from=min(x),to=max(x),length=10000) predobj = predict_flowtrend(obj, newtimes = newtimes) ## Plot the predicted means preds = lapply(1:3, function(iclust){ tibble(mn = predobj$mn %&gt;% .[,,iclust, drop = TRUE], prob = predobj$prob %&gt;% .[,iclust, drop = TRUE], time = newtimes, cluster = iclust) }) %&gt;% bind_rows() The estimated means \\(\\mu\\) in the training data are shown as solid triangle points. The out-of-sample \\(\\mu\\) predictions made on a fine grid of time points (shown by the yellow lines) look fine. g = plot_1d(ylist=ylist, obj = obj, x = x) g + ggtitle(&quot;Fitted model&quot;) g + geom_line(aes(x=time, y=mn, group = cluster), data = preds, col = &#39;yellow&#39;, size = rel(1), alpha = .7) + ggtitle(&quot;Predictions on fine grid of times&quot;) The out-of-sample \\(\\pi\\) predictions are the lines that connect the points. They look great as well. plot_prob(obj, x=x) + ## geom_line(aes(x = time, y = prob, group = cluster, color = cluster), ## data = dt_model, linetype = &quot;dashed&quot;) + facet_wrap(~cluster) + geom_line(aes(x = time, y = prob), data = preds, col = &#39;yellow&#39;, size = rel(.5), alpha = .7) Next, we’ll try evaluating an estimated model’s prediction in an out-of-sample measurement. This will be measured by the model prediction’s out-of-sample objective (negative log-likelihood). ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100)) dt_model &lt;- gendat_1d(100, rep(100, 100), return_model = TRUE) held_out = 25:35 dt_subset = dt %&gt;% subset(time %ni% held_out) ylist = dt_subset %&gt;% dt2ylist() x = dt_subset %&gt;% pull(time) %&gt;% unique() obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, ## nrestart = 5) ## Make prediction predobj = predict_flowtrend(obj, newtimes = held_out) ## Use the predicted (interpolated) model parameters obj_pred = objective(mu = predobj$mn, prob = predobj$prob, sigma = predobj$sigma, ylist = ylist[held_out], unpenalized = TRUE) truemn = array(NA, dim = dim(predobj$mn)) truemn[,1,] = dt_model %&gt;% select(time, cluster, mean) %&gt;% pivot_wider(names_from = cluster, values_from = mean) %&gt;% subset(time %in% held_out) %&gt;% select(-time) %&gt;% as.matrix() ## Use the true mean obj_better = objective(mu = truemn, prob = predobj$prob, sigma = predobj$sigma, ylist = ylist[held_out], unpenalized = TRUE) ## Here is the estimated model plot_1d(ylist=ylist, obj=obj, x= (1:100)[-held_out]) The out-of-sample prediction is similar for the predicted model and the estimated model. Below, we’re showing just the predicted means at the held-out points, overlaid with data. (This is measured by the objective (= negative log likelihood), so lower is better! Red is worse than black, naturally.) {r fit, fig.width = 7, fig.height = 5}) g = plot_1d(ylist = dt %&gt;% subset(time %in% held_out) %&gt;% dt2ylist(), x = held_out) + xlim(c(0,100)) g + geom_line(aes(x=time, y = value, group = name), data = data.frame(truemn[,1,]) %&gt;% add_column(time = held_out) %&gt;% pivot_longer(-time)) + geom_line(aes(x=time, y = value, group = name), data = data.frame(predobj$mn[,1,]) %&gt;% add_column(time = held_out) %&gt;% pivot_longer(-time), col = 'red') + ggtitle(paste0(round(obj_pred,3), \" (red, predicted) vs. \", round(obj_better, 3), \"(black, truth)\")) 5.2 Maximum \\((\\lambda_\\mu, \\lambda_\\pi)\\) values to test What should the maximum value of regularization parameters to use? It’s useful to be able to calculate the smallest value of regularization parameters that result in fully “simple” \\(\\mu\\) and \\(\\pi\\) over time, in all clusters. Call these \\(\\lambda_\\mu^{\\text{max}}\\) and \\(\\lambda_{\\pi}^{\\text{max}}\\). We use these to form a 2d grid of candidate \\(\\lambda\\) values – logarithmically-spaced pairs of values between starting at \\((\\lambda_{\\mu}^{\\text{max}}, \\lambda_{\\pi}^{\\text{max}})\\) decreasing to some small pair of values. The function get_max_lambda() numerically estimates this maximum pair \\((\\lambda_{\\mu}^{\\text{max}}, \\lambda_{\\pi}^{\\text{max}})\\). It proceeds by first running flowtrend() on a very large pair \\((\\lambda_\\mu, \\lambda_\\pi)\\), then sequentially halving both values while checking if the resulting estimated \\(\\mu\\) and \\(\\pi\\) are all as simple over time. The simplest, most regularized model of an \\(l\\)’th order trend filter estimate will be a single \\(l\\)’th order polynomial; this means that there will be no discontinuities in the \\(l-1\\)’th order differences. (For example, a linear trend filter is \\(l=1\\); the first differences should be piecewise constant, and second differences should be zero except for at the knots. A quadratic trend filter is \\(l=2\\); the first differences should be piecewise linear, the second differences should be piecewise constant, and the third differences should be zero except for at the knots.) As soon as they cease to be simple, we stop and take the immediately previous pair of values of \\((\\lambda_\\mu, \\lambda_\\pi)\\). get_max_lambda() is a wrapper around the workhorse calc_max_lambda(). It obtains the value and saves it to a maxres_file (which defaults to maxres.Rdata) in the destin directory. #&#39; A wrapper for \\code{calc_max_lambda}. Saves the two maximum lambda values in #&#39; a file. #&#39; #&#39; @param destin Where to save the output (A two-lengthed list called #&#39; &quot;maxres&quot;). #&#39; @param maxres_file Filename for output. Defaults to maxres.Rdata. #&#39; @param ... Additional arguments to \\code{flowtrend()}. #&#39; @inheritParams calc_max_lambda #&#39; #&#39; @return No return #&#39; #&#39; @export get_max_lambda &lt;- function(destin, maxres_file = &quot;maxres.Rdata&quot;, ylist, countslist, numclust, maxdev, max_lambda_mean, max_lambda_prob, ...){ if(file.exists(file.path(destin, maxres_file))){ load(file.path(destin, maxres_file)) cat(&quot;Maximum regularization values are loaded.&quot;, fill=TRUE) return(maxres) } else { print(Sys.time()) cat(&quot;Maximum regularization values being calculated.&quot;, fill = TRUE) cat(&quot;with initial lambda values (prob and mu):&quot;, fill = TRUE) print(c(max_lambda_prob, max_lambda_mean)); maxres = calc_max_lambda(ylist = ylist, countslist = countslist, numclust = numclust, maxdev = maxdev, ## This function&#39;s settings max_lambda_prob = max_lambda_prob, max_lambda_mean = max_lambda_mean, ...) print(maxres) save(maxres, file = file.path(destin, maxres_file)) cat(&quot;file was written to &quot;, file.path(destin, maxres_file), fill=TRUE) cat(&quot;maximum regularization value calculation done.&quot;, fill = TRUE) print(Sys.time()) return(maxres) } } The aforementioned workhorse calc_max_lambda() is here. #&#39; Estimate maximum lambda values numerically. First starts with a large #&#39; initial value \\code{max_lambda_mean} and \\code{max_lambda_prob}, and runs #&#39; the EM algorithm on decreasing set of values (sequentially halved). This #&#39; stops once you see non-simple probabilities or means, and returns the *smallest* #&#39; regularization (lambda) value pair that gives full sparsity. #&#39; #&#39; Note that the \\code{zero_stabilize=TRUE} option is used in #&#39; \\code{flowtrend()}, which basically means the EM algorithm runs only until #&#39; the zero pattern stabilizes. #&#39; #&#39; @param ylist List of responses. #&#39; @param numclust Number of clusters. #&#39; @param max_lambda_mean Defaults to 4000. #&#39; @param max_lambda_prob Defaults to 1000. #&#39; @param iimax Maximum value of x for 2^{-x} factors to try. #&#39; @param ... Other arguments to \\code{flowtrend_once()}. #&#39; #&#39; @return list containing the two maximum values to use. #&#39; #&#39; @export calc_max_lambda &lt;- function(ylist, countslist = NULL, numclust, max_lambda_mean = 4000, max_lambda_prob = 1000, verbose = FALSE, iimax = 16, ...){ ## Basic setup: in each dimension, the data should only vary by a relatively ## small amount (say 1/100) dimdat = ncol(ylist[[1]]) toler_by_dim = sapply(1:dimdat, function(idim){ datrange = ylist %&gt;% sapply(FUN = function(y) y %&gt;% .[,idim] %&gt;% range()) %&gt;% range() toler = (datrange[2] - datrange[1])/1E3 }) toler_prob = 1E-3 args = list(...) l = args$l l_prob = args$l_prob ## Get range of regularization parameters. facs = sapply(1:iimax, function(ii) 2^(-ii+1)) ## DECREASING order print(&quot;running the models once&quot;) for(ii in 1:iimax){ cat(&quot;###############################################################&quot;, fill=TRUE) cat(&quot;#### lambda_prob = &quot;, max_lambda_prob * facs[ii], &quot; and lambda = &quot;, max_lambda_mean * facs[ii], &quot;being tested. &quot;, fill=TRUE) cat(&quot;###############################################################&quot;, fill=TRUE) res = flowtrend_once(ylist = ylist, countslist = countslist, numclust = numclust, lambda_prob = max_lambda_prob * facs[ii], lambda = max_lambda_mean * facs[ii], verbose = verbose, ...) ## In each dimension, the data should only vary by a relatively small amount (say 1/100) mean_is_simple = sapply(1:dimdat, FUN = function(idim){ all(abs(diff(res$mn[,idim,], differences = l+1)) &lt; toler_by_dim[idim] * 2^l) }) prob_is_simple = all(abs(diff(res$prob, differences = l_prob+1)) &lt; toler_prob * 2^l_prob) all_are_simple = (all(mean_is_simple) &amp; prob_is_simple) if(!all_are_simple){ ## If there are *any* nonzero values at the first iter, prompt a restart ## with higher initial lambda values. if(ii == 1){ stop(paste0(&quot;Max lambdas: &quot;, max_lambda_mean, &quot; and &quot;, max_lambda_prob, &quot; were too small as maximum reg. values. Go up and try again!!&quot;)) ## If there are *any* nonzero values, return the immediately preceding ## lambda values -- these were the smallest values we had found that gives ## full sparsity. } else { ## Check one more time whether the model was actually zero, by fully running it; res = flowtrend_once(ylist = ylist, countslist = countslist, numclust = numclust, lambda_prob = max_lambda_prob * facs[ii], lambda = max_lambda_mean * facs[ii], ...) ## Check if both curves are maximally simple mean_is_simple = sapply(1:dimdat, FUN = function(idim){ all(abs(diff(res$mn[,idim,], differences = l+1)) &lt; toler_by_dim[idim]) }) prob_is_simple = all(abs(diff(res$prob, differences = l_prob+1)) &lt; toler_prob) all_are_simple = (all(mean_is_simple) &amp; prob_is_simple) ## If there are *any* nonzero values, stop. ## (Otherwise, just proceed to try a smaller set of lambdas.) if(!all_are_simple){ return(list(mean = max_lambda_mean * facs[ii-1], prob = max_lambda_prob * facs[ii-1])) } } } cat(fill=TRUE) } } 5.3 Define CV data folds make_cv_folds() makes the cross-validation “folds”, which are the \\(K\\) (nfold) list of data indices. These are not times! They simply split 1:length(ylist). #&#39; Define the time folds cross-validation. #&#39; #&#39; @param ylist Data. #&#39; @param TT Length of data; if provided, ylist is ignored. #&#39; @param nfold Number of folds. #&#39; @param blocksize Defaults to 1. If larger than 1, creates a set of time folds #&#39; that use contiguous time blocks (by calling #&#39; \\code{make_cv_folds_in_blocks()}). #&#39; @return List of fold indices. #&#39; @export #&#39; make_cv_folds &lt;- function(ylist = NULL, nfold, TT = NULL, blocksize = 1){ if(blocksize &gt; 1){ return(make_cv_folds_in_blocks(ylist = ylist, nfold, TT = TT, blocksize = blocksize)) } ## Make hour-long index list if(is.null(TT)) TT = length(ylist) folds &lt;- rep(1:nfold, ceiling( (TT-2)/nfold))[1:(TT-2)] inds &lt;- lapply(1:nfold, FUN = function(k) (2:(TT-1))[folds == k]) names(inds) = paste0(&quot;Fold&quot;, 1:nfold) return(inds) } We can visualize how the data is to be split. In the following plot, vertical lines mark data indices in each fold, using different colors . For nfold = 5, the first fold is every 5th point starting at 2, \\(\\{2,7,\\dots\\}\\), and the second fold is \\(\\{3,8,\\dots\\}\\), and so forth. Note: the first index \\(1\\) and the last \\(TT\\) are left out at this stage, and instead made available to all folds at training time (in cv_flowtrend()). This is because, otherwise, it would be impossible to make predictions at either ends of the data. nfold = 5 TT = 100 inds = make_cv_folds(nfold = nfold, TT = TT) print(inds) plot(NA, xlim = c(0,TT), ylim=1:2, ylab = &quot;&quot;, xlab = &quot;Data index of ylist&quot;, yaxt = &quot;n&quot;, xaxt=&quot;n&quot;) axis(1, at = c(1, seq(10, 100,10))) for(ifold in 1:nfold){ abline(v = inds[[ifold]], col = ifold, lwd = 2) } This is another function that makes the fold indices but using blocks of time points. #&#39; Define the &quot;blocked&quot; time folds for cross-validation. #&#39; This means that contiguous of times will be used to define CV folds. #&#39; #&#39; The first fold will be ( 1 2 3 16 17 18 31 32 33 46 47 48 61 62 63 76 77 78 #&#39; 91 92 93), the second fold will be (4 5 6 19 20 21 34 35 36 49 50 51 64 65 66 #&#39; 79 80 81 94 95 96), and so forth. #&#39; #&#39; @param nfold Number of folds. #&#39; @param blocksize Size of block (e.g. 3 will produce the example above). #&#39; @return List of fold indices. #&#39; @export #&#39; make_cv_folds_in_blocks &lt;- function(ylist=NULL, nfold, TT=NULL, blocksize){ ## Make hour-long index list if(is.null(TT)) TT = length(ylist) endpoints = round(seq(from = 1, to = TT + blocksize, by = blocksize)) inds = Map(function(begin, end){ if(begin &gt;= TT-1) return(NULL) return(seq(begin+1, pmin(end,TT-1))) }, endpoints[-length(endpoints)], endpoints[-1]) null.elt = sapply(inds, is.null) if(any(null.elt)){ inds = inds[-which(null.elt)] } ## Further make these into (e.g. 5) blocks of test indices. test.ii.list = lapply(1:nfold, function(ifold){ which.test.inds = seq(from = ifold, to = length(inds), by = nfold) test.ii = unlist(inds[which.test.inds]) return(test.ii) }) names(test.ii.list) = paste0(&quot;Fold&quot;, 1:nfold) ## Useful plotting code showing the plots. if(FALSE){ plot(NA, xlim = c(0,TT), ylim=1:2) lapply(1:nfold, function(ifold){ a = test.ii.list[[ifold]]; abline(v=a, col=ifold) }) } return(test.ii.list) } 5.4 CV = many single jobs Next, we build the immediate elements needed for cross-validation. There are two ways in which flowtrend_once() will be applied to data for cross-validation; one is when estimating models from held-in data folds, and the other is when re-estimating models on the full data. Estimating models on the held-in data is done by one_job(). Re-estimating models on the entire dataset is done by one_job_refit(). Here is one_job(). #&#39; Helper function to run ONE job for CV, in iprob, imu, ifold, irestart. #&#39; #&#39; @param iprob Index for prob. #&#39; @param imu Index for beta. #&#39; @param ifold Index for CV folds. #&#39; @param irestart Index for 1 through nrestart. #&#39; @param folds CV folds (from \\code{make_cv_folds()}). #&#39; @param destin Destination directory. #&#39; @param lambda_means List of regularization parameters for mean model. #&#39; @param lambda_probs List of regularization parameters for prob model. #&#39; @param ylist Data. #&#39; @param countslist Counts or biomass. #&#39; @param ... Rest of arguments for \\code{flowtrend_once()}. #&#39; #&#39; @return Nothing is returned. Instead, a file named &quot;1-1-1-1-cvscore.Rdata&quot; #&#39; is saved in \\code{destin}. (The indices here are iprob-imu-ifold-irestart). #&#39; #&#39; @export one_job &lt;- function(iprob, imu, ifold, irestart, folds, destin, lambda_means, lambda_probs, seedtab = NULL, x = NULL, ## The rest that is needed explicitly for flowtrend() ylist, countslist, l, l_prob, ...){ ## Get the train/test data TT &lt;- length(ylist) if(is.null(x)) x = 1:TT test.inds = unlist(folds[ifold]) %&gt;% sort() test.dat = ylist[test.inds] test.count = countslist[test.inds] train.inds = c(1, unlist(folds[-ifold]), TT) %&gt;% sort() train.dat = ylist[train.inds] train.count = countslist[train.inds] ## NEW: fit the model on the *time points* in training indices, and ## NEW: test the model on *time points* in the test indices. ## Check whether this job has been done already. filename = make_cvscore_filename(iprob, imu, ifold, irestart) best_filename = make_best_cvscore_filename(iprob, imu, ifold) ## if(file.exists(file.path(destin, filename)) ){ if(file.exists(file.path(destin, filename)) | file.exists(file.path(destin, best_filename)) ){ cat(fill=TRUE) cat(filename, &quot;already done.&quot;, fill=TRUE) return(NULL) } ## Get the seed ready if(!is.null(seedtab)){ seed = seedtab %&gt;% dplyr::filter(iprob == !!iprob &amp; imu == !!imu &amp; ifold == !!ifold &amp; irestart == !!irestart) %&gt;% dplyr::select(seed1, seed2, seed3, seed4, seed5, seed6, seed7) %&gt;% unlist() %&gt;% as.integer() } else { seed = NULL } lambda_prob = lambda_probs[iprob] lambda_mean = lambda_means[imu] ## Run the algorithm (all this trouble because of |nrestart|) args = list(...) args$ylist = train.dat args$countslist = train.count args$x = x[train.inds] ## NEW args$lambda = lambda_mean args$lambda_prob = lambda_prob args$l = l args$l_prob = l_prob args$seed = seed if(&quot;nrestart&quot; %in% names(args)){ args = args[-which(names(args) %in% &quot;nrestart&quot;)] ## remove |nrestart| prior to feeding to flowtrend_once(). } tryCatch({ ## Estimate model argn &lt;- lapply(names(args), as.name) names(argn) &lt;- names(args) call &lt;- as.call(c(list(as.name(&quot;flowtrend_once&quot;)), argn)) res.train = eval(call, args) ## Assign mn and prob pred = predict_flowtrend(res.train, newtimes = x[test.inds]) ## the x is NEW. stopifnot(all(pred$prob &gt;= 0)) ## Evaluate on test data, by calculating objective (penalized likelihood with penalty parameters set to 0) cvscore = objective(mu = pred$mn, prob = pred$prob, sigma = pred$sigma, ylist = test.dat, countslist = test.count, unpenalized = TRUE) ## Store (temporarily) the run times time_per_iter = res.train$time_per_iter final_iter = res.train$final.iter total_time = res.train$total_time ## Store the results. mn = res.train$mn prob = res.train$prob objectives = res.train$objectives ## Save the CV results save(cvscore, ## Time time_per_iter, final_iter, total_time, ## Results lambda_mean, lambda_prob, lambda_means, lambda_probs, mn, prob, objectives, ## Save the file file = file.path(destin, filename)) return(NULL) }, error = function(err) { err$message = paste(err$message, &quot;\\n(No file will be saved for lambdas (&quot;, signif(lambda_probs[iprob],3), &quot;, &quot;, signif(lambda_means[imu],3), &quot;) whose indices are: &quot;, iprob, &quot;-&quot;, imu, &quot;-&quot;, ifold, &quot;-&quot;, irestart, &quot; .)&quot;,sep=&quot;&quot;) cat(err$message, fill=TRUE) warning(err)}) } Here is one_job_refit(). #&#39; Refit model for one pair of regularization parameter values. Saves to #&#39; \\code{nrestart} files named like &quot;1-4-3-fit.Rdata&quot;, for #&#39; &quot;(iprob)-(imu)-(irestart)-fit.Rdata&quot;. #&#39; #&#39; (Note, \\code{nrestart} is not an input to this function.) #&#39; #&#39; @inheritParams one_job #&#39; #&#39; @export one_job_refit &lt;- function(iprob, imu, destin, lambda_means, lambda_probs, l, l_prob, seedtab = NULL, ## The rest that is needed explicitly for flowtrend_once() ylist, countslist, x, ...){ args = list(...) nrestart = args$nrestart assertthat::assert_that(!is.null(nrestart)) for(irestart in 1:nrestart){ ## Writing file filename = make_refit_filename(iprob = iprob, imu = imu, irestart = irestart) best_filename = make_best_refit_filename(iprob, imu) ## if(file.exists(file.path(destin, filename)) ){ if(file.exists(file.path(destin, filename)) | file.exists(file.path(destin, best_filename))){ cat(filename, &quot;already done.&quot;, fill=TRUE) next } else { ## Get the seed ready if(!is.null(seedtab)){ ifold = 0 seed = seedtab %&gt;% dplyr::filter(iprob == !!iprob, imu == !!imu, ifold == !!ifold, irestart == !!irestart) %&gt;% dplyr::select(seed1, seed2, seed3, seed4, seed5, seed6, seed7) %&gt;% unlist() %&gt;% as.integer() } else { seed = NULL } ## Get the fitted results on the entire data args = list(...) args$ylist = ylist args$countslist = countslist args$x = x args$lambda_prob = lambda_probs[iprob] args$lambda = lambda_means[imu] args$l = l args$l_prob = l_prob args$seed = seed if(&quot;nrestart&quot; %in% names(args)){ ## remove |nrestart| prior to feeding it into flowtrend_once args = args[-which(names(args) %in% &quot;nrestart&quot;)] } ## Call the function. argn &lt;- lapply(names(args), as.name) names(argn) &lt;- names(args) call &lt;- as.call(c(list(as.name(&quot;flowtrend_once&quot;)), argn)) res = eval(call, args) ## Save the results cat(&quot;Saving file here:&quot;, file.path(destin, filename), fill=TRUE) save(res, file=file.path(destin, filename)) } } } Since cross-validation entails running many jobs, we need to index individual “jobs” carefully. Here are some more helpers for indexing: make_iimat(): Make a table whose rows index each “job” (iprob, imu, ifold, irestart), to be used by one_job(). make_iimat_small(): Make a table whose rows index each (iprob, imu, irestart) for re-estimating models, to be used by one_job_refit(). #&#39; Indices for the cross validation jobs. #&#39; #&#39; The resulting iimat looks like this: #&#39; #&#39; ind iprob imu ifold irestart #&#39; 55 6 1 2 1 #&#39; 56 7 1 2 1 #&#39; 57 1 2 2 1 #&#39; 58 2 2 2 1 #&#39; 59 3 2 2 1 #&#39; 60 4 2 2 1 #&#39; @param cv_gridsize CV grid size. #&#39; @param nfold Number of CV folds. #&#39; @param nrestart Number of random restarts of EM algorithm. #&#39; #&#39; @return Integer matrix. #&#39; #&#39; @export make_iimat &lt;- function(cv_gridsize, nfold, nrestart){ iimat = expand.grid(iprob = 1:cv_gridsize, imu = 1:cv_gridsize, ifold = 1:nfold, irestart = 1:nrestart) iimat = cbind(ind = as.numeric(rownames(iimat)), iimat) return(iimat) } #&#39; 2d indices for the cross validation jobs. #&#39; #&#39; The resulting iimat looks like this: #&#39; (#, iprob, imu, irestart) #&#39; 1, 1, 1, 1 #&#39; 2, 1, 2, 1 #&#39; 3, 1, 3, 1 #&#39; #&#39; @inheritParams make_iimat #&#39; #&#39; @return Integer matrix. #&#39; #&#39; @export make_iimat_small &lt;- function(cv_gridsize, nrestart){ iimat = expand.grid(iprob = 1:cv_gridsize, imu = 1:cv_gridsize, irestart = 1:nrestart) iimat = cbind(ind = as.numeric(rownames(iimat)), iimat) return(iimat) } Let’s see the integer matrices that these functions make. make_iimat(cv_gridsize = 5, nfold = 5, nrestart = 10) %&gt;% head() make_iimat_small(cv_gridsize = 5, nrestart = 10) %&gt;% head() Let’s say you have ten cores to run jobs, perhaps on several different computers (“nodes”). In order to divide the jobs up into ten chunks, we’ll make function that splits a matrix iimat into a list of smaller matrices. #&#39; Helper to divide up the jobs in \\code{iimat} into a total of #&#39; \\code{arraynum_max} jobs. The purpose is to divide the jobs, in order to run #&#39; this on a server. #&#39; #&#39; @param arraynum_max Maximum SLURM array number. #&#39; @param iimat matrix whose rows contain CV job indices. #&#39; #&#39; @export make_iilist &lt;- function(arraynum_max, iimat){ iimax = nrow(iimat) if(arraynum_max &gt; iimax){ iilist = lapply(1:iimax, function(a)a) } else { ends = round(seq(from=0, to=iimax, length=arraynum_max+1)) iilist = Map(function(a,b){ (a+1):b}, ends[-length(ends)], ends[-1]) stopifnot(length(unlist(iilist)) == nrow(iimat)) } stopifnot(length(unlist(iilist)) == nrow(iimat)) stopifnot(all(sort(unique(unlist(iilist))) == sort(unlist(iilist)))) return(iilist) } Next, the functions make_cvscore_filename() and make_refit_filename() are used to form the names of the numerous output files. #&#39; Create file name (a string) for cross-validation results. #&#39; @param iprob #&#39; @param imu #&#39; @param ifold #&#39; @param irestart #&#39; #&#39; @export make_cvscore_filename &lt;- function(iprob, imu, ifold, irestart){ filename = paste0(iprob, &quot;-&quot;, imu, &quot;-&quot;, ifold, &quot;-&quot;, irestart, &quot;-cvscore.Rdata&quot;) return(filename) } #&#39; Create file name (a string) for cross-validation results. #&#39; @param iprob #&#39; @param imu #&#39; @param ifold #&#39; @param irestart #&#39; #&#39; @export make_best_cvscore_filename &lt;- function(iprob, imu, ifold){ filename = paste0(iprob, &quot;-&quot;, imu, &quot;-&quot;, ifold, &quot;-best-cvscore.Rdata&quot;) return(filename) } #&#39; Create file name (a string) for re-estimated models for the lambda values #&#39; indexed by \\code{iprob} and \\code{imu}. #&#39; @param iprob #&#39; @param imu #&#39; @param irestart #&#39; #&#39; @export make_refit_filename &lt;- function(iprob, imu, irestart){ filename = paste0(iprob, &quot;-&quot;, imu, &quot;-&quot;, irestart, &quot;-fit.Rdata&quot;) return(filename) } #&#39; Create file name (a string) for re-estimated models for the lambda values #&#39; indexed by \\code{iprob} and \\code{imu}. #&#39; @param iprob #&#39; @param imu #&#39; #&#39; @export make_best_refit_filename &lt;- function(iprob, imu){ filename = paste0(iprob, &quot;-&quot;, imu, &quot;-best-fit.Rdata&quot;) return(filename) } Here’s a useful helper logspace(max, min) to make logarithmically spaced set of numbers, given min and max. We can use this to make a grid of lambda pairs to be used for cross-validation. #&#39; Helper function to logarithmically space out R. \\code{length} values linear #&#39; on the log scale from \\code{max} down to \\code{min}. #&#39; #&#39; @param max Maximum value. #&#39; @param min Minimum value. #&#39; @param length Length of the output string. #&#39; @param min.ratio Factor to multiply to \\code{max}. #&#39; #&#39; @return Log spaced #&#39; #&#39; @export logspace &lt;- function(max, min=NULL, length, min.ratio = 1E-4){ if(is.null(min)) min = max * min.ratio vec = 10^seq(log10(min), log10(max), length = length) stopifnot(abs(vec[length(vec)] - max) &lt; 1E10) return(vec) } 5.5 Running cross-validation Putting the helpers all together, you get the main user-facing function cv_flowtrend(). #&#39; Cross-validation for flowtrend(). Saves results to separate files in #&#39; \\code{destin}. #&#39; #&#39; @param destin Directory where output files are saved. #&#39; @param nfold Number of cross-validation folds. Defaults to 5. #&#39; @param nrestart Number of repetitions. #&#39; @param save_meta If TRUE, save meta data. #&#39; @param lambda_means Regularization parameters for means. #&#39; @param lambda_probs Regularization parameters for probs. #&#39; @param folds Manually provide CV folds (list of time points of data to use #&#39; as CV folds). Defaults to NULL. #&#39; @param mc.cores Use this many CPU cores. #&#39; @param blocksize Contiguous time blocks from which to form CV time folds. #&#39; @param refit If TRUE, estimate the model on the full data, for each pair of #&#39; regularization parameters. #&#39; @param ... Additional arguments to flowtrend(). #&#39; @inheritParams flowtrend_once #&#39; #&#39; @return No return. #&#39; #&#39; @export cv_flowtrend &lt;- function(## Data ylist, countslist, x = NULL, ## THIS IS NEW ## Define the locations to save the CV. destin = &quot;.&quot;, ## Regularization parameter values lambda_means, lambda_probs, l, l_prob, iimat = NULL, ## Other settings maxdev, numclust, nfold, blocksize, nrestart, verbose = FALSE, refit = FALSE, save_meta = FALSE, mc.cores = 1, folds = NULL, seedtab = NULL, niter = 1000, ...){ ## Basic checks stopifnot(length(lambda_probs) == length(lambda_means)) cv_gridsize = length(lambda_means) ## There&#39;s an option to input one&#39;s own iimat matrix. if(is.null(iimat)){ ## Make an index of all jobs if(!refit) iimat = make_iimat(cv_gridsize, nfold, nrestart) if(refit) iimat = make_iimat_small(cv_gridsize, nrestart) } ## Define the CV folds ## folds = make_cv_folds(ylist = ylist, nfold = nfold, blocksize = 1) if(is.null(folds)){ folds = make_cv_folds(ylist = ylist, nfold = nfold, blocksize = blocksize) } else { stopifnot(length(folds) == nfold) } ## Save meta information, once. if(save_meta){ ##if(!refit){ if(file.exists(file = file.path(destin, &#39;meta.Rdata&#39;))){ ## Put aside the current guys cat(fill = TRUE) cat(&quot;Meta data already exists!&quot;) folds_current = folds nfold_current = nfold nrestart_current = nrestart cv_gridsize_current = cv_gridsize lambda_means_current = lambda_means lambda_probs_current = lambda_probs ylist_current = ylist x_current = x countslist_current = countslist ## Load the saved metadata and check if they are all the same as the current guys load(file = file.path(destin, &#39;meta.Rdata&#39;), verbose = FALSE) stopifnot(identical(folds, folds_current)) stopifnot(nfold == nfold_current) stopifnot(nrestart == nrestart_current) ## Added recently stopifnot(cv_gridsize == cv_gridsize_current) stopifnot(all(lambda_means == lambda_means_current)) stopifnot(all(lambda_probs == lambda_probs_current)) stopifnot(identical(ylist, ylist_current)) stopifnot(identical(x, x_current)) stopifnot(identical(countslist, countslist_current)) cat(fill=TRUE) cat(&quot;Successfully checked that the saved metadata is identical to the current one.&quot;, fill = TRUE) } else { save(folds, nfold, nrestart, ## Added recently cv_gridsize, lambda_means, lambda_probs, ylist, countslist, x, ## Save the file file = file.path(destin, &#39;meta.Rdata&#39;)) print(paste0(&quot;wrote meta data to &quot;, file.path(destin, &#39;meta.Rdata&#39;))) } ## } } ## Run the EM algorithm many times, for each value of (iprob, imu, ifold, irestart) start.time = Sys.time() parallel::mclapply(1:nrow(iimat), function(ii){ print_progress(ii, nrow(iimat), &quot;Jobs (EM replicates) assigned on this computer&quot;, start.time = start.time) if(!refit){ iprob = iimat[ii,&quot;iprob&quot;] imu = iimat[ii,&quot;imu&quot;] ifold = iimat[ii,&quot;ifold&quot;] irestart = iimat[ii,&quot;irestart&quot;] ## if(verbose) cat(&#39;(iprob, imu, ifold, irestart)=&#39;, c(iprob, imu, ifold, irestart), fill=TRUE) } else { iprob = iimat[ii, &quot;iprob&quot;] imu = iimat[ii, &quot;imu&quot;] ifold = 0 } if(!refit){ one_job(iprob = iprob, imu = imu, l = l, l_prob = l_prob, ifold = ifold, irestart = irestart, folds = folds, destin = destin, lambda_means = lambda_means, lambda_probs = lambda_probs, ## Arguments for flowtrend() ylist = ylist, countslist = countslist, x = x, ## Additional arguments for flowtrend(). numclust = numclust, maxdev = maxdev, verbose = FALSE, seedtab = seedtab, niter = niter) } else { one_job_refit(iprob = iprob, imu = imu, l = l, l_prob = l_prob, destin = destin, lambda_means = lambda_means, lambda_probs = lambda_probs, ## Arguments to flowtrend() ylist = ylist, countslist = countslist, x = x, ## Additional arguments for flowtrend(). numclust = numclust, maxdev = maxdev, nrestart = nrestart, verbose = FALSE, seedtab = seedtab, niter = niter) } return(NULL) }, mc.cores = mc.cores) } 5.6 Summarizing the output Once the cross-validation is finished (and saved into many files called e.g. 1-1-1-1-cvscore.Rdata or 1-1-1-fit.Rdata), we can use cv_summary() to summarize the results. If you look closely, you’ll notice that cv_aggregate() is the workhorse. #&#39; Main function for summarizing the cross-validation results. #&#39; #&#39; @inheritParams cv_flowtrend #&#39; @param save If TRUE, save to \\code{file.path(destin, filename)}. #&#39; @param filename File name to save to. #&#39; #&#39; @return List containing summarized results from cross-validation. Here are #&#39; some objects in this list: \\code{bestres} is the the overall best model #&#39; chosen from the cross-validation; \\code{cvscoremat} is a 2d matrix of CV #&#39; scores from all pairs of regularization parameters; \\code{bestreslist} is a #&#39; list of all the best models (out of \\code{nrestart} EM replications) from the #&#39; each pair of lambda values. If \\code{isTRUE(save)}, nothing is returned. #&#39; #&#39; @export cv_summary &lt;- function(destin = &quot;.&quot;, save = FALSE, filename = &quot;summary.RDS&quot;){ load(file.path(destin,&#39;meta.Rdata&#39;)) ## This loads all the necessary things: nrestart, nfold, cv_gridsize stopifnot(exists(&quot;nrestart&quot;)) stopifnot(exists(&quot;nfold&quot;)) stopifnot(exists(&quot;cv_gridsize&quot;)) ## Get the results of the cross-validation. a = cv_aggregate(destin) cvscore.mat = a$cvscore.mat cvscore.mat.se = a$cvscore.mat.se min.inds = a$min.inds min.inds.1se = a$min.inds.1se ## Get results from refitting bestreslist = cv_aggregate_res(destin = destin) bestres = bestreslist[[paste0(min.inds[1] , &quot;-&quot;, min.inds[2])]] if(is.null(bestres)){ if(min.inds[1]==2 &amp; min.inds[2]==3) browser() stop(paste0(&quot;The model with lambda indices (&quot;, min.inds[1], &quot;,&quot;, min.inds[2], &quot;) is not available.&quot;)) } out = list(bestres = bestres, cvscore.mat = cvscore.mat, cvscore.mat.se = cvscore.mat.se, min.inds = min.inds, min.inds.1se = min.inds.1se, lambda_means = lambda_means, lambda_probs = lambda_probs, ## List of all best models for all lambda pairs. bestreslist = bestreslist, destin = destin) if(save){ saveRDS(out, file=file.path(destin, filename)) } return(out) } Because the metadata (meta.Rdata) file is a list object, and you want to prevent loading it to your function environment (lest it overwrite existing variables), we write a short helper function to load it. ##&#39; Load contents from the Rdata file |filename| and return a list. ##&#39; This is specifically for when the Rddata file you load contains a /named/ R ##&#39; list. ##&#39; ##&#39; This is basically trying to treat the Rdata file more like an RDS file that contains a list. ##&#39; ##&#39; @param filename Name of the Rdata file ##&#39; ##&#39; @return List object with contents of the Rdata file. ##&#39; @export load_Rdata &lt;- function(filename){ ## loads an RData file, and returns it load(filename, new_env &lt;- new.env()) obj = lapply(ls(envir = new_env), get) names(obj) = ls(new_env) return(obj) } #&#39; From the results saved in \\code{destin}, aggregate all |nrestart| files to retain only the &quot;best&quot; restart, and delete the rest. #&#39; #&#39; All meta information (|nfold|, |cv_gridsize|, |nrestart|, |lambda_means|, |lambda_probs|) comes from \\code{meta.Rdata}. #&#39; #&#39; @param destin Directory with cross-validation output. #&#39; #&#39; @export cv_makebest &lt;- function(destin){ ## ## Read the meta data (for |nfold|, |cv_gridsize|, |nrestart|, |lambda_means|, |lambda_probs|) load(file = file.path(destin, &#39;meta.Rdata&#39;), verbose = FALSE) ## This loads all the necessary things; just double-checking. stopifnot(exists(&quot;nrestart&quot;)) stopifnot(exists(&quot;nfold&quot;)) stopifnot(exists(&quot;cv_gridsize&quot;)) stopifnot(exists(c(&quot;lambda_probs&quot;))) stopifnot(exists(c(&quot;lambda_means&quot;))) ## Aggregate the results cvscore.array = array(NA, dim = c(cv_gridsize, cv_gridsize, nfold, nrestart)) cvscore.mat = matrix(NA, nrow = cv_gridsize, ncol = cv_gridsize) for(iprob in 1:cv_gridsize){ for(imu in 1:cv_gridsize){ for(ifold in 1:nfold){ print(c(iprob, imu, ifold)) ## If the &quot;best&quot; flowtrend object has already been created, do nothing. best_filename = make_best_cvscore_filename(iprob, imu, ifold) if(file.exists(file.path(destin, best_filename))){ next ## Otherwise, attempt to load from all |nrestart| replicates } else { objectives = load_all_objectives(destin, iprob, imu, ifold, nrestart) ## If all |nrestart| files exist, delete all files but the best model. if(all(!is.na(objectives))){ best_irestart = which(objectives == min(objectives)) %&gt;% .[1] ## If there is a tie, leave it. keep_only_best(destin, iprob, imu, ifold, nrestart, best_irestart) } else { print(paste0(&quot;iprob=&quot;, iprob, &quot; imu=&quot;, imu, &quot; ifold=&quot;, ifold, &quot; had objectives: &quot;, objectives)) } } } } } ## Also go over the &quot;refit&quot; files for(iprob in 1:cv_gridsize){ for(imu in 1:cv_gridsize){ ## If the &quot;best&quot; flowtrend object has already been created, do nothing. best_filename = make_best_refit_filename(iprob, imu) if(file.exists(file.path(destin, best_filename))){ ## ## Check if any more jobs have been done since before ## objectives = load_all_refit_objectives(destin, iprob, imu, nrestart) ## if(any(!is.na(objectives))){ ## nonmissing_irestart = which(!is.na(objectives))## == min(objectives)) ## ## keep_only_best_refit(destin, iprob, imu, nrestart, best_irestart) ## load(file.path(destin, best_filename)) ## } else { ## print(paste0(&quot;iprob=&quot;, iprob, &quot; imu=&quot;, imu, &quot; had /refit/ objectives: &quot;, objectives)) ## } ## } next ## Otherwise, attempt to load from all |nrestart| replicates } else { objectives = load_all_refit_objectives(destin, iprob, imu, nrestart) if(all(!is.na(objectives))){ best_irestart = which(objectives == min(objectives)) %&gt;% .[1] ## If there is a tie, leave it. keep_only_best_refit(destin, iprob, imu, nrestart, best_irestart) } else { print(paste0(&quot;iprob=&quot;, iprob, &quot; imu=&quot;, imu, &quot; had /refit/ objectives: &quot;, objectives)) } } } } } #&#39; Loading all objectives, with NA&#39;s for missing files load_all_objectives &lt;- function(destin, iprob, imu, ifold, nrestart){ objectives = sapply(1:nrestart, function(irestart){ filename = make_cvscore_filename(iprob, imu, ifold, irestart) tryCatch({ load(file.path(destin, filename), verbose = FALSE) return(objectives[length(objectives)]) }, error = function(e){ NA }) }) return(objectives) } #&#39; Loading all objectives, with NA&#39;s for missing files load_all_refit_objectives &lt;- function(destin, iprob, imu, nrestart){ objectives = sapply(1:nrestart, function(irestart){ filename = make_refit_filename(iprob, imu, irestart) ## filename = make_best_cvscore_filename(iprob, imu, ifold) tryCatch({ load(file.path(destin, filename), verbose = FALSE) return(res$objectives[length(res$objectives)]) }, error = function(e){ NA }) }) return(objectives) } #&#39; Keeping only the output files for the &quot;best&quot; restart, and deleting the rest. keep_only_best &lt;- function(destin, iprob, imu, ifold, nrestart, best_irestart){ for(irestart in 1:nrestart){ filename = make_cvscore_filename(iprob, imu, ifold, irestart) if(irestart == best_irestart){ best_filename = make_best_cvscore_filename(iprob, imu, ifold) file.rename(from = file.path(destin, filename), to = file.path(destin, best_filename)) } else { file.remove(file.path(destin, filename)) } } } #&#39; Keeping only the output files (among refit models) for the &quot;best&quot; restart, #&#39; and deleting the rest. keep_only_best_refit &lt;- function(destin, iprob, imu, nrestart, best_irestart){ for(irestart in 1:nrestart){ filename = make_refit_filename(iprob, imu, irestart) if(irestart == best_irestart){ best_filename = make_best_refit_filename(iprob, imu) file.rename(from = file.path(destin, filename), to = file.path(destin, best_filename)) } else { file.remove(file.path(destin, filename)) } } } #&#39; Aggregate CV scores from the results, saved in \\code{destin}. #&#39; #&#39; @param destin Directory with cross-validation output. #&#39; #&#39; @export cv_aggregate &lt;- function(destin){ ## ## Read the meta data (for |nfold|, |cv_gridsize|, |nrestart|, |lambda_means|, ## ## |lambda_probs|) load(file = file.path(destin, &#39;meta.Rdata&#39;), verbose = FALSE) ## This loads all the necessary things; just double-checking. stopifnot(exists(&quot;nrestart&quot;)) stopifnot(exists(&quot;nfold&quot;)) stopifnot(exists(&quot;cv_gridsize&quot;)) stopifnot(exists(c(&quot;lambda_probs&quot;))) stopifnot(exists(c(&quot;lambda_means&quot;))) ## Aggregate the results cvscore.array = array(NA, dim = c(cv_gridsize, cv_gridsize, nfold, nrestart)) cvscore.mat = matrix(NA, nrow = cv_gridsize, ncol = cv_gridsize) cvscore.mat.se = matrix(NA, nrow = cv_gridsize, ncol = cv_gridsize) for(iprob in 1:cv_gridsize){ for(imu in 1:cv_gridsize){ obj = matrix(NA, nrow=nfold, ncol=nrestart) for(ifold in 1:nfold){ ## If the &quot;best&quot; flowtrend object has already been created, use it. best_filename = make_best_cvscore_filename(iprob, imu, ifold) if(file.exists(file.path(destin, best_filename))){ load(file.path(destin, best_filename), verbose = FALSE) cvscore.array[iprob, imu, ifold, 1] = cvscore obj[ifold, 1] = objectives[length(objectives)] ## Otherwise, aggregate directly from the individual files } else { for(irestart in 1:nrestart){ filename = make_cvscore_filename(iprob, imu, ifold, irestart) tryCatch({ load(file.path(destin, filename), verbose = FALSE) cvscore.array[iprob, imu, ifold, irestart] = cvscore obj[ifold, irestart] = objectives[length(objectives)] }, error = function(e){}) } } } ## Pick out the CV scores with the *best* (lowest) objective value cvscores = cvscore.array[iprob, imu , , ] ## nfold x nrestart best.models = apply(obj, 1, function(myrow){ ind = which(myrow == min(myrow, na.rm=TRUE)) if(length(ind)&gt;1) ind = ind[1] ## Just choose one, if there is a tie. return(ind) }) %&gt;% as.numeric() ## best.models is the *best* model out of nrestarts, for each fold. ## final.cvscores[ifold,] has the |nrestart| final cv scores for that fold. ## cvscores[ifold, best.models[ifold]] is the best CV score for that fold. final.cvscores = sapply(1:nfold, function(ifold){ cvscores[ifold, best.models[ifold]] ## Why did we get rid of this? ## cvscores[ifold] }) cvscore.mat[iprob, imu] = mean(final.cvscores) cvscore.mat.se[iprob, imu] = sd(final.cvscores) } } ## Clean a bit cvscore.mat[which(is.nan(cvscore.mat), arr.ind = TRUE)] = NA ## ## Read the meta data (for |nfold|, |cv_gridsize|, |nrestart|) rownames(cvscore.mat) = signif(lambda_probs,3) colnames(cvscore.mat) = signif(lambda_means,3) ## Find the minimum mat = cvscore.mat min.inds = which(mat == min(mat, na.rm = TRUE), arr.ind = TRUE) ## Find the 1SE minimum index too mat = cvscore.mat ## sdmat = (out$cvscore.mat) ## This is fake ## sdmat[] = sd(out$cvscore.mat) ## This is fake sdmat = cvscore.mat.se ## This is fake upper = mat[min.inds] + sdmat[min.inds] possible_inds = which(mat &lt; upper, arr.ind=TRUE) ## ## Because there is no good heuristic, we&#39;ll just go up in both directions. ## my_ord = order(abs(possible_inds[,2] - possible_inds[,1])) ## possible_inds[my_ord, ] %&gt;% apply(2, which.max) ## Or even just stick with the ones on the diagonal direction. myrows = which(possible_inds[,2] == possible_inds[,1]) possible_inds = possible_inds[myrows, ,drop=FALSE] min.inds.1se = possible_inds[which.max(possible_inds[,1]),] ## Return the results out = list(cvscore.array = cvscore.array, cvscore.mat = cvscore.mat, cvscore.mat.se = cvscore.mat.se, lambda_means = lambda_means, lambda_probs = lambda_probs, min.inds = min.inds, min.inds.1se = min.inds.1se) return(out) } #&#39; Helper to aggregate CV results and obtain the |res| object, all saved in #&#39; |destin|. #&#39; #&#39; @inheritParams cv_aggregate #&#39; #&#39; @return List containing, for every (iprob, imu), the &quot;best&quot; estimated model #&#39; out of the |nrestart| replicates (best in the sense that it had the best #&#39; likelihood value out of the |nrestart| replicates.) cv_aggregate_res &lt;- function(destin){ load(file.path(destin, &quot;meta.Rdata&quot;)) ## df.mat = matrix(NA, ncol=cv_gridsize, nrow=cv_gridsize) res.list = list() for(iprob in 1:cv_gridsize){ for(imu in 1:cv_gridsize){ ## If the &quot;best&quot; object has already been created, use it. best_refit_filename = make_best_refit_filename(iprob, imu) if(file.exists(file.path(destin, best_refit_filename))){ load(file.path(destin, best_refit_filename), verbose = FALSE) bestres = res ## Otherwise, aggregate directly from the individual files } else { obj = rep(NA, nrestart) res.list.inner = list() for(irestart in 1:nrestart){ filename = make_refit_filename(iprob, imu, irestart) tryCatch({ load(file.path(destin, filename)) res.list.inner[[irestart]] = res obj[irestart] = res$objectives[length(res$objectives)] }, error = function(e){ NULL }) } if(!all(is.na(obj))){ bestres = res.list.inner[[which.min(obj)]] ## which.min? } } ## Add the &quot;best&quot; object to a list. res.list[[paste0(iprob, &quot;-&quot;, imu)]] = bestres } } return(res.list) } 5.7 CV on your own computer Using all this functionality, we’d like to be able to cross-validate on our own laptop, using cv_flowtrend(). Let’s try it out. cv_gridsize = 3 l = 1 l_prob = 1 set.seed(332) ylist = gendat_1d(10, rep(100,10)) %&gt;% dt2ylist() plot_1d(ylist) folds = make_cv_folds(lapply(1:100, function(ii) cbind(runif(10))), nfold = 5, TT = length(ylist)) ## What is this doing? FISHY lambda_means = lambda_probs = logspace(min = 1E-5, max = 1, length = cv_gridsize) for(refit in c(FALSE, TRUE)){ cv_flowtrend(ylist = ylist, countslist = NULL, destin = &quot;~/repos/flowtrend/inst/output/test&quot;, lambda_means = lambda_means, lambda_probs = lambda_probs, l = l, l_prob = l_prob, maxdev = NULL, numclust = 3, nfold = 3, nrestart = 3, niter = 3, verbose = TRUE, refit = refit, save_meta = TRUE, mc.cores = 6) } Once all jobs are run, use cv_makebest() to go over all the output, and (1) retain the best model out of the 5 replicates (random restarts), and (2) delete 4 of the the 5 replicates. ## Clean the output directory to keep only the &quot;best&quot; results cv_makebest(destin = &quot;~/repos/flowtrend/inst/output/test&quot;) ## Plot the final result obj = cv_summary(destin = &quot;~/repos/flowtrend/inst/output/test&quot;) plot_1d(ylist, obj$bestres) Okay, now we know it’s possible to run on (say) a laptop. A more realistic application would go something like this: litr::load_all(&quot;~/repos/flowtrend/index.Rmd&quot;) get_max_lambda(&quot;~/repos/flowtrend/inst/output/test&quot;, maxres_file = &quot;maxres.Rdata&quot;, ylist, countslist = NULL, numclust = 3, maxdev = 1, max_lambda_prob = 3000000, max_lambda_mean = 3000000, verbose = TRUE, l = 1, l_prob = 1) load(&quot;maxres.Rdata&quot;, verbose = TRUE) ## Loads &quot;maxres&quot; object lambda_means = logspace(max = maxres$means, length = 5) lambda_probs = logspace(max = maxres$prob, length = 5) for(refit in c(FALSE, TRUE)){ cv_flowtrend(ylist = ylist, countslist = NULL, destin = &quot;~/repos/flowtrend/tempoutput&quot;, lambda_means = lambda_means, lambda_probs = lambda_probs, l = 2, l_prob = 1, maxdev = 2, numclust = 3, nfold = 5, nrestart = 5, verbose = TRUE, refit = refit, save_meta = TRUE, mc.cores = 6) } Next, we test whether cv_flowtrend works for unevenly spaced points. ## Setup l = 1 l_prob = 1 destin = &quot;~/repos/flowtrend/inst/output/test2&quot; ## Generate data set.seed(332) TT = 50 nt = 100 ylist = gendat_1d(TT, ntlist = rep(nt, TT)) %&gt;% dt2ylist() x = sample(1:TT, size=TT/2, replace = FALSE) %&gt;% sort() ylist = ylist[x] ## Save the data datobj = list(ylist = ylist, x = x, countslist = NULL) saveRDS(datobj, file = file.path(destin, &quot;datobj.RDS&quot;)) folds = make_cv_folds(ylist, nfold = nfold, TT = length(ylist)) ## What is this doing? FISHY lambda_means = lambda_probs = logspace(min = 1E-5, max = 1, length = 5) for(refit in c(FALSE, TRUE)){ cv_flowtrend(ylist = ylist, countslist = NULL, x = x, destin = destin, lambda_means = lambda_means, lambda_probs = lambda_probs, l = l, l_prob = l_prob, maxdev = 1, numclust = 3, nfold = 3, nrestart = 3, niter = 100, folds = folds, verbose = TRUE, refit = refit, save_meta = TRUE, mc.cores = 4) } cvres = cv_summary(destin = destin, save=TRUE) my_load() plot_1d(ylist = ylist, x=x) %&gt;% plot_1d_add_model(obj = cvres$bestres, idim=1, plot_band = TRUE) ## Plot it! plot_1d(ylist = ylist, x=x) %&gt;% plot_1d_add_model(obj = cvres$bestres, idim=1, plot_band = TRUE) + geom_line(aes(x=time, y=mean, group = cluster), data = gendat_1d(TT, ntlist = rep(nt, TT), return_model = TRUE) ) 5.8 Plotting This function plot_from_summary() creates summary plots from the results of the cross-validation. #&#39; Makes a series of plots. #&#39; #&#39; @param destin This directly has (1) summary.RDS, (2) meta.Rdata, and (3) datobj.RDS; optionally, it may have a dt_model #&#39; @return Nothing; but the pdf plots are created. #&#39; #&#39; @export plot_from_summary &lt;- function(destin){ ## Load the results cvres = readRDS(file.path(destin, paste0(&quot;summary.RDS&quot;))) load(file.path(destin, paste0(&quot;meta.Rdata&quot;)), verbose=TRUE) ## datobj = readRDS(file.path(destin, paste0(&quot;datobj.RDS&quot;))) if(file.exists(file.path(destin, &quot;dt_model.RDS&quot;))){ dt.model = readRDS(file.path(destin, paste0(&quot;dt_model.RDS&quot;))) } else { dt.model = NULL } ##ylist = datobj$ybin_list %&gt;% lapply(cbind) ##countslist = datobj$counts_list ## rows are prob, cols are means pdf(file.path(destin, &quot;cvscoremat.pdf&quot;), width = 6, height = 6) cvscoremat_lattice_plot = cvres$cvscore.mat %&gt;% drawmat_precise(ylab = &quot;lambda_prob&quot;, xlab = &quot;lambda_mean&quot;) + latticeExtra::layer(lattice::panel.points(x = cvres$min.inds[2], y = nrow(cvres$cvscore.mat) + 1 - cvres$min.inds[1], pch = 4, col = &quot;orange&quot;, cex=2)) + latticeExtra::layer(lattice::panel.points(x = cvres$min.inds.1se[2], y = nrow(cvres$cvscore.mat) + 1 - cvres$min.inds.1se[1], pch = 4, col = &quot;orange&quot;, cex=2)) plot(cvscoremat_lattice_plot) ## Need to load library(lattice) before this. graphics.off() ## The minimum values. min_lam_prob = lambda_probs[cvres$min.inds[2]] min_lam_mean = lambda_means[cvres$min.inds[1]] ## each line is a different lambda_mean pdf(file.path(destin, &quot;cvscoremat-lines-over-means.pdf&quot;), width = 7, height = 7) a = cvres$cvscore.mat %&gt;% t() %&gt;% matplot(x = lambda_probs, type=&#39;o&#39;, col = &#39;grey&#39;, main = &quot;each line is a different lambda_mean&quot;, log = &quot;x&quot;) abline(v=min_lam_prob, col = &#39;red&#39;) graphics.off() pdf(file.path(destin, &quot;cvscoremat-lines-over-probs.pdf&quot;), width = 7, height = 7) cvres$cvscore.mat %&gt;% matplot(x=lambda_means, type=&#39;o&#39;, col = &#39;grey&#39;, main = &quot;each line is a different lambda_prob&quot;, log = &quot;x&quot;) abline(v=min_lam_mean, col = &#39;red&#39;) graphics.off() n_lam_prob = nrow(cvres$cvscore.mat) n_lam_mean = ncol(cvres$cvscore.mat) ## Reorder clusters cvres$bestres = reorder_clust(cvres$bestres) for(ii in 1:n_lam_prob){ print(ii) for(jj in 1:n_lam_mean){ ## Reorder the cluster labels of the fitted (&quot;best&quot;) models. reordered_obj = reorder_kl(newres = cvres$bestreslist[[paste0(ii, &quot;-&quot;, jj)]], origres = cvres$bestres,##cvres$bestreslist[[&quot;1-1&quot;]], ylist_particle = datobj$ylist) cvres$bestreslist[[paste0(ii, &quot;-&quot;, jj)]] = reordered_obj } } ## Dependign on the dimension of the data, make a different plot ## if(obj$dimdat == 1) idim = 1 ## if(obj$dimdat &gt; 1) idim = 1 for(idim in 1:3){ print(idim) ## Make plots of the &quot;best&quot; model g1 = flowtrend::plot_1d(ylist = ylist, countslist = countslist, obj=cvres$bestres, idim = idim) if(!is.null(dt.model)){ g1 = g1 + geom_line(aes(x = time, y = mean, group = cluster), data = dt.model,## %&gt;% subset(time %ni% held_out), linetype = &quot;dashed&quot;, size=1, alpha = .7) } g2 = flowtrend::plot_prob(cvres$bestres, x=1:length(ylist)) if(!is.null(dt.model)){ g2 = g2 + geom_line(aes(x=time, y=prob, group = cluster), linetype = &quot;dashed&quot;, data=dt.model %&gt;% select(time, cluster, prob) %&gt;% unique()) } do.call(ggpubr::ggarrange, c(list(g1, g2), ncol = 1, nrow = 2)) -&gt; g g %&gt;% ggsave(file = file.path(destin, paste0(&quot;mean-and-prob-idim-&quot;, idim, &quot;.pdf&quot;)), width = 10, height=7) } ## Make all model plots lambda_means = colnames(cvres$cvscore.mat) %&gt;% as.numeric() lambda_probs = rownames(cvres$cvscore.mat) %&gt;% as.numeric() for(idim in 1:3){ kk = 1 glist = list() for(ii in 1:n_lam_prob){ for(jj in 1:n_lam_mean){ ## Calculate number of knots (complexity, df) knots_mean = sapply(1:2, function(iclust){ abs_means = cvres$bestreslist[[paste0(ii, &quot;-&quot;, jj)]] %&gt;% .$mn %&gt;% .[,1,iclust] %&gt;% diff(differences=3) %&gt;% abs() df = sum(abs_means &gt; 1E-4) }) knots_prob = sapply(1:2, function(iclust){ abs_probs = cvres$bestreslist[[paste0(ii, &quot;-&quot;, jj)]] %&gt;% .$prob_link %&gt;% .[,iclust] %&gt;% diff(differences=2) %&gt;% abs() df = sum(abs_probs &gt; 1E-4) }) ## Visualize g = flowtrend::plot_1d(ylist = ylist, countslist = countslist, obj = cvres$bestreslist[[paste0(ii, &quot;-&quot;, jj)]], idim = idim) g = g + ggtitle(paste0(&quot;lam=&quot;, signif(lambda_means[[jj]],3), &quot;, lam_prob=&quot;, signif(lambda_probs[[ii]], 3))) g = g + theme(legend.position=&quot;none&quot;) g = g + annotate(&quot;text&quot;, x=Inf, y = Inf, label = paste0(&quot;knots(prob,mean)=&quot;, knots_prob,&quot;, &quot;,knots_mean), vjust=1, hjust=1) optimal = ((ii == cvres$min.inds[1]) &amp; (jj == cvres$min.inds[2])) onese = ((ii == cvres$min.inds.1se[1]) &amp; (jj == cvres$min.inds.1se[2])) if(optimal | onese){ g = g + theme(panel.border = element_rect(colour = &quot;black&quot;, fill=NA, size=5)) } ## Save plot glist[[kk]] = g kk = kk + 1 } } all_bestres = do.call(ggpubr::ggarrange, c(glist, ncol = n_lam_mean, nrow = n_lam_prob)) all_bestres %&gt;% ggsave(file = file.path(destin, paste0(&quot;all_bestres-idim-&quot;, idim, &quot;.pdf&quot;)), width = 40, height=30) } ## All probabilities lambda_means = colnames(cvres$cvscore.mat) %&gt;% as.numeric() kk = 1 glist = list() for(ii in 1:n_lam_prob){ for(jj in 1:n_lam_mean){ ## Visualize g = flowtrend::plot_prob(obj = cvres$bestreslist[[paste0(ii, &quot;-&quot;, jj)]], x=1:length(ylist)) g = g + ggtitle(paste0(&quot;lam=&quot;, signif(lambda_means[[jj]],3), &quot;, lam_prob=&quot;, signif(lambda_probs[[ii]], 3))) if(!is.null(dt.model)){ g = g + geom_line(aes(x = time, y = prob, group = cluster),##, color = cluster), data = dt.model, linetype = &quot;dashed&quot;) } g = g + theme(legend.position = &quot;none&quot;) optimal = ((ii == cvres$min.inds[1]) &amp; (jj == cvres$min.inds[2])) onese = ((ii == cvres$min.inds.1se[1]) &amp; (jj == cvres$min.inds.1se[2])) if(optimal | onese){ g = g + theme(panel.border = element_rect(colour = &quot;black&quot;, fill=NA, size=5)) } ## Save plot glist[[kk]] = g kk = kk + 1 } } all_probs = do.call(ggpubr::ggarrange, c(glist, ncol=n_lam_mean, nrow=n_lam_prob)) all_probs %&gt;% ggsave(file = file.path(destin, paste0(&quot;all_probs.pdf&quot;)), width = 40, height=30) list_of_plots = list(all_probs = all_probs, all_bestres = all_bestres, cvscoremat_lattice_plot = cvscoremat_lattice_plot , mean_plot = g1, prob_plot = g2) return(list_of_plots) } "],["testing-the-flowtrend-method.html", "6 Testing the flowtrend method 6.1 1d example 6.2 Testing monotonicity of objective values 6.3 2d example 6.4 Working with “binned” dataset 6.5 Unevenly spaced inputs (x)", " 6 Testing the flowtrend method We’re going to assume the flowtrend() function has been built, and test it now. 6.1 1d example Generate data. set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), offset = 4) dt_model &lt;- gendat_1d(100, rep(100, 100), return_model = TRUE, offset=4) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() plot_1d(ylist) dt_model %&gt;% select(time, cluster, prob) %&gt;% ggplot() + geom_line(aes(x=time, y=prob, group=cluster, col = cluster)) Next, we fit the model. set.seed(18) obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 1, numclust = 3, l = 2, l_prob = 1, lambda = .1, lambda_prob = .1, nrestart = 5) ## Also reorder the cluster labels of the truth, to match the fitted model. ord = obj$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) lookup &lt;- setNames(c(1:obj$numclust), ord) dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %&gt;% as.factor() ## Reorder the cluster lables of the fitted model. obj = reorder_clust(obj) The data and estimated model are shown here. The dashed lines are the true means. plot_1d(ylist = ylist, obj = obj, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model,## %&gt;% subset(time %ni% held_out), linetype = &quot;dashed&quot;, size=2, alpha = .7) The estimated probabilities are shown here. plot_prob(obj=obj, x=x) + geom_line(aes(x = time, y = prob, group = cluster, color = cluster), data = dt_model, linetype = &quot;dashed&quot;) + facet_wrap(~cluster) 6.2 Testing monotonicity of objective values The objective value (that is, the penalized log likelihood) should be monotone decreasing across EM algorithm iterations. testthat::test_that(&quot;Objective value decreases over EM iterations.&quot;,{ for(iseed in 1:5){ print(iseed) ## Generate synthetic data set.seed(iseed*100) dt &lt;- gendat_1d(100, rep(10, 100)) dt_model &lt;- gendat_1d(100, rep(10, 100), return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() ## Fit model obj &lt;- flowtrend_once(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = 0.05) ## Test objective monotonicity niter_end = length(obj$objective) testthat::expect_true(all(diff(obj$objective) &lt; 1E-4)) ## Make a plot g = ggplot(tibble(iter=1:niter_end, objective=obj$objectives)) + geom_point(aes(x=iter, y=objective)) + geom_line(aes(x=iter, y=objective)) + ggtitle(paste0(&quot;Seed=&quot;, iseed*100)) print(g) } }) 6.3 2d example Next, we try out flowtrend on a synthetic 2d data example. set.seed(100) TT = 100 dt &lt;- gendat_2d(TT, rep(100, times = TT)) x = 1:TT set.seed(10) obj &lt;- flowtrend_once(ylist = dt$ylist, x = x, maxdev = 3, numclust = 3, l = 2, l_prob = 2, lambda = 0.01, lambda_prob = .01, rho_init = 0.01) The are some snapshots in time \\(t\\). timelist = c(10, 20, 30, 40, 60, 80) plist = lapply(timelist, function(tt){ plot_2d(dt$ylist, obj = obj, tt = tt) + coord_fixed() + ylim(-6, 3) + xlim(-6, 3) }) do.call(ggpubr::ggarrange, c(plist, ncol=3, nrow=2)) The plot above shows 1d projections in the two dimensions of the response data. You can use the idim argument in plot_1d() with a 2d model obj and dataset ylist. g1 = plot_1d(ylist = dt$ylist, obj = obj, idim = 2) + ylim(c(-7, 4)) g2 = plot_1d(ylist = dt$ylist, obj = obj, idim = 1) + ylim(c(-7, 4)) do.call(ggpubr::ggarrange, c(list(g1, g2), ncol=1, nrow=2)) 6.4 Working with “binned” dataset Recall that “binning” means we will use binned frequency histogram estimates of the original particle-level dataset; this is useful when there are many particles. ## library(flowmix) set.seed(10232) TT = 100 dt &lt;- gendat_2d(TT, rep(100, times = TT)) manual_grid = flowmix::make_grid(dt$ylist, gridsize = 20) binres = flowmix::bin_many_cytograms(dt$ylist, manual.grid = manual_grid) set.seed(100) obj = flowtrend(ylist = binres$ybin_list, countslist = binres$counts_list, maxdev = 2, numclust = 3, l = 2, l_prob = 2, lambda = .01, lambda_prob = .005, rho_init = .01, nrestart = 1) ## Plot cluster means g1 = plot_1d(ylist = dt$ylist, obj = obj, idim = 2) + ylim(c(-7, 4)) + ggtitle(&quot;Estimated data and model, 2nd dimension&quot;) g2 = plot_1d(ylist = dt$ylist, obj = obj, idim = 1) + ylim(c(-7, 4)) + ggtitle(&quot;Estimated data and model, 1st dimension&quot;) do.call(ggpubr::ggarrange, c(list(g1, g2), ncol=1, nrow=2)) Also plot the cluster probabilities: plot_prob(obj) ## true_prob_long = dt$probs %&gt;% as_tibble() %&gt;% add_column(time=1:TT) %&gt;% pivot_longer(-time) ## plot_prob(obj) + geom_line(aes(x=time,y=value,group=name), data = true_prob_long) 6.5 Unevenly spaced inputs (x) Let’s try the EM algorithm out with unevenly spaced inputs. set.seed(100) dt &lt;- gendat_1d(TT=100, rep(100, times=100)) dt_model &lt;- gendat_1d(TT=100, rep(100, times=100), return_model = TRUE) ylist_orig = dt %&gt;% dt2ylist() ## Two ways of removing some time points ind_rm_list = list(seq(from=10, to=100, by=10), ind_rm = 30:50) ## Try both ways, and see that the objective values objlist = list() for(ii in 1:2){ ind_rm = ind_rm_list[[ii]] x = (1:100)[-ind_rm] ylist = ylist_orig[x] set.seed(100) obj &lt;- flowtrend_once(ylist = ylist, x = x, maxdev = 100, numclust = 3, l = 2, l_prob = 2, lambda = 0.1, lambda_prob = 0.1, ## admm_local_adapt = TRUE, rho_init = 0.01) objlist[[ii]] = obj } ## Plot both results (and objectives) ii = 1 ind_rm = ind_rm_list[[ii]] x = (1:100)[-ind_rm] plot_1d(obj = objlist[[1]], ylist = ylist_orig[x], x = x) objlist[[1]]$objectives %&gt;% plot(type=&#39;o&#39;, ylab = &quot;EM objectives&quot;) ii=2 ind_rm = ind_rm_list[[ii]] x = (1:100)[-ind_rm] plot_1d(obj = objlist[[2]], ylist = ylist_orig[x], x = x) objlist[[2]]$objectives %&gt;% plot(type=&#39;o&#39;, ylab = &quot;EM objectives&quot;) "],["helpers-for-simulations.html", "7 Helpers for simulations 7.1 Two alternative clusterings 7.2 Example using other cluster algorithms 7.3 Oracle 7.4 Evaluating performance: soft RAND index 7.5 Generating “pseudo-real” data 7.6 Miscellaneous helpers", " 7 Helpers for simulations There are two alternatives to consider when gating (classifying) points in a series of cytograms. The first is to estimate the clusterings to be identical across all time points. This is equivalent to collapsing all cytograms into one and clustering it. The second alternative is to estimate clusters in each cytogram, then connect the cytograms as much as one can, e.g., by finding similar clusters across time points and combining them. We use the flowmeans method (https://www.bioconductor.org/packages/release/bioc/html/flowMeans.html) to to cluster the cytograms in this section. 7.1 Two alternative clusterings We will call the two alternatives underfit_flowmean() and overfit_flowmean(). #&#39; Pool the entire series of cytograms, fit a flowMeans model, and re-aggregate parameters #&#39; #&#39; @param ylist Data. #&#39; @param numclust Number of clusters. #&#39; @return List object with flowtrend model estimates using l=lprob=0 and #&#39; extremely large regularization parameters so that all cytograms are the #&#39; same across time. #&#39; @export underfit_flowmeans &lt;- function(ylist, numclust){ ## Pool all data TT &lt;- length(ylist) nt &lt;- sapply(ylist, nrow) ylist_pooled &lt;- do.call(rbind, ylist) %&gt;% as_tibble() colnames(ylist_pooled) = &quot;Y&quot; ## Fit the model fmns_pooled &lt;- flowMeans::flowMeans(x = ylist_pooled, NumC = numclust) labeled_ylist &lt;- data.frame(Y = ylist_pooled$Y, Cluster = fmns_pooled@Label, Time = rep(1:TT, times = nt)) ## Separate out list of memberships memlist = labeled_ylist %&gt;% group_by(Time) %&gt;% group_split() %&gt;% lapply(function(a)pull(a, Cluster)) ## Format nicer output for objective calculations params &lt;- labeled_ylist %&gt;% group_by(Cluster, Time) %&gt;% summarise(mu = mean(Y), prob = n(), sigma = var(Y)) %&gt;% group_by(Time) %&gt;% mutate(prob = prob/sum(prob)) mu &lt;- matrix(nrow = TT, ncol = numclust) sigma &lt;- matrix(nrow = TT, ncol = numclust) prob &lt;- matrix(nrow = TT, ncol = numclust) ## Fill in missing times for some clusters for(tt in 1:TT){ params.tt &lt;- params %&gt;% filter(Time == tt) for(iclust in 1:numclust){ mu[tt,iclust] &lt;- params.tt$mu[iclust] sigma[tt,iclust] &lt;- params.tt$sigma[iclust] prob[tt,iclust] &lt;- params.tt$prob[iclust] if(is.na(mu[tt,iclust])){ mu[tt,iclust] &lt;- mu[tt-1,iclust] } if(is.na(sigma[tt,iclust])){ sigma[tt,iclust] &lt;- sigma[tt-1,iclust] } if(is.na(prob[tt,iclust])){ prob[tt,iclust] &lt;- 0 } } } return(list(labeled_ylist = labeled_ylist, memlist = memlist, params = params, mu = mu, prob = prob, sigma = sigma, numclust = numclust)) } #&#39; Pool the entire series of cytograms, fit a GMM model, and re-aggregate parameters. #&#39; #&#39; @param ylist Data. #&#39; @param numclust Number of clusters. #&#39; @return #&#39; #&#39; @export underfit_gmm &lt;- function(ylist, numclust){ ## ## Tempoerary ## isignal = 0 ## isim = 1 ## destin = file.path(&quot;~/repos/flowtrend/inst/output/1dsim-even/data&quot;) ## datobj = readRDS(file = file.path(destin, paste0(&quot;isignal-&quot;, isignal,&quot;-isim-&quot;, isim, &quot;-&quot;, &quot;datobj.RDS&quot;)) ) ## datobj_new = readRDS(file = file.path(destin, paste0(&quot;isignal-&quot;, isignal,&quot;-isim-&quot;, isim, &quot;-&quot;, &quot;datobj-new.RDS&quot;)) ) ## ylist = datobj$ylist_unsorted ## numclust = 2 ## ## end of temporary ## Pool all data TT &lt;- length(ylist) nt &lt;- sapply(ylist, nrow) ylist_pooled &lt;- do.call(rbind, ylist) %&gt;% as_tibble() colnames(ylist_pooled) = &quot;y&quot; ## Fit the GMM model gmm_pooled &lt;- mclust::Mclust(data = ylist_pooled, G = numclust, modelNames = &quot;V&quot;, verbose = FALSE) ## Memberships (soft- and hard-clustered) hard_mem = gmm_pooled$z %&gt;% apply(1, which.max) soft_mem = gmm_pooled$z %&gt;% apply(1, function(myrow){ sample(1:2, size = 1, replace = FALSE, prob=myrow) }) ## Put together in a table tab &lt;- tibble(y = ylist_pooled$y, soft_cluster = factor(soft_mem, levels = c(2,1)), hard_cluster = factor(hard_mem, levels = c(2,1)), time = rep(1:TT, times = nt)) tab_long = tab %&gt;% pivot_longer(-c(&quot;time&quot;, &quot;y&quot;), values_to = &quot;cluster&quot;, names_to = &quot;type&quot;) ## ## Optional plotting code ## ggplot(tab_long) + ## geom_point(aes(x=time, y=y, col = cluster), alpha = .5) + ## facet_grid(cluster~type) ## ## We need this to return the same mu, prob, and sigma as before ## mu = obj_flowtrend$mn ## prob = obj_flowtrend$prob ## sigma = obj_flowtrend$sigma ## That&#39;s it! Return the results ## param_mat = matrix(NA, nrow = TT, ncol = 6) ## colnames(param_mat) = c(&quot;time&quot;, &quot;mn1&quot;, &quot;mn2&quot;, &quot;prob1&quot;, &quot;prob2&quot;, &quot;sd1&quot;, &quot;sd2&quot;) ## param_mat$time = 1:TT param_mat = tibble(time=1:TT, mn1 = gmm_pooled$parameters$mean[1], mn2 = gmm_pooled$parameters$mean[2], sd1 = gmm_pooled$parameters$variance$sigmasq[1], sd2 = gmm_pooled$parameters$variance$sigmasq[2], prob1 = gmm_pooled$parameters$pro[1], prob2 = gmm_pooled$parameters$pro[2]) mu = param_mat[,c(&quot;mn1&quot;, &quot;mn2&quot;)] %&gt;% as.matrix() prob = param_mat[,c(&quot;prob1&quot;, &quot;prob2&quot;)] %&gt;% as.matrix() sigma = param_mat[,c(&quot;sd1&quot;, &quot;sd2&quot;)] %&gt;% as.matrix() ## Soft membership memlist = tab_long %&gt;% subset(type == &quot;soft_cluster&quot;) %&gt;% group_by(time) %&gt;% group_split() %&gt;% lapply(function(a)pull(a, cluster)) ## Responsibilities soft_mem = gmm_pooled$z %&gt;% as_tibble() colnames(soft_mem) = c(&quot;clust1&quot;, &quot;clust2&quot;) resp_list = soft_mem %&gt;% add_column( time = rep(1:TT, times = nt)) %&gt;% group_by(time) %&gt;% group_split() %&gt;% lapply(select, c(&quot;clust1&quot;, &quot;clust2&quot;)) %&gt;% lapply(as.matrix) ## List of sigmas TT = nrow(sigma) sigma_list = lapply(1:TT, function(tt){ one_row = sigma[tt,] %&gt;% as.numeric() one_sigma = array(NA, dim = c(2,1,1)) one_sigma[,1,1] = one_row return(one_sigma) }) return(list(tab_long = tab_long, param_mat = param_mat, mu = mu, prob = prob, sigma = sigma, sigma_list = sigma_list, memlist = memlist, resp_list = resp_list, numclust = numclust)) } Here is a plotter function. #&#39; Plotter for underfit and overfit flowmeans. #&#39; @param ylist Data #&#39; @param countslist Optional: counts for each ylist #&#39; @param obj Output from \\code{underfit_flowmeans()} #&#39; @param bin Whether data is binned. #&#39; @return ggplot object #&#39; @export plot_1d_flowmeans &lt;- function(obj, ylist, countslist=NULL, bin = FALSE){ ## Make plot of only data gg = plot_1d(ylist = ylist, countslist = countslist, bin = bin) my_wrangle &lt;- function(a, values_to){ a %&gt;% as_tibble() %&gt;% setNames(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;)) %&gt;% mutate(time=row_number()) %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = values_to) } numclust = obj$numclust mn_long = obj$mu %&gt;% my_wrangle(&quot;mean&quot;) prob_long = obj$prob %&gt;% my_wrangle(&quot;prob&quot;) ## prob_long = probmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) est_long = full_join(mn_long, prob_long, by = c(&quot;time&quot;,&quot;cluster&quot;)) gg = gg + geom_path(aes(x = time, y = mean, linewidth = prob, group = cluster, color = cluster), data = est_long, lineend = &quot;round&quot;, linejoin=&quot;mitre&quot;)+ scale_linewidth(range = c(0,3), limits = c(0,1)) ## Add the estimated 95% probability regions for data. ## stdev = obj$sigma %&gt;% .[,,1] %&gt;% sqrt() stdev_long = obj$sigma %&gt;% sqrt() %&gt;% my_wrangle(&quot;stdev&quot;) band_long = full_join(mn_long, stdev_long, by = c(&quot;time&quot;, &quot;cluster&quot;)) %&gt;% mutate(upper = mean + 1.96 * stdev) %&gt;% mutate(lower = mean - 1.96 * stdev) gg = gg + geom_line(aes(x = time, y = upper, group = cluster, color = cluster), data = band_long, size = rel(.7), alpha = .5) + geom_line(aes(x = time, y = lower, group = cluster, color = cluster), data = band_long, size = rel(.7), alpha = .5) + guides(size = &quot;none&quot;) # To turn off line size from legend return(gg) } Here’s a helper to reorder the clusters of the output created using underfit_flowmeans() or overfit_flowmeans(). #&#39; Reordering function reorder_flowmeans &lt;- function(obj, new_order){ ## Setup numclust = max(new_order) ## Reorder everything obj$prob = obj$prob[,new_order] obj$mu = obj$mu[,new_order] obj$sigma = obj$sigma[,new_order] obj$memlist = lapply(obj$memlist, function(a){ a_copy = a for(iclust in 1:numclust){ a_copy[which(a == iclust)] = new_order[iclust] } return(a_copy) }) ## Reorder the responsibilities obj$resp_list = lapply(obj$resp_list, function(oneresp){ return(oneresp[,new_order]) }) return(obj) } Next, coding overfit_flowmeans(): #&#39; Apply flowmeans sequentially and then cluster match #&#39; #&#39; @param ylist Data. #&#39; @param numclust Number of clusters. #&#39; @return #&#39; @export overfit_flowmeans &lt;- function(ylist, numclust, verbose = FALSE){ fmns_obj &lt;- lapply(ylist, FUN = flowmeans_each, numclust = numclust) hybrid_out &lt;- match_clusters(fmns_obj) return(hybrid_out) } We’re going to be using gmm instead of flowmeans now. #&#39; Pool the entire series of cytograms, fit a GMM model, and re-aggregate parameters. #&#39; #&#39; @param ylist Data. #&#39; @param numclust Number of clusters (only works for 2). #&#39; @return #&#39; @export overfit_gmm &lt;- function(ylist, numclust = 2, reorder = TRUE){ ## Basic checks stopifnot(numclust == 2) ## Estimate individual GMM models gmm_list &lt;- lapply(ylist, gmm_each, numclust = numclust) ## Reorder the cluster memberships sequentially if(reorder){ gmm_list = match_clusters_gmm(gmm_list, numclust = 2) } ## Obtain the resulting data in long format tab_list &lt;- lapply(gmm_list, function(a) a$tab) TT = length(ylist) names(tab_list) = 1:TT tab_long = tab_list %&gt;% bind_rows(.id = &quot;time&quot;) %&gt;% pivot_longer(-c(&quot;time&quot;, &quot;y&quot;), values_to = &quot;cluster&quot;, names_to = &quot;type&quot;) memlist = tab_long %&gt;% subset(type == &quot;soft_cluster&quot;) %&gt;% group_by(time) %&gt;% group_split() %&gt;% lapply(function(a)pull(a, cluster)) ## ## temporary: make a plot ## ggplot(tab_long) + ## geom_point(aes(x=time, y=y, col = cluster), alpha = .5) + ## facet_grid(cluster~type) ## Get the Gaussian distribution parameters param_list &lt;- lapply(gmm_list, function(a) a$param) param_mat = param_list %&gt;% bind_rows(.id = &quot;time&quot;) mu = param_mat[,c(&quot;mn1&quot;, &quot;mn2&quot;)] %&gt;% as.matrix() prob = param_mat[,c(&quot;prob1&quot;, &quot;prob2&quot;)] %&gt;% as.matrix() sigma = param_mat[,c(&quot;sd1&quot;, &quot;sd2&quot;)] %&gt;% .^2 %&gt;% as.matrix() ## Responsibilities resp_list = gmm_list %&gt;% lapply(function(a) a$resp) ## Make a list of each time points&#39; sigmas TT = nrow(sigma) sigma_list = lapply(1:TT, function(tt){ one_row = sigma[tt,] %&gt;% as.numeric() one_sigma = array(NA, dim = c(2,1,1)) one_sigma[,1,1] = one_row return(one_sigma) }) return(list(tab_long = tab_long, param_mat = param_mat, mu = mu, prob = prob, sigma = sigma, sigma_list = sigma_list, resp_list = resp_list, numclust = numclust)) } ##&#39; Sequentially permute a time series of GMMs. ##&#39; ##&#39; @param gmm_list Output of an lapply of gmm_each over ylist. ##&#39; @param numclust Number of clusters. ##&#39; ##&#39; @return Same format as |gmm_list| match_clusters_gmm &lt;- function(gmm_list, numclust = 2){ ## Initialize some objects TT &lt;- length(gmm_list) ## Helper to permute one GMM my_reorder &lt;- function(one_gmm, new_order){ one_gmm_reordered = one_gmm one_gmm_reordered$tab = one_gmm_reordered$tab %&gt;% mutate(hard_cluster = hard_cluster %&gt;% plyr::revalue(replace=c(&quot;1&quot;=new_order[1],&quot;2&quot;=new_order[2]))) %&gt;% mutate(soft_cluster = soft_cluster %&gt;% plyr::revalue(replace=c(&quot;1&quot;=new_order[1],&quot;2&quot;=new_order[2]))) ## reorder the parameters param = one_gmm$param new_mns = param[,c(&quot;mn1&quot;, &quot;mn2&quot;)][new_order] %&gt;% as.numeric() new_sds = param[,c(&quot;sd1&quot;, &quot;sd2&quot;)][new_order] %&gt;% as.numeric() new_probs = param[,c(&quot;prob1&quot;, &quot;prob2&quot;)][new_order] %&gt;% as.numeric() one_gmm_reordered$param &lt;- data.frame(mn1 = new_mns[1], mn2 = new_mns[2], sd1 = new_sds[1], sd2 = new_sds[2], prob1 = new_probs[1], prob2 = new_probs[2]) ## if(all(new_order == 2:1)) browser() ## reorder the responsibilities one_gmm_reordered$resp = one_gmm$resp[,new_order] return(one_gmm_reordered) } ## Fill in the rest of the rows by sequentially matching clusters from t-1 to ## t, using the Hungarian Algorithm new_orders = matrix(NA, ncol = 2, nrow = TT) new_orders[1,] = c(1, 2) gmm_list_copy = gmm_list for(tt in 2:TT){ ## Find order param1 = gmm_list_copy[[tt-1]] %&gt;% .$param param2 = gmm_list[[tt]] %&gt;% .$param klmat &lt;- symmetric_kl_between_gaussians(param1 = param1, param2 = param2) new_order &lt;- RcppHungarian::HungarianSolver(klmat) %&gt;% .$pairs %&gt;% data.frame() %&gt;% arrange(.[,2]) %&gt;% .[,1] new_orders[tt,] = new_order ## Perform the reordering gmm_list_copy[[tt]] &lt;- my_reorder(gmm_list[[tt]], new_order) } return(gmm_list_copy) } ##&#39; Between two sets of Gaussian mean/sd parameters, what is the symmetric KL ##&#39; distance. ##&#39; @param param1 list of mn1, mn2, sd1, sd2 ##&#39; @param param2 another such list. ##&#39; ##&#39; @return (numclust x numclust) distance matrix. symmetric_kl_between_gaussians &lt;- function(param1, param2, numclust = 2){ ## First model&#39;s parameters mn_model1 = c(param1$mn1, param1$mn2) sd_model1 = c(param1$sd1, param1$sd2) ## Second model&#39;s parameters mn_model2 = c(param2$mn1, param2$mn2) sd_model2 = c(param2$sd1, param2$sd2) ## Fill out distance matrix dist_cs &lt;- matrix(0, ncol = numclust, nrow = numclust) for(iclust_1 in 1:numclust){ for(iclust_2 in 1:numclust){ dist_cs[iclust_2, iclust_1] &lt;- one_symmetric_kl(mu1 = mn_model1[iclust_1], mu2 = mn_model2[iclust_2], sd1 = sd_model1[iclust_1], sd2 = sd_model2[iclust_2]) } } rownames(dist_cs) &lt;- rep(&quot;C1&quot;, numclust) colnames(dist_cs) &lt;- rep(&quot;C2&quot;, numclust) return(dist_cs) } #&#39; Apply a gmm in each 1-dimensional cytogram. #&#39; #&#39; @param one_y One-column matrix. #&#39; @param numclust Number of clusters. #&#39; @return #&#39; @export gmm_each &lt;- function(one_y, numclust){ ## Basic check stopifnot(ncol(one_y) == 1) ## Do Gaussian Mixture model ## obj &lt;- mclust::Mclust(data = one_y, G = numclust, ## modelNames = &quot;V&quot;, verbose = FALSE) obj &lt;- my_mclust(one_y, numclust, FALSE) ## Memberships (soft- and hard-clustered) hard_mem = obj$z %&gt;% apply(1, which.max) soft_mem = obj$z %&gt;% apply(1, function(myrow){ sample(1:2, size = 1, replace = FALSE, prob=myrow) }) tab &lt;- tibble(y = as.numeric(one_y), soft_cluster = factor(soft_mem, levels = c(1,2)), hard_cluster = factor(hard_mem, levels = c(1,2))) ## parameter table mn1 = obj$parameters$mean[[1]] mn2 = obj$parameters$mean[[2]] sds = obj %&gt;% .$parameters %&gt;% .$variance %&gt;% .$sigmasq %&gt;% sqrt() sd1 = sds[1] sd2 = sds[2] prob1 = obj$parameters$pro[1] prob2 = obj$parameters$pro[2] ## ggplot(data.frame(x=as.numeric(one_y))) + ## geom_histogram(aes(x=x)) + ## geom_vline(xintercept = mn1, col = &#39;blue&#39;) + ## geom_vline(xintercept = mn1 + 2*sd1, col = &#39;skyblue&#39;) + ## geom_vline(xintercept = mn1 - 2*sd1, col = &#39;skyblue&#39;) + ## geom_vline(xintercept = mn2, col = &#39;red&#39;) + ## geom_vline(xintercept = mn2 - 2*sd2, col = &#39;orange&#39;) + ## geom_vline(xintercept = mn2 + 2*sd2, col = &#39;orange&#39;) param = tibble(mn1 = mn1, mn2 = mn2, sd1 = sd1, sd2 = sd2, prob1 = prob1, prob2 = prob2) resp = obj$z return(list(tab = tab, param = param, resp = resp))##, sigma_list = sigma_list)) } Here’s a function for soft-gating given a 2-column responsibility matrix. #&#39; Soft-gates a responsibility matrix by a bernoulli (or multinoulli) draw. #&#39; #&#39; @param oneresp A 2-column responsibility matrix #&#39; #&#39; @return soft_gate_one_responsibility_matrix &lt;- function(oneresp){ ## Setup numclust = ncol(oneresp) vec = rep(0,numclust) ## Draw the 0-1 memberships zero_one_mat = apply(oneresp, 1, function(myrow){ draw = sample(1:numclust, size=1, prob=myrow) vec[draw]= 1 vec }) %&gt;% t() ## Check dimensions and return stopifnot(all(dim(zero_one_mat) == dim(oneresp))) return(zero_one_mat) } The many_mixtures() function uses a few helpers: #&#39; Apply flowmeans in each cytogram. #&#39; #&#39; @param ylist Data. #&#39; @param numclust Number of clusters. #&#39; @return #&#39; @export flowmeans_each &lt;- function(ylist, numclust){ ## Basic check stopifnot(ncol(ylist[[1]])==1) ## Get cluster labels from the peaks fmns_obj &lt;- flowMeans::flowMeans(x = ylist, NumC = numclust) labeled_ylist &lt;- data.frame(cluster = fmns_obj@Label, ylist) %&gt;% tibble::as_tibble() colnames(labeled_ylist)[2] = &quot;Y&quot; ## Calculate cluster parameters cluster_params &lt;- labeled_ylist %&gt;% group_by(cluster) %&gt;% summarise(mu = mean(Y), prob = n()/nrow(labeled_ylist), sigma = ifelse(!is.na(var(Y)), var(Y), 1e-10)) return(list(fmns_obj = fmns_obj, cluster_params = cluster_params)) } #&#39; Apply the hungarian matching algorithm to each time point. #&#39; #&#39; @param flowmeans_obj #&#39; #&#39; @return match_clusters &lt;- function(fmns_obj){ ## Initialize some objects TT &lt;- length(fmns_obj) numc &lt;- nrow(fmns_obj[[1]]$cluster_params) mu &lt;- matrix(nrow = TT, ncol = numc) sigma &lt;- matrix(nrow = TT, ncol = numc) prob &lt;- matrix(nrow = TT, ncol = numc) costs &lt;- rep(NA, TT) memlist &lt;- list(fmns_obj[[1]]$fmns_obj@Label) ## First row is spoken for mu[1,] &lt;- fmns_obj[[1]]$cluster_params$mu sigma[1,] &lt;- fmns_obj[[1]]$cluster_params$sigma prob[1,] &lt;- fmns_obj[[1]]$cluster_params$prob ## Fill in the rest of the rows by sequentially matching clusters from t-1 to ## t, using the Hungarian Algorithm for(tt in 2:TT){ dt_to_tp1 &lt;- my_symmetric_kl(c1 = fmns_obj[[tt-1]]$cluster_params, c2 = fmns_obj[[tt]]$cluster_params) hng_tt &lt;- RcppHungarian::HungarianSolver(dt_to_tp1) hng_order &lt;- hng_tt$pairs %&gt;% data.frame() %&gt;% arrange(.[,2]) %&gt;% .[,1] fmns_obj[[tt]]$cluster_params &lt;- fmns_obj[[tt]]$cluster_params[hng_order,] mu[tt,] &lt;- fmns_obj[[tt]]$cluster_params$mu prob[tt,] &lt;- fmns_obj[[tt]]$cluster_params$prob sigma[tt,] &lt;- fmns_obj[[tt]]$cluster_params$sigma[hng_order] costs[tt] &lt;- hng_tt$cost if(mean(abs(mu[tt,] - mu[tt-1,]))/max(abs(mu[tt,])) &gt; 1){ cat(paste(&quot;Bad Match at time&quot;, tt, &quot;\\n&quot;)) } ## Convert labels label.convert &lt;- function(c1){hng_tt$pairs[c1,2]} memlist[[tt]] &lt;- sapply(fmns_obj[[tt]]$fmns_obj@Label, label.convert) } return(list(mu = mu, prob = prob, sigma = sigma, costs = costs, memlist = memlist)) } #&#39; Given two K x 2 columns with parameters, make K x K distance matrix. #&#39; #&#39; @param c1 Data frame with K rows and 2 columns (mu and sigma) #&#39; @param c2 Another data frame of the same size as \\code{c1}. #&#39; #&#39; @return K x K matrix. my_symmetric_kl &lt;- function(c1, c2){ ## Make sure were using the same number of clusters assertthat::assert_that(nrow(c1) == nrow(c2)) numclust &lt;- nrow(c1) ## Fill out distance matrix dist_cs &lt;- matrix(0, ncol = numclust, nrow = numclust) for(iclust_1 in 1:numclust){ for(iclust_2 in 1:numclust){ dist_cs[iclust_2, iclust_1] &lt;- one_symmetric_kl(mu1 = c1$mu[iclust_1], mu2 = c2$mu[iclust_2], sd1 = sqrt(c1$sigma[iclust_1]), sd2 = sqrt(c2$sigma[iclust_2])) } } rownames(dist_cs) &lt;- rep(&quot;C1&quot;, numclust) colnames(dist_cs) &lt;- rep(&quot;C2&quot;, numclust) return(dist_cs) } #&#39; Symmetric KL divergence between two Gaussian distributions. #&#39; @param mu1 Mean for distribution 1. #&#39; @param mu2 Mean for distribution 2. #&#39; @param sd1 Standard deviation for distribution 1. #&#39; @param sd2 Standard deviation for distribution 2. one_symmetric_kl &lt;- function(mu1, mu2, sd1, sd2){ stopifnot(length(mu1)==1 &amp; length(mu2) == 1 &amp; length(sd1) == 1 &amp; length(sd2) == 1) kl12 &lt;- log(sd2/sd1) + (sd1^2 + (mu1 - mu2)^2)/(2*sd2^2) - 1/2 kl21 &lt;- log(sd1/sd2) + (sd2^2 + (mu2 - mu1)^2)/(2*sd1^2) - 1/2 kl_sym &lt;- 0.5*(kl12 + kl21) return(kl_sym) } Here’s a useful function to create a flowtrend-like object but containing the “oracle” information about the cluster means, variances, and mixture probabilities. #&#39; Reformatting |datobj| to create a flowtrend-like object that contains #&#39; &quot;oracle&quot; information of the model that #&#39; generates the simulated pseudo-real data. #&#39; #&#39; @param datobj A data object. #&#39; #&#39; @return A flowtrend-like object containing mn, sigma, and prob. #&#39; #&#39; @export create_oracle &lt;- function(datobj){ TT = nrow(datobj$mns) dimdat = 1 numclust = ncol(datobj$mns) ## Make a fake object in a similar format as a |flowtrend| object fake_obj = list() fake_obj$mn = array(NA, dim = c(TT, dimdat,numclust)) fake_obj$mn[,1,] = datobj$mns fake_obj$prob = datobj$prob fake_obj$sigma = array(NA, dim = c(numclust, dimdat, dimdat)) fake_obj$sigma[1,1,1] = datobj$sd1^2 fake_obj$sigma[2,1,1] = datobj$sd2^2 fake_obj$numclust = numclust return(fake_obj) } 7.2 Example using other cluster algorithms We will see some examples of how to use underfit_flowmeans() and overfit_flowmeans(). ## Generate data numclust = 3 TT = 100 dimdat = 1 set.seed(0) dt = gendat_1d(TT = TT, ntlist = rep(TT, 100)) ylist = dt %&gt;% dt2ylist() truth = gendat_1d(TT = TT, ntlist = rep(TT, 100), return_model=TRUE) ## Fit the two methods obj_underfit = underfit_flowmeans(ylist = ylist, numclust = 3) obj_overfit = overfit_flowmeans(ylist = ylist, numclust = 3) ## Make the mean plots o = obj_underfit$mu %&gt;% colMeans() %&gt;% order() obj_underfit = reorder_flowmeans(obj_underfit, o) g1 = plot_1d_flowmeans(reorder_flowmeans(obj_underfit, o), ylist) + geom_line(aes(x=time, y=mean, group = cluster), data = truth, col = &#39;black&#39;) o = obj_overfit$mu %&gt;% colMeans() %&gt;% order() obj_overfit = reorder_flowmeans(obj_overfit, o) g2 = plot_1d_flowmeans(obj_overfit, ylist) + geom_line(aes(x=time, y=mean, group = cluster), data = truth, col = &#39;black&#39;) do.call(ggpubr::ggarrange, c(list(g1, g2), ncol=1, nrow=2)) ## Make the probability plots g1 = obj_overfit$prob %&gt;% my_wrangle(&quot;prob&quot;) %&gt;% ggplot() + geom_line(aes(x=time, y = prob, group = cluster, col = cluster), size = rel(1)) + ggtitle(&quot;Estimated cluster probability&quot;) + geom_line(aes(x=time, y=prob, group = cluster), data = truth, col = &#39;black&#39;) g2 = obj_underfit$prob %&gt;% my_wrangle(&quot;prob&quot;) %&gt;% ggplot() + geom_line(aes(x=time, y = prob, group = cluster, col = cluster), size = rel(1)) + ggtitle(&quot;Estimated cluster probability&quot;) + geom_line(aes(x=time, y=prob, group = cluster), data = truth, col = &#39;black&#39;) do.call(ggpubr::ggarrange, c(list(g1, g2), ncol=1, nrow=2)) 7.3 Oracle A flowtrend-like object that contains “oracle” information of the model that generates the simulated pseudo-real data. #&#39; Reformatting |datobj| to create a flowtrend-like object that contains &quot;oracle&quot; information of the model that #&#39; generates the simulated pseudo-real data. #&#39; #&#39; @param datobj A data object. #&#39; #&#39; @return A flowtrend-like object containing mn, sigma, and prob. #&#39; #&#39; @export create_oracle &lt;- function(datobj){ TT = nrow(datobj$mns) dimdat = 1 numclust = ncol(datobj$mns) fake_obj = list() fake_obj$mn = array(NA, dim = c(TT,dimdat,numclust)) fake_obj$mn[,1,] = datobj$mns fake_obj$prob = datobj$prob fake_obj$sigma = array(NA, dim = c(numclust, dimdat, dimdat)) fake_obj$sigma[1,1,1] = datobj$sd1^2 fake_obj$sigma[2,1,1] = datobj$sd2^2 fake_obj$numclust = numclust return(fake_obj) } 7.4 Evaluating performance: soft RAND index We will use a “soft” Rand index that replaces the regular Rand index: \\[ \\sum_{i, i&#39;} 1\\{ \\hat C_i = \\hat C_{i&#39;}, C_i^* \\neq C_{i&#39;}^*\\}.\\] which measures, for every pair of points \\(i\\) and \\(i&#39;\\), the number of times that the two clustering mechanisms disagree. Now, let’s say that the two mechanisms give probabilities: \\[\\hat \\gamma_{ik} = \\hat P(\\hat C_i = k),\\] \\[\\hat \\gamma^*_{ik} = \\hat P(\\hat C_i^* = k).\\] Then, the probability that the clustering is the same \\(P(C_i^* = C_{i&#39;}^*)\\) for the pair of points \\(i\\) and \\(i&#39;\\) is: \\[ (\\gamma_i^*)^T (\\gamma_{i&#39;}^*) = \\sum_{k=1} P(C_i = k) P(C_i^* = k) = P(C_i^* = C_{i&#39;}^*).\\] and the probaility they are different is: \\[ (\\gamma_i^*)^T (\\gamma_{i&#39;}^*) = \\sum_{k=1} P(C_i = k) P(C_i^* = k) = P(C_i^* = C_{i&#39;}^*).\\] So, we can measure the difference as: \\[\\sum_{i,i&#39;} (\\hat \\gamma_i^T \\hat \\gamma_{i&#39;})\\cdot(1- \\gamma^*_i^T \\gamma^*_{i&#39;}) \\] And when one of the clusterings is soft, we can still use a 0-1 vector as \\(\\gamma\\). #&#39; A &quot;soft&quot; version of a rand index between two sets of responsibility #&#39; (membership probability) matrices. Measures the /disagreement/ between the two clusterings. #&#39; #&#39; @param resp_list1 One list of responsibility matrices. #&#39; @param resp_list2 Another list of responsibility matrices. #&#39; @param times Optional; if you would like to isolate your attention to some specific times. #&#39; #&#39; @return A single soft rand index number #&#39; @export rand_old &lt;- function(resp_list1, resp_list2, times = NULL, prop = .25){ if(!is.null(times)) resp_list1 = resp_list1[times] if(!is.null(times)) resp_list2 = resp_list2[times] rand_onetime &lt;- function(resp1, resp2){ stopifnot(nrow(resp1) == nrow(resp2)) stopifnot(ncol(resp1) == ncol(resp2)) nt = nrow(resp1) ## Form the score mat11 = resp1 %*% t(resp1) mat22 = resp2 %*% t(resp2) mat11not = (1-mat11) mat22not = (1-mat22) ## Make the diagonals not matter anywhere diag(mat11) = 0 diag(mat22) = 0 diag(mat11not) = 0 diag(mat22not) = 0 a = mat11 * mat22 b = mat11not * mat22not c = mat11 * mat22not d = mat11not * mat22 return(c(a = sum(a), b = sum(b), c = sum(c), d = sum(d))) } abcd_over_time = mapply(rand_onetime, resp_list1, resp_list2) abcdmat = abcd_over_time %&gt;% t() abcd = abcdmat %&gt;% colSums() score = (abcd[&quot;a&quot;] + abcd[&quot;b&quot;])/ (sum(abcd)) return(score) } #&#39; Rand index between responsibilities. #&#39; #&#39; @param resp_list1 One list. #&#39; @param resp_list2 Another list. #&#39; @param times A subset of the time points (out of 1:length(resp_list1)) to #&#39; examine. #&#39; @param smaller If TRUE, use only a small (sampled) subset of the particles&#39; #&#39; responsibilities for calculating RAND. #&#39; @param prop How much to downsample; defaults to 0.1. #&#39; @export rand &lt;- function(resp_list1, resp_list2, times = NULL, smaller = TRUE, prop = 0.1){ ## Basic checks stopifnot(all(sapply(resp_list1, nrow) == sapply(resp_list2, nrow))) ## Subset the times if needed if(!is.null(times)) resp_list1 = resp_list1[times] if(!is.null(times)) resp_list2 = resp_list2[times] ## names(everything) ## list2env(everything,envir = environment()) ## resp_list1 = resp_oracle ## resp_list2 = true_resp if(smaller){ indslist = lapply(resp_list1, function(oneresp){ inds = sample(x=1:nrow(oneresp), size=ceiling(nrow(oneresp)*prop), replace=FALSE) %&gt;% sort() }) resp_list1 = mapply(function(a,b)a[b,,drop=FALSE], resp_list1, indslist, SIMPLIFY=FALSE) resp_list2 = mapply(function(a,b)a[b,,drop=FALSE], resp_list2, indslist, SIMPLIFY=FALSE) } ## Make each list into one long matrix resp1 = Reduce(rbind, resp_list1) resp2 = Reduce(rbind, resp_list2) ## resp1 = do.call(rbind, resp_list1)## %&gt;% bind_rows() ## resp2 = do.call(rbind, resp_list2)## %&gt;% bind_rows() stopifnot(nrow(resp1) == nrow(resp2)) stopifnot(ncol(resp1) == ncol(resp2)) nt = nrow(resp1) ## Form the score mat11 = resp1 %*% t(resp1) mat22 = resp2 %*% t(resp2) mat11not = (1-mat11) mat22not = (1-mat22) ## Make the diagonals not matter anywhere diag(mat11) = 0 diag(mat22) = 0 diag(mat11not) = 0 diag(mat22not) = 0 a = mat11 * mat22 b = mat11not * mat22not c = mat11 * mat22not d = mat11not * mat22 abcd = c(a = sum(a), b = sum(b), c = sum(c), d = sum(d)) score = (abcd[&quot;a&quot;] + abcd[&quot;b&quot;])/ (sum(abcd)) return(score) } #&#39; 2 x 2 contigency table from membership vectors. #&#39; #&#39; @param mem1 Membership vector. #&#39; @param mem2 Another membership vector. #&#39; #&#39; @return A 3x3 matrix containing (1) a 2 x 2 table in the first [1:2,1:2] #&#39; entries, and (2) row sums and column sums and total sums in the [,3], [3,] #&#39; entries. #&#39; @export make_contingency_table&lt;-function(mem1, mem2){ tab = table(mem1, mem2) tab = cbind(tab, rowSums(tab)) tab = rbind(tab, colSums(tab)) return(tab) } #&#39; RAND index. #&#39; #&#39; @param tab A matrix containing a 2 x 2 table in the first [1:2,1:2] #&#39; entries; e.g., from make_contingency_table(). #&#39; #&#39; @return Rand index. #&#39; @export get_rand_from_table &lt;- function(tab){ numer = sum(sapply(as.numeric(tab), function(nij) choose(nij, 2))) + tab[1,1] * tab[2,2] + tab[1,2] * tab[2,1] denom = (choose(sum(tab), 2)) ri = numer/denom return(ri) } #&#39; Faster rand index calculation from membership vectors. #&#39; #&#39; @param mem1 #&#39; @param mem2 #&#39; #&#39; @return RAND index. #&#39; @export rand_from_mems &lt;- function(mem1, mem2){ make_contingency_table(mem1, mem2) %&gt;% .[1:2,1:2] %&gt;% get_rand_from_table() } #&#39; A &quot;soft&quot; *and adjusted* version of a rand index between two lists of responsibility #&#39; (membership probability) matrices. Measures the /disagreement/ between the two clusterings. #&#39; #&#39; @param resp_list1 One list of responsibility matrices. #&#39; @param resp_list2 Another list of responsibility matrices. #&#39; @param times Optional; if you would like to isolate your attention to some #&#39; specific times. #&#39; #&#39; @return A single soft rand index for all particles across all times. #&#39; #&#39; @export adjusted_rand &lt;- function(resp_list1, resp_list2, times = NULL){ ## Basic checks if(!is.null(times)) resp_list1 = resp_list1[times] if(!is.null(times)) resp_list2 = resp_list2[times] all_mat_over_time = mapply(adjusted_rand_onetime, resp_list1, resp_list2, SIMPLIFY = FALSE) mat = Reduce(&#39;+&#39;, all_mat_over_time) n = sum(mat) sum_a = sum(sapply(rowSums(mat), choose, 2)) sum_b = sum(sapply(colSums(mat), choose, 2)) numer = sum(sapply(mat, function(a) choose(a,2))) - sum_a * sum_b / choose(n, 2) denom = (sum_a + sum_b)/2 - (sum_a * sum_b)/choose(n,2) ari = numer/denom return(ari) } #&#39; A &quot;soft&quot; *and adjusted* version of a rand index between two responsibility #&#39; (membership probability) matrices. Measures the /disagreement/ between the #&#39; two clusterings. #&#39; #&#39; @param resp1 One responsibility matrices. #&#39; @param resp2 Another responsibility matrices. #&#39; #&#39; @return A single soft rand index for all particles. #&#39; @export adjusted_rand_onetime &lt;- function(resp1, resp2){ ## Basic checks stopifnot(nrow(resp1) == nrow(resp2)) stopifnot(ncol(resp1) == ncol(resp2)) nt = nrow(resp1) ## Next, build contingency table mem1 = resp1 %&gt;% apply(1, function(a){(1:2)[which(a==0)]}) ## why is this a==0?? TODO address this. mem2 = resp2 %&gt;% apply(1, function(a){(1:2)[which(a==0)]}) mat = matrix(NA, nrow=2, ncol=2) mat[1,1] = sum(mem1==1 &amp; mem2 == 1) mat[2,2] = sum(mem1==2 &amp; mem2 == 2) mat[1,2] = sum(mem1==1 &amp; mem2 == 2) mat[2,1] = sum(mem1==2 &amp; mem2 == 1) return(mat) } numclust = 3 TT = 100 dimdat = 1 set.seed(0) dt = gendat_1d(TT = TT, ntlist = rep(TT, 100)) ylist = dt %&gt;% dt2ylist() truth = gendat_1d(TT = TT, ntlist = rep(TT, 100), return_model=TRUE) res = flowtrend(ylist=ylist, numclust=3, l=2, l_prob=1, lambda = .01, lambda_prob=0.01, verbose=TRUE, nrestart=1) res2 = flowtrend(ylist=ylist, numclust=3, l=2, l_prob=1, lambda = 1, lambda_prob=1, verbose=TRUE, nrestart=1) ## Calculate the soft rand index of two different models rand(res$resp, res2$resp) rand(res$resp, res$resp) rand(res2$resp, res2$resp) g1 = plot_1d(ylist=ylist, obj=res2) g2 = plot_1d(ylist=ylist, obj=res) do.call(ggpubr::ggarrange, c(list(g1, g2), ncol=1, nrow=2)) ## Case 1: above ## Case 2: one is the TRUTH; based on the ## Case 3: one is a hard clustering. plot(ylist[[1]], col = obj$memlist[[1]]) obj$resp = get_resp_from_labels(obj$memlist) rand(res$resp, res2$resp) rand(res$resp, res$resp) ## Next, code up the truth a = dt %&gt;% group_by(time ) %&gt;% group_split() %&gt;% lapply(function(one_dt) one_dt %&gt;% pull(cluster)) #&#39; Helper to get, from labels get_resp_from_labels &lt;- function(labels){ numclust = max(labels[[1]]) lapply(labels, function(one_time_label){ one_resp = rep(0,3) resp = sapply(obj$memlist[[1]], function(ii){ one_resp[ii] = 1 return(one_resp) }) %&gt;% t() return(resp) }) } It’s also helpful to have a function that takes a list of discrete memberships (integers 1,..,K) and convert it to a list of responsibility matrices similar in format to obj$resp of a flowtrend object obj. #&#39; Convert list of memberships into a list of responsibilities. #&#39; #&#39; @param memlist List of memberships #&#39; #&#39; @return #&#39; @export memlist_to_respmat &lt;- function(memlist){ numclust = max(sapply(memlist, max)) respmats = lapply(memlist, function(mem){ respmat = lapply(mem, function(onemem){ onerow = rep(0, numclust)##c(0,0) onerow[onemem] = 1 return(onerow) }) %&gt;% do.call(rbind, .) return(respmat) }) return(respmats) } 7.5 Generating “pseudo-real” data Two clusters will be taken from an estimated flowtrend model fit on real data, downloaded from here: https://zenodo.org/records/6471995 to ./data. ## This data is from the results from here::i_am(&quot;7simulations.Rmd&quot;) datadir = file.path(here::here(), &quot;inst&quot;, &quot;data&quot;) res = readRDS(file.path(datadir, &quot;1d-cvres.rds&quot;)) %&gt;% .$bestres ## Picoeuk iclust = 3 mn1 = cbind(1, res$X) %*% res$beta[[iclust]] %&gt;% as.numeric() ## Prochlorococcus iclust = 4 mn2 = cbind(1, res$X) %*% res$beta[[iclust]] %&gt;% as.numeric() ## Cluster probabilities probs = res$pie[,c(3, 4)] probs = probs / rowSums(probs) ## Save them as separte objects mns_orig = cbind(mn1, mn2) probs_orig = probs Let’s smooth these using a trend-filter; The cluster means will be taken to directly trend filtered at 2. The log odds will be smoothed. Specifically, recall that \\(p = e^\\gamma / (1 + e^\\gamma)\\). So, we will take \\(\\gamma = \\log(p/(1-p)) \\propto \\log(p)\\), smooth it, and recreate \\(p\\). Next, generate new 1d data. nt = 1000 sd1 = res$sigma %&gt;% .[,1,1] %&gt;% .[3] %&gt;% sqrt() sd2 = res$sigma %&gt;% .[,1,1] %&gt;% .[4] %&gt;% sqrt() ## Smooth means mn1 = mns_orig[,1] mn2 = mns_orig[,2] gap = mean(mn1) - mean(mn2) mn1 = mn1 - gap mn1 = mn1 %&gt;% fit_tf(ord = 2) mn2 = mn2 %&gt;% fit_tf(ord = 2) ## Smooth probabilities probs = probs_orig gamma = log(probs[,1]/probs[,2]) gamma = gamma %&gt;% fit_tf(ord = 1) probs[,1] = gamma %&gt;% exp() probs[,1] = probs[,1]/(1 + probs[,1]) probs[,2] = 1 - probs[,1] ## Create all the data for(isignal in 0:12){ print(&quot;isignal&quot;) print(isignal) parallel::mclapply(1:10, function(isim){ printprogress(isim, 10) ## for(isim in 1:10){ mns = cbind(mn1 + isignal * 0.05, mn2) set.seed(10000 + isignal * 100 + isim) datobj = pseudoreal_gendat_1d(mns, probs, sd1, sd2, nt = 1000) datobj[[&quot;mns&quot;]] = mns datobj[[&quot;probs&quot;]] = probs datobj[[&quot;sd1&quot;]] = sd1 datobj[[&quot;sd2&quot;]] = sd2 destin = file.path(&quot;~/repos/flowtrend/inst/output/1dsim-pseudoreal&quot;, paste0(&quot;isignal-&quot;, isignal), paste0(&quot;isim-&quot;, isim)) create_destin(destin) ## if(file.exists(file.path(destin, &quot;datobj.RDS&quot;))) next saveRDS(datobj, file = file.path(destin, &quot;datobj.RDS&quot;)) ## } }, mc.cores = 8) cat(fill=TRUE) } 7.6 Miscellaneous helpers Lastly, there are some miscellaneous helpers that are useful when running actual jobs on a server. The first one is create_destin(), which creates a directory if it doesn’t already exist. #&#39; Creates a directory \\code{destin}, if it doesn&#39;t already exist. #&#39; #&#39; @param destin Destination directory. #&#39; #&#39; @return Nothing. #&#39; @export create_destin &lt;- function(destin){ if(!dir.exists(destin)){ dir.create(destin, recursive = TRUE) cat(&quot;Creating destin: &quot;, destin, fill=TRUE) } else { cat(&quot;All output goes out to destin: &quot;, destin, fill = TRUE) } } Another helper parse_args() helps R read in trailing arguments from the command line, like this: parse_args(args = commandArgs(trailingOnly = TRUE), verbose=TRUE) so that one can run jobs using a SLURM command such as: sbatch --export=summ=0 --array=1 run-3dreal.slurm from which the R script can access the summ=0 variable. #&#39; Parse command line arguments and assigns the values of them. |args| is meant #&#39; to just be additional command line arguments. #&#39; #&#39; @param args Argument. #&#39; #&#39; @return Nothing. #&#39; @export parse_args &lt;- function(args, verbose=FALSE){ args = sapply(args, strsplit, &quot;=&quot;) print(args) for(arg in args){ ## Check if the thing is integer all_numbers = str_detect(arg[2], &quot;^[:digit:]+$&quot;) ## Assign the variable if(all_numbers){ assign(arg[1], as.numeric(arg[2]), inherits = TRUE) } else { assign(arg[1], arg[2], inherits = TRUE) } if(verbose){ cat(arg[1], &quot;takes the value of&quot;, arg[2], &quot;from command line input&quot;, fill=TRUE) } print(&quot;===============================&quot;) } } "],["documenting-the-package-and-building.html", "8 Documenting the package and building", " 8 Documenting the package and building We finish by running commands that will document, build, and install the package. It may also be a good idea to check the package from within this file. litr::document() # &lt;-- use instead of devtools::document() ## ℹ Updating flowtrend documentation ## ℹ Loading flowtrend ## ℹ Re-compiling flowtrend (debug build) ## Warning: [Estep.R:5] @param requires name and description ## Warning: [Estep.R:7] @param requires name and description ## Warning: [Estep.R:8] @param requires name and description ## Warning: [Estep.R:9] @param requires name and description ## Warning: [Estep.R:10] @param requires name and description ## Warning: [Estep.R:11] @param requires name and description ## Warning: [Estep.R:14] @return requires a value ## Warning: [Mstep_mu.R:9] @param requires name and description ## Warning: [Mstep_mu.R:10] @param requires name and description ## Warning: [Mstep_mu.R:11] @param requires name and description ## Warning: [Mstep_mu.R:12] @param requires name and description ## Warning: [Mstep_mu.R:13] @param requires name and description ## Warning: [Mstep_mu.R:14] @param requires name and description ## Warning: [Mstep_mu.R:15] @param requires name and description ## Warning: [Mstep_mu.R:16] @param requires name and description ## Warning: [Mstep_mu.R:17] @param requires name and description ## Warning: [Mstep_mu.R:18] @param requires name and description ## Warning: [Mstep_mu.R:19] @param requires name and description ## Warning: [Mstep_mu.R:20] @param requires name and description ## Warning: [Mstep_mu.R:21] @param requires name and description ## Warning: [Mstep_mu.R:22] @param requires name and description ## Warning: [Mstep_mu.R:23] @param requires name and description ## Warning: [Mstep_mu.R:24] @param requires name and description ## Warning: [Mstep_mu.R:25] @param requires name and description ## Warning: [Mstep_mu.R:26] @param requires name and description ## Warning: [Mstep_mu.R:27] @param requires name and description ## Warning: [Mstep_mu.R:28] @param requires name and description ## Warning: [Mstep_mu.R:29] @param requires name and description ## Warning: [Mstep_mu.R:30] @param requires name and description ## Warning: [Mstep_mu.R:33] @return requires a value ## Warning: [Mstep_mu.R:36] @examples requires a value ## Warning: [Mstep_mu_cvxr.R:4] @param requires name and description ## Warning: [Mstep_mu_cvxr.R:5] @param requires name and description ## Warning: [Mstep_mu_cvxr.R:6] @param requires name and description ## Warning: [Mstep_mu_cvxr.R:7] @param requires name and description ## Warning: [Mstep_sigma.R:13] @examples requires a value ## Warning: [flowmeans_each.R:7] @return requires a value ## Warning: [flowtrend.R:8] @return requires a value ## Warning: [flowtrend.R:11] @examples requires a value ## Warning: [flowtrend_once.R:22] @param requires name and description ## Warning: [flowtrend_once.R:23] @param requires name and description ## Warning: [flowtrend_once.R:24] @param requires name and description ## Warning: [flowtrend_once.R:25] @param requires name and description ## Warning: [flowtrend_once.R:31] @examples requires a value ## Warning: [gen_diff_mat.R:12] @examples requires a value ## Warning: [la_admm_oneclust.R:5] @param requires name and description ## Warning: [la_admm_oneclust.R:8] @return requires a value ## Warning: [la_admm_oneclust.R:11] @examples requires a value ## Warning: [make_cvscore_filename.R:4] @param requires name and description ## Warning: [make_cvscore_filename.R:5] @param requires name and description ## Warning: [make_cvscore_filename.R:6] @param requires name and description ## Warning: [make_cvscore_filename.R:16] @param requires name and description ## Warning: [make_cvscore_filename.R:17] @param requires name and description ## Warning: [make_cvscore_filename.R:18] @param requires name and description ## Warning: [make_cvscore_filename.R:30] @param requires name and description ## Warning: [make_cvscore_filename.R:31] @param requires name and description ## Warning: [make_cvscore_filename.R:42] @param requires name and description ## Warning: [match_clusters.R:7] @return requires a value ## Warning: [memlist_to_respmat.R:7] @return requires a value ## Warning: [objective.R:5] @param requires name and description ## Warning: [objective.R:6] @param requires name and description ## Warning: [objective.R:7] @param requires name and description ## Warning: [objective.R:8] @param requires name and description ## Warning: [objective.R:9] @param requires name and description ## Warning: [objective.R:10] @param requires name and description ## Warning: [objective.R:11] @param requires name and description ## Warning: [objective.R:12] @param requires name and description ## Warning: [objective.R:13] @param requires name and description ## Warning: [objective.R:14] @param requires name and description ## Warning: [objective.R:15] @param requires name and description ## Warning: [objective.R:16] @param requires name and description ## Warning: [objective.R:17] @param requires name and description ## Warning: [objective.R:18] @param requires name and description ## Warning: [objective.R:19] @param requires name and description ## Warning: [objective.R:22] @return requires a value ## Warning: [objective.R:25] @examples requires a value ## Warning: [overfit_flowmeans.R:7] @return requires a value ## Warning: [overfit_gmm.R:7] @return requires a value ## Warning: [overfit_gmm.R:157] @return requires a value ## Warning: [plot_3d.R:12] @return requires a value ## Warning: [rand_old.R:148] @param requires name and description ## Warning: [soft_gate_one_responsibility_matrix.R:7] @return requires a value ## Warning: [underfit_gmm.R:7] @return requires a value ## Writing &#39;NAMESPACE&#39; ## Warning: Skipping &#39;U_update_Z.Rd&#39; ## ℹ File lacks name and/or title ## Warning: Skipping &#39;U_update_W.Rd&#39; ## ℹ File lacks name and/or title ## Warning: Skipping &#39;etilde_mat.Rd&#39; ## ℹ File lacks name and/or title ## Warning: Skipping &#39;myschur.Rd&#39; ## ℹ File lacks name and/or title ## Writing &#39;NAMESPACE&#39; ## Writing &#39;Estep.Rd&#39; ## Writing &#39;Mstep_mu.Rd&#39; ## Writing &#39;Mstep_mu_cvxr.Rd&#39; ## Writing &#39;Mstep_prob.Rd&#39; ## Writing &#39;Mstep_sigma.Rd&#39; ## Writing &#39;matrix_function_solve_triangular_sylvester_barebonesC2.Rd&#39; ## Writing &#39;W_update_fused.Rd&#39; ## Writing &#39;projCmat.Rd&#39; ## Writing &#39;adjusted_rand.Rd&#39; ## Writing &#39;adjusted_rand_onetime.Rd&#39; ## Writing &#39;admm_oneclust.Rd&#39; ## Writing &#39;aug_lagr.Rd&#39; ## Writing &#39;calc_max_lambda.Rd&#39; ## Writing &#39;check_converge.Rd&#39; ## Writing &#39;check_converge_rel.Rd&#39; ## Writing &#39;create_destin.Rd&#39; ## Writing &#39;create_oracle.Rd&#39; ## Writing &#39;cv_aggregate.Rd&#39; ## Writing &#39;cv_aggregate_res.Rd&#39; ## Writing &#39;cv_flowtrend.Rd&#39; ## Writing &#39;cv_makebest.Rd&#39; ## Writing &#39;load_all_objectives.Rd&#39; ## Writing &#39;load_all_refit_objectives.Rd&#39; ## Writing &#39;keep_only_best.Rd&#39; ## Writing &#39;keep_only_best_refit.Rd&#39; ## Writing &#39;cv_summary.Rd&#39; ## Writing &#39;dt2ylist.Rd&#39; ## Writing &#39;flowmeans_each.Rd&#39; ## Writing &#39;flowtrend-package.Rd&#39; ## Writing &#39;flowtrend.Rd&#39; ## Writing &#39;flowtrend_once.Rd&#39; ## Writing &#39;form_symmetric_kl_distmat.Rd&#39; ## Writing &#39;gen_diff_mat.Rd&#39; ## Writing &#39;gen_tf_mat.Rd&#39; ## Writing &#39;gen_tf_mat_equalspace.Rd&#39; ## Writing &#39;gendat_1d.Rd&#39; ## Writing &#39;gendat_2d.Rd&#39; ## Writing &#39;gendat_3d.Rd&#39; ## Writing &#39;get_best_match_from_kl.Rd&#39; ## Writing &#39;get_max_lambda.Rd&#39; ## Writing &#39;init_mn.Rd&#39; ## Writing &#39;init_sigma.Rd&#39; ## Writing &#39;interpolate_mn.Rd&#39; ## Writing &#39;interpolate_prob.Rd&#39; ## Writing &#39;la_admm_oneclust.Rd&#39; ## Writing &#39;loglik_tt.Rd&#39; ## Writing &#39;logspace.Rd&#39; ## Writing &#39;make_cv_folds.Rd&#39; ## Writing &#39;make_cv_folds_in_blocks.Rd&#39; ## Writing &#39;make_cvscore_filename.Rd&#39; ## Writing &#39;make_best_cvscore_filename.Rd&#39; ## Writing &#39;make_refit_filename.Rd&#39; ## Writing &#39;make_best_refit_filename.Rd&#39; ## Writing &#39;make_iilist.Rd&#39; ## Writing &#39;make_iimat.Rd&#39; ## Writing &#39;make_iimat_small.Rd&#39; ## Writing &#39;match_clusters.Rd&#39; ## Writing &#39;my_symmetric_kl.Rd&#39; ## Writing &#39;one_symmetric_kl.Rd&#39; ## Writing &#39;memlist_to_respmat.Rd&#39; ## Writing &#39;my_mfrow.Rd&#39; ## Writing &#39;objective.Rd&#39; ## Writing &#39;objective_per_cluster.Rd&#39; ## Writing &#39;one_job.Rd&#39; ## Writing &#39;one_job_refit.Rd&#39; ## Writing &#39;overfit_flowmeans.Rd&#39; ## Writing &#39;overfit_gmm.Rd&#39; ## Writing &#39;match_clusters_gmm.Rd&#39; ## Writing &#39;symmetric_kl_between_gaussians.Rd&#39; ## Writing &#39;gmm_each.Rd&#39; ## Writing &#39;parse_args.Rd&#39; ## Writing &#39;plot_1d.Rd&#39; ## Writing &#39;plot_1d_add_model.Rd&#39; ## Writing &#39;plot_1d_flowmeans.Rd&#39; ## Writing &#39;plot_1d_with_membership.Rd&#39; ## Writing &#39;plot_2d.Rd&#39; ## Writing &#39;plot_3d.Rd&#39; ## Writing &#39;plot_from_summary.Rd&#39; ## Writing &#39;plot_prob.Rd&#39; ## Writing &#39;predict_flowtrend.Rd&#39; ## Writing &#39;print_progress.Rd&#39; ## Writing &#39;rand_old.Rd&#39; ## Writing &#39;rand.Rd&#39; ## Writing &#39;make_contingency_table.Rd&#39; ## Writing &#39;get_rand_from_table.Rd&#39; ## Writing &#39;rand_from_mems.Rd&#39; ## Writing &#39;reorder_clust.Rd&#39; ## Writing &#39;reorder_flowmeans.Rd&#39; ## Writing &#39;reorder_kl.Rd&#39; ## Writing &#39;soft_gate_one_responsibility_matrix.Rd&#39; ## Writing &#39;softmax.Rd&#39; ## Writing &#39;symmetric_kl.Rd&#39; ## Writing &#39;underfit_flowmeans.Rd&#39; ## Writing &#39;underfit_gmm.Rd&#39; ## Writing &#39;pipe.Rd&#39; "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
